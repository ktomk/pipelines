const local_index = {"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Pipelines \u00b6 Run Bitbucket Pipelines Wherever They Dock \u00b6 Command line pipeline runner written in PHP. Available from Github or Packagist. Usage | Environment | Exit Status | Details | References Usage \u00b6 From anywhere within a project or (Git) repository with a Bitbucket Pipeline file: $ pipelines Runs pipeline commands from bitbucket-pipelines.yml [BBPL]. Memory and time limits are ignored. Press ctrl + c to quit. The Bitbucket limit of 100 (previously 10) steps per pipeline is ignored. Exit status is from last pipeline script command, if a command fails the following script commands and steps are not executed. The default pipeline is run, if there is no default pipeline in the file, pipelines tells it and exists with non-zero status. To execute a different pipeline use the --pipeline <id> option where <id> is one of the list by the --list option. Even more information about the pipelines is available via --show . Both --list and --show output and exit. Use --steps <steps> to specify which step(s) to execute (also in which order). If the next pipeline step has a manual trigger, pipelines stops the execution and outputs a short message on standard error giving info about the fact. Manual triggers can be ignored with the --no-manual option. Run the pipeline as if a tag/branch or bookmark has been pushed with --trigger <ref> where <ref> is tag:<name> , branch:<name> , bookmark:<name> or pr:<branch-name>[:<destination-branch>] . If there is no tag, branch, bookmark or pull-request pipeline with that name, the name is compared against the patterns of the referenced type and if found, that pipeline is run. Otherwise the default pipeline is run, if there is no default pipeline, no pipeline at all is run and the command exits with non-zero status. --pipeline and --trigger can be used together, --pipeline overrides pipeline from --trigger but --trigger still influences the container environment variables. To specify a different file use the --basename <basename> or --file <path> option and/or set the working directory --working-dir <path> in which the file is looked for unless an absolute path is set by --file <path> . By default pipelines operates on the current working tree which is copied into the container to isolate running the pipeline from the working directory (implicit --deploy copy ). Alternatively the working directory can be mounted into the pipelines container by using --deploy mount . Use --keep flag to keep containers after the pipeline has finished for further inspection. By default all containers are destroyed. Sometimes for development it is interesting to keep containers on error only, the --error-keep flag is for that. In any case, if a pipeline runs again and it finds an existing container with the same name (generated by the pipeline name etc.), the existing container will be re-used. This can be very useful to re-iterate quickly. Manage leftover containers with --docker-list showing all pipeline containers, --docker-kill to kill running containers and --docker-clean to remove stopped pipeline containers. Use in combination to fully clean, e.g.: $ pipelines --docker-list --docker-kill --docker-clean Or just run for a more shy clean-up: $ pipelines --docker-zap to kill and remove all pipeline containers (w/o showing a list) first. \"zap\" is pipelines \"make clean\" equivalent for --keep . All containers run by pipelines are labeled to ease maintaining them. Validate your bitbucket-pipelines.yml file with --show which highlights errors found. For schema-validation use --validate [<file>] . Schema validation might show errors that are not an issue when executing a pipeline ( --show and/or --dry-run is better for that) but validates against a schema which is aligned with the one that Atlassian/ Bitbucket provides (the schema is more lax compared to upstream for the cases known to offer a better practical experience). E.g. use it for checks in your CI pipeline or linting files before push in a pre-commit hook or your local build. Inspect your pipeline with --dry-run which will process the pipeline but not execute anything. Combine with -v (, --verbose ) to show the commands which would have run verbatim which allows to better understand how pipelines actually works. Nothing to hide here. Use --no-run to not run the pipeline at all, this can be used to test the utilities' options. Pipeline environment variables can be passed/exported to or set for your pipeline by name or file with -e , --env and --env-file options. Environment variables are also loaded from dot env files named .env.dist and .env and processed in that order before the environment options. Use of --no-dot-env-files prevents automatic loading, --no-dot-env-dot-dist for the .env.dist file only. More information on pipelines environment variables in the environment section below. Help \u00b6 A full display of the pipelines utility options and arguments is available via -h , --help : usage: pipelines [<options>] --version | -h | --help pipelines [<options>] [--working-dir <path>] [--file <path>] [--basename <basename>] [--prefix <prefix>] [--verbatim] [--[no-|error-]keep] [--no-run] [(-e | --env) <variable>] [--env-file <path>] [--no-dot-env-files] [--no-dot-env-dot-dist] [--docker-client <package>] [--ssh] [--user[=<name|uid>[:<group|gid>]]] [--deploy mount | copy ] [--pipeline <id>] [(--step | --steps) <steps>] [--no-manual] [--trigger <ref>] [--no-cache] pipelines [<options>] --service <service> pipelines [<options>] --list | --show | --images | --show-pipelines | --show-services | --step-script[=(<id> | <step>[:<id>])] | --validate[=<path>] pipelines [<options>] --docker-client-pkgs pipelines [<options>] [--docker-list] [--docker-kill] [--docker-clean] [--docker-zap] Generic options -h, --help show usage and help information --version show version information -v, --verbose be more verbose, show more information and commands to be executed --dry-run do not execute commands, e.g. invoke docker or run containers, with --verbose show the commands that would have run w/o --dry-run -c <name>=<value> pass a configuration parameter to the command Pipeline runner options --basename <basename> set basename for pipelines file, defaults to 'bitbucket-pipelines.yml' --deploy mount|copy how files from the working directory are placed into the pipeline container: copy (default) working dir is copied into the container. stronger isolation as the pipeline scripts can change all files without side-effects in the working directory mount the working directory is mounted. fastest, no isolation --file <path> path to the pipelines file, overrides looking up the <basename> file from the current working directory, use '-' to read from stdin --trigger <ref> build trigger; <ref> can be either of: tag:<name>, branch:<name>, bookmark:<name> or pr:<branch-name>[:<destination-branch>] determines the pipeline to run --pipeline <id> run pipeline with <id>, use --list for a list of all pipeline ids available. overrides --trigger for the pipeline while keeping environment from --trigger. --step, --steps <steps> execute not all but this/these <steps>. all duplicates and orderings allowed, <steps> are a comma/space separated list of step and step ranges, e.g. 1 2 3; 1-3; 1,2-3; 3-1 or -1,3- and 1,1,3,3,2,2 --no-manual ignore manual steps, by default manual steps stop the pipeline execution when not the first step in invocation of a pipeline --verbatim only give verbatim output of the pipeline, do not display other information like which step currently executes, which image is in use ... --working-dir <path> run as if pipelines was started in <path> --no-run do not run the pipeline --prefix <prefix> use a different prefix for container names, default is 'pipelines' --no-cache disable step caches; docker always caches File information options --images list all images in file, in order of use, w/o duplicate names and exit --list list pipeline <id>s in file and exit --show show information about pipelines in file and exit --show-pipelines same as --show but with old --show output format without services and images / steps are summarized - one line for each pipeline --show-services show all defined services in use by pipeline steps and exit --validate[=<path>] schema-validate file, shows errors if any, exits; can be used more than once, exit status is non-zero on error --step-script[=(<id> | <step>[:<id>])] write the step-script of pipeline <id> and <step> to standard output and exit Environment control options -e, --env <variable> pass or set an environment <variable> for the docker container, just like a docker run, <variable> can be the name of a variable which adds the variable to the container as export or a variable definition with the name of the variable, the equal sign \"=\" and the value, e.g. --env NAME=<value> --env-file <path> pass variables from environment file to the docker container --no-dot-env-files do not pass .env.dist and .env files as environment files to docker --no-dot-env-dot-dist dot not pass .env.dist as environment file to docker only Keep options --keep always keep docker containers --error-keep keep docker containers if a step failed; outputs non-zero exit status and the id of the container kept and exit w/ container exec exit status --no-keep do not keep docker containers; default Container runner options --ssh ssh agent forwarding: if $SSH_AUTH_SOCK is set and accessible, mount SSH authentication socket read only and set SSH_AUTH_SOCK in the pipeline step container to the mount point. --user[=<name|uid>[:<group|gid>]] run pipeline step container as current or given <user>/<group>; overrides container default <user> - often root, (better) run rootless by default. Service runner options --service <service> runs <service> attached to the current shell and waits until the service exits, exit status is the one of the docker run service container; for testing services, run in a shell of its own or background Docker service options --docker-client <package> which docker client binary to use for the pipeline service 'docker' defaults to the 'docker-19.03.1-linux-static-x86_64' package --docker-client-pkgs list all docker client packages that ship with pipelines and exit Docker container maintenance options usage might leave containers on the system. either by interrupting a running pipeline step or by keeping the running containers (--keep, --error-keep) pipelines uses a <prefix> 'pipelines' by default, followed by '-' and a compound name based on step-number, step-name, pipeline id and image name for container names. the prefix can be set by the --prefix <prefix> option and argument. three options are built-in to monitor and interact with leftovers, if one or more of these are given, the following operations are executed in the order from top to down: --docker-list list prefixed containers --docker-kill kills prefixed containers --docker-clean remove (non-running) containers with pipelines prefix for ease of use: --docker-zap kill and remove all prefixed containers at once; no show/listing Less common options --debug flag for trouble-shooting (fatal) errors, warnings, notices and strict warnings; useful for trouble-shooting and bug-reports Usage Scenario \u00b6 Give your project and pipeline changes a quick test run from the staging area. As pipelines are normally executed far away, setting them up becomes cumbersome, the guide given in Bitbucket Pipelines documentation [BBPL-LOCAL-RUN] has some hints and is of help, but it is not about a bitbucket pipelines runner. This is where the pipelines command jumps in. The pipelines command closes the gap between local development and remote pipeline execution by executing any pipeline configured on your local development box. As long as Docker is accessible locally, the bitbucket-pipelines.yml file is parsed and it is taken care of to execute all steps and their commands within the container of choice. Pipelines YAML file parsing, container creation and script execution is done as closely as possible compared to the Atlassian Bitbucket Pipeline service. Environment variables can be passed into each pipeline as needed. You can even switch to a different CI/CD service like Github/Travis with little integration work fostering your agility and vendor independence. Features \u00b6 Features include: Dev Mode \u00b6 Pipeline from your working tree like never before. Pretend to be on any branch, tag or bookmark ( --trigger ) even in a different repository or none at all. Check if the reference matches a pipeline or just run the default (default) or a specific one ( --list , --pipeline ). Use a different pipelines file ( --file ) or swap the \"repository\" by changing the working directory ( --working-dir <path> ). If a pipeline step fails, the steps container can be kept for further inspection on error with the --error-keep option. The container id is shown then which makes it easy to spawn a shell inside: $ docker exec -it $ID /bin/sh Containers can be always kept for debugging and manual testing of a pipeline with --keep and with the said --error-keep on error only. Kept containers are re-used by their name regardless of any --keep (, --error-keep ) option. Continue on a (failed) step with the --steps <steps> argument, the <steps> option can be any step number or sequence ( 1-3 ), separate multiple with comma ( 3-,1-2 ), you can even repeat steps or reverse order ( 4,3,2,1 ). For example, if the second step failed, continue with use of --steps 2- to re-run the second and all following steps ( --steps 2 or --step 2 will run only the next step; to do a step-by-step approach). Afterwards manage left overs with --docker-list|kill|clean or clean up with --docker-zap . Debugging options to dream for; benefit from the local build, the pipeline container. Container Isolation \u00b6 There is one container per step, like it is on Bitbucket. Files are isolated by being copied into the container before the pipeline step script is executed (implicit --deploy copy ). Alternatively files can be mounted into the container instead with --deploy mount which normally is faster on Linux, but the working tree might become changed by the container script which causes side-effect that may be unwanted. Docker runs system-wide and containers do not isolate users (e.g. root is root). Better with --deploy mount (and peace of mind) is using Docker in rootless mode where files manipulated in the pipeline container are accessible to the own user account (like root is your user automatically mapped). Further reading: How-To Rootless Pipelines Pipeline Integration \u00b6 Export files from the pipeline by making use of artifacts, these are copied back into the working tree while in (implicit) --deploy copy mode. Artifacts' files are always created by the user running pipelines. This also (near) perfectly emulates the file format artifacts section with the benefit/downside that you might want to prepare a clean build in a pipeline step script while you can keep artifacts from pipelines locally. This is a trade-off that has turned out to be acceptable over the years. wrap pipelines in a script for clean checkouts or wait for future options to stage first ( git-deployment feature). In any case, control your build first of all. Ready for Offline \u00b6 On the plane? Riding Deutsche Bahn? Or just a rainy day on a remote location with broken net? Coding while abroad? Or just Bitbucket down again? Before going into offline mode, read about Working Offline you'll love it. Services? Check! \u00b6 The local pipeline runner runs service containers on your local box/system (that is your pipelines' host). This is similar to use services and databases in Bitbucket Pipelines [BBPL-SRV]. Even before any pipeline step makes use of a service, a service definition can already be tested with the --service option turning setting up services in pipelines into a new experience. A good way to test service definitions and to get an impression on additional resources being consumed. Further reading: Working with Pipeline Services Default Image \u00b6 The pipelines command uses the default image like Bitbucket Pipelines does (\" atlassian/default-image \"). Get started out of the box, but keep in mind it has roughly 1.4 GB. Pipelines inside Pipeline \u00b6 As a special feature and by default pipelines mounts the docker socket into each container (on systems where the socket is available). This allows to launch pipelines from a pipeline as long as pipelines and the Docker client is available in the pipelines' container. pipelines will take care of the Docker client as /usr/bin/docker as long as the pipeline has the docker service ( services: [docker] ). This feature is similar to run Docker commands in Bitbucket Pipelines [BBPL-DCK]. The pipelines inside pipeline feature serves pipelines itself well for integration testing the projects build. In combination with --deploy mount , the original working-directory is mounted from the host (again). Additional protection against endless loops by recursion is implemented to prevent accidental pipelines inside pipeline invocations that would be endlessly on-going. Further reading: How-To Docker Client Binary Packages for Pipelines Environment \u00b6 Pipelines mimics \"all\" of the Bitbucket Pipeline in-container environment variables [BBPL-ENV], also known as environment parameters: BITBUCKET_BOOKMARK - conditionally set by --trigger BITBUCKET_BUILD_NUMBER - always set to \" 0 \" BITBUCKET_BRANCH - conditionally set by --trigger BITBUCKET_CLONE_DIR - always set to deploy point in container BITBUCKET_COMMIT - faux as no revision triggers a build; always set to \" 0000000000000000000000000000000000000000 \" BITBUCKET_REPO_OWNER - current username from environment or if not available \" nobody \" BITBUCKET_REPO_SLUG - base name of project directory BITBUCKET_TAG - conditionally set by --trigger CI - always set to \" true \" All of these (but not BITBUCKET_CLONE_DIR ) can be set within the environment pipelines runs in and are taken over into container environment. Example: $ BITBUCKET_BUILD_NUMBER=123 pipelines # build no. 123 More information on (Bitbucket) pipelines environment variables can be found in the Pipelines Environment Variable Usage Reference . Additionally pipelines sets some environment variables for introspection: PIPELINES_CONTAINER_NAME - name of the container itself PIPELINES_ID - <id> of the pipeline that currently runs PIPELINES_IDS - list of space separated md5 hashes of so far running <id> s. used to detect pipelines inside pipeline recursion, preventing execution until system failure. PIPELINES_PARENT_CONTAINER_NAME - name of the container name if it was already set when the pipeline started (pipelines inside pipeline \"pip\"). PIPELINES_PIP_CONTAINER_NAME - name of the first (initial) pipeline container. Used by pipelines inside pipelines (\"pip\"). PIPELINES_PROJECT_PATH - path of the original project as if it would be used for --deploy with copy or mount so that it is possible inside a pipeline to do --deploy mount when the current container did not mount. A mount always requires the path of the project directory on the system running pipelines. With no existing mount (e.g. --deploy copy ) it would otherwise be unknown. Manipulating this parameter within a pipeline leads to undefined behaviour and can have system security implications. These environment variables are managed by pipelines itself. Some of them can be injected which can lead to undefined behaviour and can have system security implications as well. Next to these special purpose environment variables, any other environment variable can be imported into or set in the container via the -e , --env and --env-file options. These behave exactly as documented for the docker run command [DCK-RN]. Instead of specifying custom environment parameters for each invocation, pipelines by default automatically uses the .env.dist and .env files from each project supporting the same file-format for environment variables as docker. Exit Status \u00b6 Exit status on success is 0 (zero). A non zero exit status denotes an error: 1 : An argument supplied (also a missing one) caused the error. 2 : An error is caused by the system not being able to fulfill the command (e.g. a file could not be read). 127: Running pipelines inside pipelines failed detecting an endless loop. Example \u00b6 Not finding a file might cause exit status 2 (two) on error because a file is not found, however with a switch like --show the exit status might still be 1 (one) as there was an error showing that the file does not exists (indirectly) and the error is more prominently showing all pipelines of that file. Details \u00b6 Requirements | User Tests | Installation | Known Bugs | Todo Requirements \u00b6 Pipelines works best on a POSIX compatible system having a PHP runtime. Docker needs to be available locally as docker command as it is used to run the pipelines. Rootless Docker is supported. A recent PHP version is favored, the pipelines command needs PHP to run. It should work with PHP 5.3.3+. A development environment should be PHP 7+, this is especially suggested for future releases. PHP 8+ is supported as well. Installing the PHP YAML extension [PHP-YAML] is highly recommended as it does greatly improve parsing the pipelines file which is otherwise with a YAML parser on it's own as a fall-back and is not bad at all. There are subtle differences between these parsers, so why not have both at hand? User Tests \u00b6 Successful use on Ubuntu 16.04 LTS, Ubuntu 18.04 LTS, Ubuntu 20.04 LTS and Mac OS X Sierra and High Sierra with PHP and Docker installed. Known Bugs \u00b6 The command \" : \" in pipelines exec layer is never really executed but emulated having exit status 0 and no standard or error output. It is intended for pipelines testing. Brace expansion (used for glob patterns with braces) is known to fail in some cases. This could affect matching pipelines, collecting asset paths and did affect building the phar file. For the first two, this has never been reported nor experienced, for building the phar file the workaround was to entail the larger parts of the pattern. The sf2yaml based parser does not support the backslash at the end of a line to fold without a space with double quoted strings . The libyaml based parser does not support dots (\" . \") in anchor names. The libyaml based parser does not support folded scalar (\" > \") as block style indicator . Suggested workaround is to use literal style (\" | \"). NUL bytes (\" \\0 \") are not supported verbatim in step-scripts due to defense-in-depth protection on passthru in the PHP-runtime to prevent Null character injection. When the project directory is large (e.g. a couple of GBs) and copying it into the pipeline container, it may appear as if pipelines hangs as the copying operation is ongoing and taking a long time. Pressing ctrl + c may stop pipelines but not the copying operation. Kill the process of the copy operation ( tar pipe to docker cp ) to stop the operation. Installation \u00b6 Phar (Download) | Composer | Phive | Source (also w/ Phar) | Full Project (Development) Installation is available by downloading the phar archive from Github, via Composer/Packagist or with Phive and it should always work from source which includes building the phar file. Download the PHAR (PHP Archive) File \u00b6 Downloads are available on Github. To obtain the latest released version, use the following URL: https://github.com/ktomk/pipelines/releases/latest/download/pipelines.phar Rename the phar file to just \" pipelines \", set the executable bit and move it into a directory where executables are found. Downloads from Github are available since version 0.0.4. All releases are listed on the following website: https://github.com/ktomk/pipelines/releases Install with Composer \u00b6 Suggested is to install it globally (and to have the global composer vendor/bin in $PATH) so that it can be called with ease and there are no dependencies in a local project: $ composer global require ktomk/pipelines This will automatically install the latest available version. Verify the installation by invoking pipelines and output the version: $ pipelines --version pipelines version 0.0.19 To uninstall remove the package: $ composer global remove ktomk/pipelines Take a look at Composer from getcomposer.org [COMPOSER], a Dependency Manager for PHP . Pipelines has support for composer based installations, which might include upstream patches (composer 2 is supported, incl. upstream patches). Install with Phive \u00b6 Perhaps the most easy way to install when phive is available: $ phive install pipelines Even if your PHP version does not have the Yaml extension this should work out of the box. If you use composer and you're a PHP aficionado, dig into phive for your systems and workflow. Take a look at Phive from phar.io [PHARIO], the PHAR Installation and Verification Environment (PHIVE) . Pipelines has full support for phar.io/phar based installations which includes support for the phive utility including upstream patches. Install from Source \u00b6 To install from source, checkout the source repository and symlink the executable file bin/pipelines into a segment of $PATH, e.g. your $HOME/bin directory or similar. Verify the installation by invoking pipelines and output the version: $ pipelines --version pipelines version 0.0.19 # NOTE: the version is exemplary To create a phar archive from sources, invoke from within the projects root directory the build script: $ composer build building 0.0.19-1-gbba5a43 ... pipelines version 0.0.19-1-gbba5a43 file.....: build/pipelines.phar size.....: 240 191 bytes SHA-1....: 9F118A276FC755C21EA548A77A9DBAF769B93524 SHA-256..: 0C38CBBB12E10E80F37ECA5C4C335BF87111AC8E8D0490D38683BB3DA7E82DEF file.....: 1.1.0 api......: 1.1.1 extension: 2.0.2 php......: 7.2.16-[...] uname....: [...] count....: 62 file(s) signature: SHA-1 E638E7B56FAAD7171AE9838DF6074714630BD486 The phar archive then is (as written in the output of the build): build/pipelines.phar Check the version by invoking it: $ build/pipelines.phar --version pipelines version 0.0.19-1-gbba5a43 # NOTE: the version is exemplary Php Compatibility and Undefined Behaviour \u00b6 The pipelines project aims to support php 5.3.3 up to php 8.1. Using any of its PHP functions or methods with named parameters falls into undefined behaviour. Reproducible Phar Builds \u00b6 The pipelines project practices reproducible builds since it's first phar build. The build is self-contained, which means that the repository ships with all required files to build with only little dependencies: PHP (for build.php ) Composer Git Reproducible builds of the phar file would be incomplete without the fine work from the composer projects phar-utils (Seldaek/Jordi Boggiano) which is forked by the pipelines project in Timestamps.php by keeping the original license with the file (MIT), providing bug-fixes to upstream under that license (see Phar-Utils #2 and Phar-Utils #3 ). This file is used to set the timestamps inside the phar file to that of the release as otherwise those would be at the time of build. This is the same as the Composer project does (see Composer #3927 ). Additionally in the pipelines project that file is used to change the access permissions of the files in the phar. That is because across PHP versions the behaviour has changed so the build is kept backwards and forwards compatible. As this has been noticed later in the projects' history, the build might show different binaries depending on which PHP version is used (see PHP #77022 and PHP #79082 ) and the patch state of the timestamps file. Install Full Project For Development \u00b6 When working with git , clone the repository and then invoke composer install . The project is setup for development then. Alternatively it's possible to do the same via composer directly: $ composer create-project --prefer-source --keep-vcs ktomk/pipelines ... $ cd pipelines Verify the installation by invoking the local build: $ composer ci Should exit with status 0 when it went fine, non 0 when there is an issue. Composer tells which individual script did fail. Follow the instructions in Install from Source to use the development version for pipelines . Todo \u00b6 Support for private Docker repositories Inject docker client if docker service is enabled Run specific steps of a pipeline (only) to put the user back into command on errors w/o re-running everything Stop at manual steps ( --no-manual to override) Support BITBUCKET_PR_DESTINATION_BRANCH with --trigger pr:<source>:<destination> Pipeline services Run as current user with --user ( --deploy mount should not enforce the container default user [often \"root\"] for project file operations any longer), however the Docker utility still requires you (the current user) to be root like, so technically there is little win (see Rootless Pipelines for what works better in this regard) Have caches on a per-project basis Copy local composer cache into container for better (offline) usage in PHP projects (see Populate Caches ) Run scripts with /bin/bash if available ( #17 ) ( bash-runner feature) Support for BITBUCKET_DOCKER_HOST_INTERNAL environment variable / host.docker.internal hostname within pipelines Count BITBUCKET_BUILD_NUMBER on a per project basis ( build-number feature) Option to not mount docker.sock Limit projects' paths below $HOME , excluding dot . directory children. More accessible offline preparation (e.g. --docker-pull-images , --go-offline or similar) Check Docker existence before running a pipeline Pipes support ( pipe feature) Show scripts with pipe/s Fake run script with pipe/s showing information Create test/demo pipe Run script with pipe/s Write about differences from Bitbucket Pipelines Write about the file format support/limitations Pipeline file properties support: step.after-script ( after-script feature) step.trigger ( --steps / --no-manual options) step.caches (to disable use --no-cache option) definitions services ( services feature) caches ( caches feature) step.condition ( #13 ) clone ( git-deployment feature) max-time (never needed this for local run) size (likely neglected for local run, limited support for Rootless Pipelines ) Get VCS revision from working directory ( git-deployment feature) Use a different project directory --project-dir <path> to specify the root path to deploy into the container, which currently is the working directory ( --working-dir <path> works already) Run on a specific revision, reference it ( --revision <ref> ); needs a clean VCS checkout into a temporary folder which then should be copied into the container ( git-deployment feature) Override the default image name ( --default-image <name> ; never needed this for local run) References \u00b6 [BBPL]: https://confluence.atlassian.com/bitbucket/configure-bitbucket-pipelines-yml-792298910.html [BBPL-ENV]: https://confluence.atlassian.com/bitbucket/environment-variables-794502608.html [BBPL-LOCAL-RUN]: https://confluence.atlassian.com/bitbucket/debug-your-pipelines-locally-with-docker-838273569.html [BBPL-DCK]: https://confluence.atlassian.com/bitbucket/run-docker-commands-in-bitbucket-pipelines-879254331.html [BBPL-SRV]: https://confluence.atlassian.com/bitbucket/use-services-and-databases-in-bitbucket-pipelines-874786688.html [COMPOSER]: https://getcomposer.org/ [DCK-RN]: https://docs.docker.com/engine/reference/commandline/run/ [PHARIO]: https://phar.io/ [PHP-YAML]: https://pecl.php.net/package/yaml","title":"Pipelines"},{"location":"index.html#pipelines","text":"","title":"Pipelines"},{"location":"index.html#run-bitbucket-pipelines-wherever-they-dock","text":"Command line pipeline runner written in PHP. Available from Github or Packagist. Usage | Environment | Exit Status | Details | References","title":"Run Bitbucket Pipelines Wherever They Dock"},{"location":"index.html#usage","text":"From anywhere within a project or (Git) repository with a Bitbucket Pipeline file: $ pipelines Runs pipeline commands from bitbucket-pipelines.yml [BBPL]. Memory and time limits are ignored. Press ctrl + c to quit. The Bitbucket limit of 100 (previously 10) steps per pipeline is ignored. Exit status is from last pipeline script command, if a command fails the following script commands and steps are not executed. The default pipeline is run, if there is no default pipeline in the file, pipelines tells it and exists with non-zero status. To execute a different pipeline use the --pipeline <id> option where <id> is one of the list by the --list option. Even more information about the pipelines is available via --show . Both --list and --show output and exit. Use --steps <steps> to specify which step(s) to execute (also in which order). If the next pipeline step has a manual trigger, pipelines stops the execution and outputs a short message on standard error giving info about the fact. Manual triggers can be ignored with the --no-manual option. Run the pipeline as if a tag/branch or bookmark has been pushed with --trigger <ref> where <ref> is tag:<name> , branch:<name> , bookmark:<name> or pr:<branch-name>[:<destination-branch>] . If there is no tag, branch, bookmark or pull-request pipeline with that name, the name is compared against the patterns of the referenced type and if found, that pipeline is run. Otherwise the default pipeline is run, if there is no default pipeline, no pipeline at all is run and the command exits with non-zero status. --pipeline and --trigger can be used together, --pipeline overrides pipeline from --trigger but --trigger still influences the container environment variables. To specify a different file use the --basename <basename> or --file <path> option and/or set the working directory --working-dir <path> in which the file is looked for unless an absolute path is set by --file <path> . By default pipelines operates on the current working tree which is copied into the container to isolate running the pipeline from the working directory (implicit --deploy copy ). Alternatively the working directory can be mounted into the pipelines container by using --deploy mount . Use --keep flag to keep containers after the pipeline has finished for further inspection. By default all containers are destroyed. Sometimes for development it is interesting to keep containers on error only, the --error-keep flag is for that. In any case, if a pipeline runs again and it finds an existing container with the same name (generated by the pipeline name etc.), the existing container will be re-used. This can be very useful to re-iterate quickly. Manage leftover containers with --docker-list showing all pipeline containers, --docker-kill to kill running containers and --docker-clean to remove stopped pipeline containers. Use in combination to fully clean, e.g.: $ pipelines --docker-list --docker-kill --docker-clean Or just run for a more shy clean-up: $ pipelines --docker-zap to kill and remove all pipeline containers (w/o showing a list) first. \"zap\" is pipelines \"make clean\" equivalent for --keep . All containers run by pipelines are labeled to ease maintaining them. Validate your bitbucket-pipelines.yml file with --show which highlights errors found. For schema-validation use --validate [<file>] . Schema validation might show errors that are not an issue when executing a pipeline ( --show and/or --dry-run is better for that) but validates against a schema which is aligned with the one that Atlassian/ Bitbucket provides (the schema is more lax compared to upstream for the cases known to offer a better practical experience). E.g. use it for checks in your CI pipeline or linting files before push in a pre-commit hook or your local build. Inspect your pipeline with --dry-run which will process the pipeline but not execute anything. Combine with -v (, --verbose ) to show the commands which would have run verbatim which allows to better understand how pipelines actually works. Nothing to hide here. Use --no-run to not run the pipeline at all, this can be used to test the utilities' options. Pipeline environment variables can be passed/exported to or set for your pipeline by name or file with -e , --env and --env-file options. Environment variables are also loaded from dot env files named .env.dist and .env and processed in that order before the environment options. Use of --no-dot-env-files prevents automatic loading, --no-dot-env-dot-dist for the .env.dist file only. More information on pipelines environment variables in the environment section below.","title":"Usage"},{"location":"index.html#help","text":"A full display of the pipelines utility options and arguments is available via -h , --help : usage: pipelines [<options>] --version | -h | --help pipelines [<options>] [--working-dir <path>] [--file <path>] [--basename <basename>] [--prefix <prefix>] [--verbatim] [--[no-|error-]keep] [--no-run] [(-e | --env) <variable>] [--env-file <path>] [--no-dot-env-files] [--no-dot-env-dot-dist] [--docker-client <package>] [--ssh] [--user[=<name|uid>[:<group|gid>]]] [--deploy mount | copy ] [--pipeline <id>] [(--step | --steps) <steps>] [--no-manual] [--trigger <ref>] [--no-cache] pipelines [<options>] --service <service> pipelines [<options>] --list | --show | --images | --show-pipelines | --show-services | --step-script[=(<id> | <step>[:<id>])] | --validate[=<path>] pipelines [<options>] --docker-client-pkgs pipelines [<options>] [--docker-list] [--docker-kill] [--docker-clean] [--docker-zap] Generic options -h, --help show usage and help information --version show version information -v, --verbose be more verbose, show more information and commands to be executed --dry-run do not execute commands, e.g. invoke docker or run containers, with --verbose show the commands that would have run w/o --dry-run -c <name>=<value> pass a configuration parameter to the command Pipeline runner options --basename <basename> set basename for pipelines file, defaults to 'bitbucket-pipelines.yml' --deploy mount|copy how files from the working directory are placed into the pipeline container: copy (default) working dir is copied into the container. stronger isolation as the pipeline scripts can change all files without side-effects in the working directory mount the working directory is mounted. fastest, no isolation --file <path> path to the pipelines file, overrides looking up the <basename> file from the current working directory, use '-' to read from stdin --trigger <ref> build trigger; <ref> can be either of: tag:<name>, branch:<name>, bookmark:<name> or pr:<branch-name>[:<destination-branch>] determines the pipeline to run --pipeline <id> run pipeline with <id>, use --list for a list of all pipeline ids available. overrides --trigger for the pipeline while keeping environment from --trigger. --step, --steps <steps> execute not all but this/these <steps>. all duplicates and orderings allowed, <steps> are a comma/space separated list of step and step ranges, e.g. 1 2 3; 1-3; 1,2-3; 3-1 or -1,3- and 1,1,3,3,2,2 --no-manual ignore manual steps, by default manual steps stop the pipeline execution when not the first step in invocation of a pipeline --verbatim only give verbatim output of the pipeline, do not display other information like which step currently executes, which image is in use ... --working-dir <path> run as if pipelines was started in <path> --no-run do not run the pipeline --prefix <prefix> use a different prefix for container names, default is 'pipelines' --no-cache disable step caches; docker always caches File information options --images list all images in file, in order of use, w/o duplicate names and exit --list list pipeline <id>s in file and exit --show show information about pipelines in file and exit --show-pipelines same as --show but with old --show output format without services and images / steps are summarized - one line for each pipeline --show-services show all defined services in use by pipeline steps and exit --validate[=<path>] schema-validate file, shows errors if any, exits; can be used more than once, exit status is non-zero on error --step-script[=(<id> | <step>[:<id>])] write the step-script of pipeline <id> and <step> to standard output and exit Environment control options -e, --env <variable> pass or set an environment <variable> for the docker container, just like a docker run, <variable> can be the name of a variable which adds the variable to the container as export or a variable definition with the name of the variable, the equal sign \"=\" and the value, e.g. --env NAME=<value> --env-file <path> pass variables from environment file to the docker container --no-dot-env-files do not pass .env.dist and .env files as environment files to docker --no-dot-env-dot-dist dot not pass .env.dist as environment file to docker only Keep options --keep always keep docker containers --error-keep keep docker containers if a step failed; outputs non-zero exit status and the id of the container kept and exit w/ container exec exit status --no-keep do not keep docker containers; default Container runner options --ssh ssh agent forwarding: if $SSH_AUTH_SOCK is set and accessible, mount SSH authentication socket read only and set SSH_AUTH_SOCK in the pipeline step container to the mount point. --user[=<name|uid>[:<group|gid>]] run pipeline step container as current or given <user>/<group>; overrides container default <user> - often root, (better) run rootless by default. Service runner options --service <service> runs <service> attached to the current shell and waits until the service exits, exit status is the one of the docker run service container; for testing services, run in a shell of its own or background Docker service options --docker-client <package> which docker client binary to use for the pipeline service 'docker' defaults to the 'docker-19.03.1-linux-static-x86_64' package --docker-client-pkgs list all docker client packages that ship with pipelines and exit Docker container maintenance options usage might leave containers on the system. either by interrupting a running pipeline step or by keeping the running containers (--keep, --error-keep) pipelines uses a <prefix> 'pipelines' by default, followed by '-' and a compound name based on step-number, step-name, pipeline id and image name for container names. the prefix can be set by the --prefix <prefix> option and argument. three options are built-in to monitor and interact with leftovers, if one or more of these are given, the following operations are executed in the order from top to down: --docker-list list prefixed containers --docker-kill kills prefixed containers --docker-clean remove (non-running) containers with pipelines prefix for ease of use: --docker-zap kill and remove all prefixed containers at once; no show/listing Less common options --debug flag for trouble-shooting (fatal) errors, warnings, notices and strict warnings; useful for trouble-shooting and bug-reports","title":"Help"},{"location":"index.html#usage-scenario","text":"Give your project and pipeline changes a quick test run from the staging area. As pipelines are normally executed far away, setting them up becomes cumbersome, the guide given in Bitbucket Pipelines documentation [BBPL-LOCAL-RUN] has some hints and is of help, but it is not about a bitbucket pipelines runner. This is where the pipelines command jumps in. The pipelines command closes the gap between local development and remote pipeline execution by executing any pipeline configured on your local development box. As long as Docker is accessible locally, the bitbucket-pipelines.yml file is parsed and it is taken care of to execute all steps and their commands within the container of choice. Pipelines YAML file parsing, container creation and script execution is done as closely as possible compared to the Atlassian Bitbucket Pipeline service. Environment variables can be passed into each pipeline as needed. You can even switch to a different CI/CD service like Github/Travis with little integration work fostering your agility and vendor independence.","title":"Usage Scenario"},{"location":"index.html#features","text":"Features include:","title":"Features"},{"location":"index.html#dev-mode","text":"Pipeline from your working tree like never before. Pretend to be on any branch, tag or bookmark ( --trigger ) even in a different repository or none at all. Check if the reference matches a pipeline or just run the default (default) or a specific one ( --list , --pipeline ). Use a different pipelines file ( --file ) or swap the \"repository\" by changing the working directory ( --working-dir <path> ). If a pipeline step fails, the steps container can be kept for further inspection on error with the --error-keep option. The container id is shown then which makes it easy to spawn a shell inside: $ docker exec -it $ID /bin/sh Containers can be always kept for debugging and manual testing of a pipeline with --keep and with the said --error-keep on error only. Kept containers are re-used by their name regardless of any --keep (, --error-keep ) option. Continue on a (failed) step with the --steps <steps> argument, the <steps> option can be any step number or sequence ( 1-3 ), separate multiple with comma ( 3-,1-2 ), you can even repeat steps or reverse order ( 4,3,2,1 ). For example, if the second step failed, continue with use of --steps 2- to re-run the second and all following steps ( --steps 2 or --step 2 will run only the next step; to do a step-by-step approach). Afterwards manage left overs with --docker-list|kill|clean or clean up with --docker-zap . Debugging options to dream for; benefit from the local build, the pipeline container.","title":"Dev Mode"},{"location":"index.html#container-isolation","text":"There is one container per step, like it is on Bitbucket. Files are isolated by being copied into the container before the pipeline step script is executed (implicit --deploy copy ). Alternatively files can be mounted into the container instead with --deploy mount which normally is faster on Linux, but the working tree might become changed by the container script which causes side-effect that may be unwanted. Docker runs system-wide and containers do not isolate users (e.g. root is root). Better with --deploy mount (and peace of mind) is using Docker in rootless mode where files manipulated in the pipeline container are accessible to the own user account (like root is your user automatically mapped). Further reading: How-To Rootless Pipelines","title":"Container Isolation"},{"location":"index.html#pipeline-integration","text":"Export files from the pipeline by making use of artifacts, these are copied back into the working tree while in (implicit) --deploy copy mode. Artifacts' files are always created by the user running pipelines. This also (near) perfectly emulates the file format artifacts section with the benefit/downside that you might want to prepare a clean build in a pipeline step script while you can keep artifacts from pipelines locally. This is a trade-off that has turned out to be acceptable over the years. wrap pipelines in a script for clean checkouts or wait for future options to stage first ( git-deployment feature). In any case, control your build first of all.","title":"Pipeline Integration"},{"location":"index.html#ready-for-offline","text":"On the plane? Riding Deutsche Bahn? Or just a rainy day on a remote location with broken net? Coding while abroad? Or just Bitbucket down again? Before going into offline mode, read about Working Offline you'll love it.","title":"Ready for Offline"},{"location":"index.html#services-check","text":"The local pipeline runner runs service containers on your local box/system (that is your pipelines' host). This is similar to use services and databases in Bitbucket Pipelines [BBPL-SRV]. Even before any pipeline step makes use of a service, a service definition can already be tested with the --service option turning setting up services in pipelines into a new experience. A good way to test service definitions and to get an impression on additional resources being consumed. Further reading: Working with Pipeline Services","title":"Services? Check!"},{"location":"index.html#default-image","text":"The pipelines command uses the default image like Bitbucket Pipelines does (\" atlassian/default-image \"). Get started out of the box, but keep in mind it has roughly 1.4 GB.","title":"Default Image"},{"location":"index.html#pipelines-inside-pipeline","text":"As a special feature and by default pipelines mounts the docker socket into each container (on systems where the socket is available). This allows to launch pipelines from a pipeline as long as pipelines and the Docker client is available in the pipelines' container. pipelines will take care of the Docker client as /usr/bin/docker as long as the pipeline has the docker service ( services: [docker] ). This feature is similar to run Docker commands in Bitbucket Pipelines [BBPL-DCK]. The pipelines inside pipeline feature serves pipelines itself well for integration testing the projects build. In combination with --deploy mount , the original working-directory is mounted from the host (again). Additional protection against endless loops by recursion is implemented to prevent accidental pipelines inside pipeline invocations that would be endlessly on-going. Further reading: How-To Docker Client Binary Packages for Pipelines","title":"Pipelines inside Pipeline"},{"location":"index.html#environment","text":"Pipelines mimics \"all\" of the Bitbucket Pipeline in-container environment variables [BBPL-ENV], also known as environment parameters: BITBUCKET_BOOKMARK - conditionally set by --trigger BITBUCKET_BUILD_NUMBER - always set to \" 0 \" BITBUCKET_BRANCH - conditionally set by --trigger BITBUCKET_CLONE_DIR - always set to deploy point in container BITBUCKET_COMMIT - faux as no revision triggers a build; always set to \" 0000000000000000000000000000000000000000 \" BITBUCKET_REPO_OWNER - current username from environment or if not available \" nobody \" BITBUCKET_REPO_SLUG - base name of project directory BITBUCKET_TAG - conditionally set by --trigger CI - always set to \" true \" All of these (but not BITBUCKET_CLONE_DIR ) can be set within the environment pipelines runs in and are taken over into container environment. Example: $ BITBUCKET_BUILD_NUMBER=123 pipelines # build no. 123 More information on (Bitbucket) pipelines environment variables can be found in the Pipelines Environment Variable Usage Reference . Additionally pipelines sets some environment variables for introspection: PIPELINES_CONTAINER_NAME - name of the container itself PIPELINES_ID - <id> of the pipeline that currently runs PIPELINES_IDS - list of space separated md5 hashes of so far running <id> s. used to detect pipelines inside pipeline recursion, preventing execution until system failure. PIPELINES_PARENT_CONTAINER_NAME - name of the container name if it was already set when the pipeline started (pipelines inside pipeline \"pip\"). PIPELINES_PIP_CONTAINER_NAME - name of the first (initial) pipeline container. Used by pipelines inside pipelines (\"pip\"). PIPELINES_PROJECT_PATH - path of the original project as if it would be used for --deploy with copy or mount so that it is possible inside a pipeline to do --deploy mount when the current container did not mount. A mount always requires the path of the project directory on the system running pipelines. With no existing mount (e.g. --deploy copy ) it would otherwise be unknown. Manipulating this parameter within a pipeline leads to undefined behaviour and can have system security implications. These environment variables are managed by pipelines itself. Some of them can be injected which can lead to undefined behaviour and can have system security implications as well. Next to these special purpose environment variables, any other environment variable can be imported into or set in the container via the -e , --env and --env-file options. These behave exactly as documented for the docker run command [DCK-RN]. Instead of specifying custom environment parameters for each invocation, pipelines by default automatically uses the .env.dist and .env files from each project supporting the same file-format for environment variables as docker.","title":"Environment"},{"location":"index.html#exit-status","text":"Exit status on success is 0 (zero). A non zero exit status denotes an error: 1 : An argument supplied (also a missing one) caused the error. 2 : An error is caused by the system not being able to fulfill the command (e.g. a file could not be read). 127: Running pipelines inside pipelines failed detecting an endless loop.","title":"Exit Status"},{"location":"index.html#example","text":"Not finding a file might cause exit status 2 (two) on error because a file is not found, however with a switch like --show the exit status might still be 1 (one) as there was an error showing that the file does not exists (indirectly) and the error is more prominently showing all pipelines of that file.","title":"Example"},{"location":"index.html#details","text":"Requirements | User Tests | Installation | Known Bugs | Todo","title":"Details"},{"location":"index.html#requirements","text":"Pipelines works best on a POSIX compatible system having a PHP runtime. Docker needs to be available locally as docker command as it is used to run the pipelines. Rootless Docker is supported. A recent PHP version is favored, the pipelines command needs PHP to run. It should work with PHP 5.3.3+. A development environment should be PHP 7+, this is especially suggested for future releases. PHP 8+ is supported as well. Installing the PHP YAML extension [PHP-YAML] is highly recommended as it does greatly improve parsing the pipelines file which is otherwise with a YAML parser on it's own as a fall-back and is not bad at all. There are subtle differences between these parsers, so why not have both at hand?","title":"Requirements"},{"location":"index.html#user-tests","text":"Successful use on Ubuntu 16.04 LTS, Ubuntu 18.04 LTS, Ubuntu 20.04 LTS and Mac OS X Sierra and High Sierra with PHP and Docker installed.","title":"User Tests"},{"location":"index.html#known-bugs","text":"The command \" : \" in pipelines exec layer is never really executed but emulated having exit status 0 and no standard or error output. It is intended for pipelines testing. Brace expansion (used for glob patterns with braces) is known to fail in some cases. This could affect matching pipelines, collecting asset paths and did affect building the phar file. For the first two, this has never been reported nor experienced, for building the phar file the workaround was to entail the larger parts of the pattern. The sf2yaml based parser does not support the backslash at the end of a line to fold without a space with double quoted strings . The libyaml based parser does not support dots (\" . \") in anchor names. The libyaml based parser does not support folded scalar (\" > \") as block style indicator . Suggested workaround is to use literal style (\" | \"). NUL bytes (\" \\0 \") are not supported verbatim in step-scripts due to defense-in-depth protection on passthru in the PHP-runtime to prevent Null character injection. When the project directory is large (e.g. a couple of GBs) and copying it into the pipeline container, it may appear as if pipelines hangs as the copying operation is ongoing and taking a long time. Pressing ctrl + c may stop pipelines but not the copying operation. Kill the process of the copy operation ( tar pipe to docker cp ) to stop the operation.","title":"Known Bugs"},{"location":"index.html#installation","text":"Phar (Download) | Composer | Phive | Source (also w/ Phar) | Full Project (Development) Installation is available by downloading the phar archive from Github, via Composer/Packagist or with Phive and it should always work from source which includes building the phar file.","title":"Installation"},{"location":"index.html#download-the-phar-php-archive-file","text":"Downloads are available on Github. To obtain the latest released version, use the following URL: https://github.com/ktomk/pipelines/releases/latest/download/pipelines.phar Rename the phar file to just \" pipelines \", set the executable bit and move it into a directory where executables are found. Downloads from Github are available since version 0.0.4. All releases are listed on the following website: https://github.com/ktomk/pipelines/releases","title":"Download the PHAR (PHP Archive) File"},{"location":"index.html#install-with-composer","text":"Suggested is to install it globally (and to have the global composer vendor/bin in $PATH) so that it can be called with ease and there are no dependencies in a local project: $ composer global require ktomk/pipelines This will automatically install the latest available version. Verify the installation by invoking pipelines and output the version: $ pipelines --version pipelines version 0.0.19 To uninstall remove the package: $ composer global remove ktomk/pipelines Take a look at Composer from getcomposer.org [COMPOSER], a Dependency Manager for PHP . Pipelines has support for composer based installations, which might include upstream patches (composer 2 is supported, incl. upstream patches).","title":"Install with Composer"},{"location":"index.html#install-with-phive","text":"Perhaps the most easy way to install when phive is available: $ phive install pipelines Even if your PHP version does not have the Yaml extension this should work out of the box. If you use composer and you're a PHP aficionado, dig into phive for your systems and workflow. Take a look at Phive from phar.io [PHARIO], the PHAR Installation and Verification Environment (PHIVE) . Pipelines has full support for phar.io/phar based installations which includes support for the phive utility including upstream patches.","title":"Install with Phive"},{"location":"index.html#install-from-source","text":"To install from source, checkout the source repository and symlink the executable file bin/pipelines into a segment of $PATH, e.g. your $HOME/bin directory or similar. Verify the installation by invoking pipelines and output the version: $ pipelines --version pipelines version 0.0.19 # NOTE: the version is exemplary To create a phar archive from sources, invoke from within the projects root directory the build script: $ composer build building 0.0.19-1-gbba5a43 ... pipelines version 0.0.19-1-gbba5a43 file.....: build/pipelines.phar size.....: 240 191 bytes SHA-1....: 9F118A276FC755C21EA548A77A9DBAF769B93524 SHA-256..: 0C38CBBB12E10E80F37ECA5C4C335BF87111AC8E8D0490D38683BB3DA7E82DEF file.....: 1.1.0 api......: 1.1.1 extension: 2.0.2 php......: 7.2.16-[...] uname....: [...] count....: 62 file(s) signature: SHA-1 E638E7B56FAAD7171AE9838DF6074714630BD486 The phar archive then is (as written in the output of the build): build/pipelines.phar Check the version by invoking it: $ build/pipelines.phar --version pipelines version 0.0.19-1-gbba5a43 # NOTE: the version is exemplary","title":"Install from Source"},{"location":"index.html#php-compatibility-and-undefined-behaviour","text":"The pipelines project aims to support php 5.3.3 up to php 8.1. Using any of its PHP functions or methods with named parameters falls into undefined behaviour.","title":"Php Compatibility and Undefined Behaviour"},{"location":"index.html#reproducible-phar-builds","text":"The pipelines project practices reproducible builds since it's first phar build. The build is self-contained, which means that the repository ships with all required files to build with only little dependencies: PHP (for build.php ) Composer Git Reproducible builds of the phar file would be incomplete without the fine work from the composer projects phar-utils (Seldaek/Jordi Boggiano) which is forked by the pipelines project in Timestamps.php by keeping the original license with the file (MIT), providing bug-fixes to upstream under that license (see Phar-Utils #2 and Phar-Utils #3 ). This file is used to set the timestamps inside the phar file to that of the release as otherwise those would be at the time of build. This is the same as the Composer project does (see Composer #3927 ). Additionally in the pipelines project that file is used to change the access permissions of the files in the phar. That is because across PHP versions the behaviour has changed so the build is kept backwards and forwards compatible. As this has been noticed later in the projects' history, the build might show different binaries depending on which PHP version is used (see PHP #77022 and PHP #79082 ) and the patch state of the timestamps file.","title":"Reproducible Phar Builds"},{"location":"index.html#install-full-project-for-development","text":"When working with git , clone the repository and then invoke composer install . The project is setup for development then. Alternatively it's possible to do the same via composer directly: $ composer create-project --prefer-source --keep-vcs ktomk/pipelines ... $ cd pipelines Verify the installation by invoking the local build: $ composer ci Should exit with status 0 when it went fine, non 0 when there is an issue. Composer tells which individual script did fail. Follow the instructions in Install from Source to use the development version for pipelines .","title":"Install Full Project For Development"},{"location":"index.html#todo","text":"Support for private Docker repositories Inject docker client if docker service is enabled Run specific steps of a pipeline (only) to put the user back into command on errors w/o re-running everything Stop at manual steps ( --no-manual to override) Support BITBUCKET_PR_DESTINATION_BRANCH with --trigger pr:<source>:<destination> Pipeline services Run as current user with --user ( --deploy mount should not enforce the container default user [often \"root\"] for project file operations any longer), however the Docker utility still requires you (the current user) to be root like, so technically there is little win (see Rootless Pipelines for what works better in this regard) Have caches on a per-project basis Copy local composer cache into container for better (offline) usage in PHP projects (see Populate Caches ) Run scripts with /bin/bash if available ( #17 ) ( bash-runner feature) Support for BITBUCKET_DOCKER_HOST_INTERNAL environment variable / host.docker.internal hostname within pipelines Count BITBUCKET_BUILD_NUMBER on a per project basis ( build-number feature) Option to not mount docker.sock Limit projects' paths below $HOME , excluding dot . directory children. More accessible offline preparation (e.g. --docker-pull-images , --go-offline or similar) Check Docker existence before running a pipeline Pipes support ( pipe feature) Show scripts with pipe/s Fake run script with pipe/s showing information Create test/demo pipe Run script with pipe/s Write about differences from Bitbucket Pipelines Write about the file format support/limitations Pipeline file properties support: step.after-script ( after-script feature) step.trigger ( --steps / --no-manual options) step.caches (to disable use --no-cache option) definitions services ( services feature) caches ( caches feature) step.condition ( #13 ) clone ( git-deployment feature) max-time (never needed this for local run) size (likely neglected for local run, limited support for Rootless Pipelines ) Get VCS revision from working directory ( git-deployment feature) Use a different project directory --project-dir <path> to specify the root path to deploy into the container, which currently is the working directory ( --working-dir <path> works already) Run on a specific revision, reference it ( --revision <ref> ); needs a clean VCS checkout into a temporary folder which then should be copied into the container ( git-deployment feature) Override the default image name ( --default-image <name> ; never needed this for local run)","title":"Todo"},{"location":"index.html#references","text":"[BBPL]: https://confluence.atlassian.com/bitbucket/configure-bitbucket-pipelines-yml-792298910.html [BBPL-ENV]: https://confluence.atlassian.com/bitbucket/environment-variables-794502608.html [BBPL-LOCAL-RUN]: https://confluence.atlassian.com/bitbucket/debug-your-pipelines-locally-with-docker-838273569.html [BBPL-DCK]: https://confluence.atlassian.com/bitbucket/run-docker-commands-in-bitbucket-pipelines-879254331.html [BBPL-SRV]: https://confluence.atlassian.com/bitbucket/use-services-and-databases-in-bitbucket-pipelines-874786688.html [COMPOSER]: https://getcomposer.org/ [DCK-RN]: https://docs.docker.com/engine/reference/commandline/run/ [PHARIO]: https://phar.io/ [PHP-YAML]: https://pecl.php.net/package/yaml","title":"References"},{"location":"CHANGELOG.html","text":"Change Log \u00b6 All notable changes to Pipelines will be documented in this file. The format is based on Keep a Changelog and Pipelines adheres to Semantic Versioning . 0.0.67 - 2022-07-17 \u00b6 Change \u00b6 Environment variable definitions in -e , --env and --env-file parameters with NUL character in their value part <name>=<value> are invalid now; NUL bytes were allowed previously Environment variable definition errors in --env-file give message about the file and line Fix \u00b6 Deprecation warning when running pipelines w/ PHP 8.1 w/o the yaml extension (#21) (thanks Billy Romano ) 0.0.66 - 2022-06-22 \u00b6 Add \u00b6 Run step scripts with /bin/bash if it is available; disable with script.bash-runner=false for previous behaviour (#17) (thanks Tim Clephas ) script.runner configuration parameter to change /bin/sh , the default script runner Change \u00b6 -c <name>=true and false are now supported for boolean configuration parameter <value> Fix \u00b6 Fix cache and data directory creation mode, limit to user access only 0.0.65 - 2022-04-24 \u00b6 Add \u00b6 --show and --show-pipelines : Annotate steps' conditions with *C Schema for <step>.artifacts.download and paths properties Mar 2021 ( Tina Yu ) Support for step artifacts with paths attribute --show and --show-pipelines : Annotate steps' artifacts with *A Fix \u00b6 --show and --show-pipelines : Handle parse errors with step annotations, since 0.0.62 0.0.64 - 2022-04-20 \u00b6 Add \u00b6 Support digests as an alternative to tags for container images in pipeline steps, see Digest Support global docker option (#15) (thanks Adrian Cederberg ) PHP 7.4 Alpine based build container Change \u00b6 Extract tar-copier for copy deploy-mode PHP 8.1 Alpine based build container upgrade from RC to stable html-docs: Upgrade mkdocs-material to 8.2.9 0.0.63 - 2022-04-14 \u00b6 Change \u00b6 Enrich YAML parse error information (#14) (thanks Adrian Cederberg ) html-docs: Upgrade mkdocs-material to 8.1.3 Fix \u00b6 Fix composer version in phar build info for PHP 5.3 (and up sometimes), since 0.0.51 Fix portable path check, since 0.0.42 0.0.62 - 2021-12-19 \u00b6 Add \u00b6 --show and --show-pipelines : Annotate steps' manual triggers with *M Change \u00b6 Verbose last error report on Phpunit test-suite shutdown if fatal, improves 0.0.32 --show-pipelines : Show unnamed steps as no-name (like --show ) Fix \u00b6 Fix internal file descriptor system paths mapping flaws, since 0.0.39 0.0.61 - 2021-12-13 \u00b6 Change \u00b6 html-docs: Upgrade mkdocs-material to 8.1.0 Fix \u00b6 Fix pipelines --show for steps with a manual trigger and all following steps, since 0.0.38 0.0.60 - 2021-12-07 \u00b6 Add \u00b6 Getting Started mini tutorial, see Getting Started with Pipelines html-docs (mkdocs build, pipeline), see as well Pipelines HTML Documentation Build Change \u00b6 Documentation (diverse) 0.0.59 - 2021-11-08 \u00b6 Fix \u00b6 Remove temporary repository for development dependencies Change log 0.0.58 release date malformed 0.0.58 - 2021-11-07 \u00b6 Add \u00b6 PHP 8.1 compatibility, build and docker images for pipelines. Change \u00b6 Build with Composer 2.1; pin composer version to 2.1.11 (from 2.0.13), since 0.0.55 Fix \u00b6 Container base directory creation with multiple directory components in step.clone-path (symlink errors) Wrong $BITBUCKET_CLONE_DIR on non-default step.clone-path , since 0.0.45 Travis changed URLs 0.0.57 - 2021-05-14 \u00b6 Change \u00b6 Phar file build from sources and pipelines running from sources now show the same --version format. Fix \u00b6 Restore build reproducibility when building the phar file with composer 2, since 0.0.55 0.0.56 - 2021-05-13 \u00b6 Add \u00b6 Show --step-script , optionally by <id> and <step> Build the phar file with PHP 5.3 (as well, all 5.3-8.1 reproducible) Change \u00b6 Fail early if git command is n/a in phar build Fix \u00b6 Fix pipelines --version when installed via composer with composer version 2.0.0 or higher, since 0.0.51 Fix very rare php timezone warning in the phar build when modifying the timestamps, since 0.0.1 Fix phar build error in bare and isolated repository on removing non-existing development package stub, since 0.0.1 Fix Phpstorm meta for the phpunit based testsuite (mocks, WI-60242 ) 0.0.55 - 2021-05-02 \u00b6 Change \u00b6 Build with Composer 2; pin composer version to 2.0.13 (from 1.10.17) Fix \u00b6 $PHP_BINARY support while making test stub packages, since 0.0.25 PHP-Binary detection in meagre environments, since 0.0.19 0.0.54 - 2021-04-17 \u00b6 Add \u00b6 Support changed docker remove behaviour (Docker 20.10) Fix \u00b6 Exit on docker service definition, since 0.0.37 (#10) (thanks Manuel ) 0.0.53 - 2021-01-03 \u00b6 Add \u00b6 Show --validate file-name/ -path Change \u00b6 Reduce of past-tense in change log headlines Fix \u00b6 Phar uploads for the releases 0.0.52 and 0.0.51 on Github as they did not match the original signatures from date of release due to an error in the build re-building them within a dirty repo (old files show \"+\" at the end of their version number (0.0.52+; 0.0.51+), correct phar files do not. Shell tests in CI taint phar build, since 0.0.51 Validating w/ empty file-name ( --validate= ), since 0.0.44 Changelog missing links to \"since x.x.x\" revisions Changelog missing dash \"-\" in last revision headline 0.0.52 - 2020-12-31 \u00b6 Change \u00b6 Tests expect Xdebug 3 by default, run $ composer ppconf xdebug2 for Xdebug 2 compatibility. Continue migration from Travis-CI to Github-Actions Rename tests folder to test to streamline directory names. Fix \u00b6 Composer which script compatibility with composer 2 < 2.0.7. Quoting new-line character at the end of argument, since 0.0.1 Phpunit test-case shim for invalid-argument-helper since Phpunit 6.x, missing in 0.0.51 0.0.51 - 2020-12-09 \u00b6 Add \u00b6 Support for PHP 8 Support for Composer 2 Migration from Travis-CI to Github-Actions Documentation about development (source only) pipelines --xdebug option to run within php with xdebug extension and config for CLI (server-name is pipelines-cli ) Composer script descriptions More pipeline example YAML files Change \u00b6 Testsuite for PHP 8 changes Updated documentation about working offline Fix \u00b6 Unintended object to array conversion, supports PHP 8 Done message for --validate saying \"verify done\" instead of \"validate done\" since 0.0.44 Detecting readable local streams wrong for non-local remote streams Shell test for artifacts, missing in 0.0.50 0.0.50 - 2020-09-14 \u00b6 Add \u00b6 Documentation about working offline incl. pipelines example yaml files Shell test for artifacts Change \u00b6 Updated readme Fix \u00b6 Artifacts. Broken since 0.0.43 0.0.49 - 2020-08-10 \u00b6 Add \u00b6 --show step caches step caches validation against cache definitions (broken name, undefined custom cache) when parsing step caches ( --show , running a pipeline step etc.). php cs-fixer custom fixers (thanks Kuba Werlos ) Change \u00b6 improve pipeline cache docs 0.0.48 - 2020-07-31 \u00b6 Add \u00b6 dependency caches and --no-cache for previous behavior, caches documentation Fix \u00b6 Integration test polluting $HOME for docker client stub 0.0.47 - 2020-07-23 \u00b6 Add \u00b6 version info to --debug above the stacktrace/s (thanks Andreas Sundqvist ) 0.0.46 - 2020-07-10 \u00b6 Fix \u00b6 Regression of missing labels for step containers since 0.0.43 0.0.45 - 2020-07-09 \u00b6 Add \u00b6 step.clone-path configuration parameter for the path inside the step container to deploy the project files, defaults to /app Schema for new <pipeline>.step.condition directives Jun 2020 ( Peter Plewa ) Schema for new clone and <pipeline>.step.clone options Feb/Apr 2020 ( Antoine B\u00fcsch ) 0.0.44 - 2020-07-06 \u00b6 Add \u00b6 --validate[=<path>] option to schema-validate a bitbucket-pipelines.yml ; can be used multiple times; validates and exists, non-zero if one or multiple files do not validate 0.0.43 - 2020-07-05 \u00b6 Add \u00b6 Labels for step and service containers Change \u00b6 Re-arrange help section for more specific runner options Fix \u00b6 Regression identifying pipelines service container ( --docker-list etc.) since 0.0.42 0.0.42 - 2020-06-25 \u00b6 Add \u00b6 script.exit-early bool configuration parameter to exit early in step scripts on error more strictly. defaults to false. --ssh option to mount $SSH_AUTH_SOCK into the pipeline/step container, SSH agent forwarding (#6) --user[=<uid>:<gid>] option to run pipeline/step container as current or specific user (and group) (#6) Change \u00b6 Improved container names, service containers names start with pipelines.<service> instead of pipelines-<service> . 0.0.41 - 2020-06-21 \u00b6 Add \u00b6 Add -c <name>=<value> option to pass a configuration parameter to the command. Support for BITBUCKET_STEP_RUN_NUMBER environment parameter: defaults to 1 and set to 1 after first successful step. 0.0.40 - 2020-06-17 \u00b6 Fix \u00b6 Wording of pipe scripts comments Tmp cleanup in tests Parsing a pipe in after-script Parsing a pipeline with variables keyword 0.0.39 - 2020-06-02 \u00b6 Add \u00b6 Service documentation --service <service> run <service> attached for trouble shooting a service configuration or watch logs while the service is running --file accepts the special file-name - to read pipelines from standard input Fix \u00b6 Parsing empty step 0.0.38 - 2020-06-01 \u00b6 Add \u00b6 --show each step and step services --images shows service images --show-pipelines for old --show format/ behaviour --show-services to show services in use of pipeline steps Change \u00b6 Improve parse error reporting of variables in service definitions 0.0.37 - 2020-05-30 \u00b6 Add \u00b6 Pipeline services other than docker (redis, mysql, ...) Fix \u00b6 Comment formatting in .env.dist (minor) 0.0.36 - 2020-05-28 \u00b6 Add \u00b6 Help section w/ help message from src in readme Fix \u00b6 Pipeline default variables command line arguments parsing Help message on trigger destination branch name Remove static calls to throw a status exception 0.0.35 - 2020-05-24 \u00b6 Change \u00b6 Updated readme for instructions Fix \u00b6 Shell test runner run on invalid test-case/driver Type-handling in code and dead/superfluous code Remove static calls to throw a parse exception After-script ignored all script errors, fix is to exit on first error Remove outdated docker client package basename property from test harness Add Apache-2.0 license text (in doc) Markdown documentation file typos, wording and structure Docker name tag syntax (in doc) Change log two release links 0.0.34 - 2020-05-13 \u00b6 Fix \u00b6 Show and fake run pipelines with a pipe (#5) 0.0.33 - 2020-04-26 \u00b6 Change \u00b6 Readme for container re-use Fix \u00b6 Build for Phpunit 7.5+ Diverse lower-level code issues 0.0.32 - 2020-04-11 \u00b6 Add \u00b6 Travis PHP 7.4 build Composer script phpunit for use in diverse test scripts using the same configuration Composer scripts which and which-php to obtain the path to composer and php in use Change \u00b6 Test-case forward compatibility for Phpunit 8 (for PHP 7.4 build) Fix \u00b6 Deprecation warning when running pipelines w/ PHP 7.4 w/o the yaml extension Code coverage w/ PHP 7.4 / Xdebug 2.9.3 Composer script ci to run pipelines by build PHP version Shell test runner usage information Travis build configuration validation fixes Do not hide errors in Phpunit test-suite 0.0.31 - 2020-04-06 \u00b6 Fix \u00b6 Patch fstat permission bits after PHP bug #79082 & #77022 fix to restore reproducible phar build Correct missing link in Change Log 0.0.30 - 2020-04-05 \u00b6 Add \u00b6 Support for after-script: incl. BITBUCKET_EXIT_CODE environment parameter. Fix \u00b6 Corrections in Read Me and Change Log 0.0.29 - 2020-04-04 \u00b6 Change \u00b6 Pin composer version in phar build; show which composer version in use Add destination to pull request --trigger pr: : 0.0.28 - 2020-03-16 \u00b6 Change \u00b6 Phar build: Do not require platform dependencies for composer install Travis build: Handle GPG key import before install completely Fix \u00b6 Tainted phar build on Travis (adding \"+\" to versions in error), since 0.0.25 0.0.27 - 2020-03-15 \u00b6 Add \u00b6 --no-manual option to not stop at manual step(s). The new default is to stop at steps marked manual trigger: manual . The first step of a pipeline can not be manual, and the first step executed with --steps will never stop even if it has a trigger: manual . Fix \u00b6 Base unit-test-case missing shim createConfiguredMock method 0.0.26 - 2020-03-09 \u00b6 Add \u00b6 --steps option to specify which step(s) of a pipeline to run. Same as --step and to reserve both. 1 as well as 1,2,3 , 1-3 or -2,3- are valid Fix \u00b6 Base unit-test-case exception expectation optional message parameter Travis build pulling php:5.3 Read Me and Change Log fixes for links, WS fixes and typo for other documentation files (.md, comment in .sh) Travis build w/ peer verification issues in composer (disabling HTTPS/TLS in custom/unit-tests-php-5.3 pipeline) Patch fstat permission bits after PHP bug #79082 fix to restore reproducible phar build 0.0.25 - 2019-12-30 \u00b6 Add \u00b6 Support of Docker in rootless mode and a How-To in the docs folder. Support of DOCKER_HOST parameter for unix:// sockets (Docker Service) --docker-client-pkgs option to list available docker client binary packages (Docker Service) --docker-client option to specify which docker client binary to use (Docker Service) Docker Service in YAML injects Linux X86_64 docker client binary (Docker Service) Change \u00b6 Internal improvements of the step runner 0.0.24 - 2019-12-21 \u00b6 Add \u00b6 PIPELINES_PROJECT_PATH parameter Change \u00b6 More readable step scripts Fix \u00b6 Pipelines w/ --deploy mount inside a pipeline of --deploy copy , the current default. Busybox on Atlassian Bitbucket Cloud 0.0.23 - 2019-12-17 \u00b6 Change \u00b6 Improve --help display Show scripts with -v and drop temporary files for scripts Fix \u00b6 Exec tester unintentionally override of phpunit test case results 0.0.22 - 2019-10-12 \u00b6 Change \u00b6 Use Symfony YAML as fall-back parser, replaces Mustangostang Spyc (#4) 0.0.21 - 2019-09-23 \u00b6 Fix \u00b6 Unintended output of \"\\x1D\" on some container systems 0.0.20 - 2019-09-20 \u00b6 Add \u00b6 File format support to check if a step has services Test case base class fall-back to Phpunit create* mock functions Change \u00b6 Execute script as a single script instead of executing line by line Fix \u00b6 Container exited while running script (136, broken pipe on socket etc.) Remove PHP internal variables like $argv from the environment variable maps in containers 0.0.19 - 2019-04-02 \u00b6 Add \u00b6 Suggestion to install the PHP YAML extension Kept containers are automatically re-used if they still exist Support for pull request pipelines Change \u00b6 Reduce artifact chunk size from fixed number 1792 to string length based Fix \u00b6 Patch fstat permission bits after PHP bug #77022 fix to restore reproducible phar build 0.0.18 - 2018-08-07 \u00b6 Add \u00b6 Add --docker-zap flag kill and clean all pipeline docker containers at once Fallback for readable file check for systems w/ ACLs where a file is not readable by permission but can be read (#1) Change \u00b6 Pipeline step specific container names instead of random UUIDs so that keeping pipelines (and only if in mind) makes this all much more predictable 0.0.17 - 2018-05-29 \u00b6 Change \u00b6 Reduce artifact chunk size from 2048 to 1792 Fix \u00b6 Symbolic links in artifacts Read me file has some errors and inconsistencies. Again. 0.0.16 - 2018-05-04 \u00b6 Add \u00b6 Support for PHP YAML extension, is preferred over Spyc lib if available; highly recommended Fix \u00b6 All uppercase hexits in builder phar info 0.0.15 - 2018-04-23 \u00b6 Add \u00b6 Add --no-dot-env-files and --no-dot-env-dot-dist flags to not pass .env.dist and .env files to docker as --env-file arguments 0.0.14 - 2018-04-18 \u00b6 Add \u00b6 Tag script to make releases Change \u00b6 More useful default BITBUCKET_REPO_SLUG value Fix \u00b6 Coverage checker script precision Duplicate output of non-zero exit code information 0.0.13 - 2018-03-20 \u00b6 Fix \u00b6 Fix --error-keep keeping containers 0.0.12 - 2018-03-19 \u00b6 Add \u00b6 Utility status exception Change \u00b6 Streamline of file parse error handling Streamline of utility option and argument errors Parsing of utility options and arguments in run routine Fix \u00b6 Code coverage for unit tests 0.0.11 - 2018-03-13 \u00b6 Add \u00b6 Keep container on error option: --error-keep Change \u00b6 Do not keep containers by default, not even on error Fix \u00b6 Code style 0.0.10 - 2018-03-12 \u00b6 Add \u00b6 Coverage check Change \u00b6 Code style Readme for corrections and coverage Fix \u00b6 Resolution of environment variables (esp. w/ numbers in name) 0.0.9 - 2018-02-28 \u00b6 Add \u00b6 Traverse upwards for pipelines file Fix \u00b6 Phive release signing App coverage for deploy copy mode 0.0.8 - 2018-02-27 \u00b6 Add \u00b6 Phive release signing Fix \u00b6 Hardencoded /tmp directory 0.0.7 - 2018-02-27 \u00b6 Fix \u00b6 Describe missing --trigger in help text Build directory owner and attributes for deploy copy mode Do not capture artifacts files after failed step 0.0.6 - 2018-02-14 \u00b6 Add \u00b6 Support for .env / .env.dist file(s) Support for Docker Hub private repositories incl. providing credentials via --env or --env-file environment variables Change \u00b6 Readme for corrections and coverage Fix \u00b6 Support for large number of artifacts files Crash with image run-as-user property in pipelines file Deploy copy mode fail-safe against copying errors (e.g. permission denied on a file to copy) 0.0.5 - 2018-01-29 \u00b6 Add \u00b6 Docker environment variables options: -e , --env for variables and --env-file for files Composer \"ci\" script to integrate continuously --no-keep option to never keep containers, even on error Change \u00b6 Default --deploy mode is now copy , was mount previously Fix \u00b6 Image name validation Image as a section Show same image name only once Remove version output from -v, --verbose Validation of --basename actually being a basename Error messages now show the utility name 0.0.4 - 2018-01-16 \u00b6 Add \u00b6 Release phar files on Github Change \u00b6 Various code style improvements Readme for corrections and coverage 0.0.3 - 2018-01-14 \u00b6 Add \u00b6 Keep container on pipeline step failure automatically --verbatim option to only output from pipeline, not pipelines Change \u00b6 --help information Various code style improvements 0.0.2 - 2018-01-11 \u00b6 Add \u00b6 Brace glob pattern in pipelines Change log 0.0.1 - 2018-01-10 \u00b6 Add \u00b6 Initial release","title":"Change Log"},{"location":"CHANGELOG.html#change-log","text":"All notable changes to Pipelines will be documented in this file. The format is based on Keep a Changelog and Pipelines adheres to Semantic Versioning .","title":"Change Log"},{"location":"CHANGELOG.html#0067---2022-07-17","text":"","title":"0.0.67 - 2022-07-17"},{"location":"CHANGELOG.html#change","text":"Environment variable definitions in -e , --env and --env-file parameters with NUL character in their value part <name>=<value> are invalid now; NUL bytes were allowed previously Environment variable definition errors in --env-file give message about the file and line","title":"Change"},{"location":"CHANGELOG.html#fix","text":"Deprecation warning when running pipelines w/ PHP 8.1 w/o the yaml extension (#21) (thanks Billy Romano )","title":"Fix"},{"location":"CHANGELOG.html#0066---2022-06-22","text":"","title":"0.0.66 - 2022-06-22"},{"location":"CHANGELOG.html#add","text":"Run step scripts with /bin/bash if it is available; disable with script.bash-runner=false for previous behaviour (#17) (thanks Tim Clephas ) script.runner configuration parameter to change /bin/sh , the default script runner","title":"Add"},{"location":"CHANGELOG.html#change_1","text":"-c <name>=true and false are now supported for boolean configuration parameter <value>","title":"Change"},{"location":"CHANGELOG.html#fix_1","text":"Fix cache and data directory creation mode, limit to user access only","title":"Fix"},{"location":"CHANGELOG.html#0065---2022-04-24","text":"","title":"0.0.65 - 2022-04-24"},{"location":"CHANGELOG.html#add_1","text":"--show and --show-pipelines : Annotate steps' conditions with *C Schema for <step>.artifacts.download and paths properties Mar 2021 ( Tina Yu ) Support for step artifacts with paths attribute --show and --show-pipelines : Annotate steps' artifacts with *A","title":"Add"},{"location":"CHANGELOG.html#fix_2","text":"--show and --show-pipelines : Handle parse errors with step annotations, since 0.0.62","title":"Fix"},{"location":"CHANGELOG.html#0064---2022-04-20","text":"","title":"0.0.64 - 2022-04-20"},{"location":"CHANGELOG.html#add_2","text":"Support digests as an alternative to tags for container images in pipeline steps, see Digest Support global docker option (#15) (thanks Adrian Cederberg ) PHP 7.4 Alpine based build container","title":"Add"},{"location":"CHANGELOG.html#change_2","text":"Extract tar-copier for copy deploy-mode PHP 8.1 Alpine based build container upgrade from RC to stable html-docs: Upgrade mkdocs-material to 8.2.9","title":"Change"},{"location":"CHANGELOG.html#0063---2022-04-14","text":"","title":"0.0.63 - 2022-04-14"},{"location":"CHANGELOG.html#change_3","text":"Enrich YAML parse error information (#14) (thanks Adrian Cederberg ) html-docs: Upgrade mkdocs-material to 8.1.3","title":"Change"},{"location":"CHANGELOG.html#fix_3","text":"Fix composer version in phar build info for PHP 5.3 (and up sometimes), since 0.0.51 Fix portable path check, since 0.0.42","title":"Fix"},{"location":"CHANGELOG.html#0062---2021-12-19","text":"","title":"0.0.62 - 2021-12-19"},{"location":"CHANGELOG.html#add_3","text":"--show and --show-pipelines : Annotate steps' manual triggers with *M","title":"Add"},{"location":"CHANGELOG.html#change_4","text":"Verbose last error report on Phpunit test-suite shutdown if fatal, improves 0.0.32 --show-pipelines : Show unnamed steps as no-name (like --show )","title":"Change"},{"location":"CHANGELOG.html#fix_4","text":"Fix internal file descriptor system paths mapping flaws, since 0.0.39","title":"Fix"},{"location":"CHANGELOG.html#0061---2021-12-13","text":"","title":"0.0.61 - 2021-12-13"},{"location":"CHANGELOG.html#change_5","text":"html-docs: Upgrade mkdocs-material to 8.1.0","title":"Change"},{"location":"CHANGELOG.html#fix_5","text":"Fix pipelines --show for steps with a manual trigger and all following steps, since 0.0.38","title":"Fix"},{"location":"CHANGELOG.html#0060---2021-12-07","text":"","title":"0.0.60 - 2021-12-07"},{"location":"CHANGELOG.html#add_4","text":"Getting Started mini tutorial, see Getting Started with Pipelines html-docs (mkdocs build, pipeline), see as well Pipelines HTML Documentation Build","title":"Add"},{"location":"CHANGELOG.html#change_6","text":"Documentation (diverse)","title":"Change"},{"location":"CHANGELOG.html#0059---2021-11-08","text":"","title":"0.0.59 - 2021-11-08"},{"location":"CHANGELOG.html#fix_6","text":"Remove temporary repository for development dependencies Change log 0.0.58 release date malformed","title":"Fix"},{"location":"CHANGELOG.html#0058---2021-11-07","text":"","title":"0.0.58 - 2021-11-07"},{"location":"CHANGELOG.html#add_5","text":"PHP 8.1 compatibility, build and docker images for pipelines.","title":"Add"},{"location":"CHANGELOG.html#change_7","text":"Build with Composer 2.1; pin composer version to 2.1.11 (from 2.0.13), since 0.0.55","title":"Change"},{"location":"CHANGELOG.html#fix_7","text":"Container base directory creation with multiple directory components in step.clone-path (symlink errors) Wrong $BITBUCKET_CLONE_DIR on non-default step.clone-path , since 0.0.45 Travis changed URLs","title":"Fix"},{"location":"CHANGELOG.html#0057---2021-05-14","text":"","title":"0.0.57 - 2021-05-14"},{"location":"CHANGELOG.html#change_8","text":"Phar file build from sources and pipelines running from sources now show the same --version format.","title":"Change"},{"location":"CHANGELOG.html#fix_8","text":"Restore build reproducibility when building the phar file with composer 2, since 0.0.55","title":"Fix"},{"location":"CHANGELOG.html#0056---2021-05-13","text":"","title":"0.0.56 - 2021-05-13"},{"location":"CHANGELOG.html#add_6","text":"Show --step-script , optionally by <id> and <step> Build the phar file with PHP 5.3 (as well, all 5.3-8.1 reproducible)","title":"Add"},{"location":"CHANGELOG.html#change_9","text":"Fail early if git command is n/a in phar build","title":"Change"},{"location":"CHANGELOG.html#fix_9","text":"Fix pipelines --version when installed via composer with composer version 2.0.0 or higher, since 0.0.51 Fix very rare php timezone warning in the phar build when modifying the timestamps, since 0.0.1 Fix phar build error in bare and isolated repository on removing non-existing development package stub, since 0.0.1 Fix Phpstorm meta for the phpunit based testsuite (mocks, WI-60242 )","title":"Fix"},{"location":"CHANGELOG.html#0055---2021-05-02","text":"","title":"0.0.55 - 2021-05-02"},{"location":"CHANGELOG.html#change_10","text":"Build with Composer 2; pin composer version to 2.0.13 (from 1.10.17)","title":"Change"},{"location":"CHANGELOG.html#fix_10","text":"$PHP_BINARY support while making test stub packages, since 0.0.25 PHP-Binary detection in meagre environments, since 0.0.19","title":"Fix"},{"location":"CHANGELOG.html#0054---2021-04-17","text":"","title":"0.0.54 - 2021-04-17"},{"location":"CHANGELOG.html#add_7","text":"Support changed docker remove behaviour (Docker 20.10)","title":"Add"},{"location":"CHANGELOG.html#fix_11","text":"Exit on docker service definition, since 0.0.37 (#10) (thanks Manuel )","title":"Fix"},{"location":"CHANGELOG.html#0053---2021-01-03","text":"","title":"0.0.53 - 2021-01-03"},{"location":"CHANGELOG.html#add_8","text":"Show --validate file-name/ -path","title":"Add"},{"location":"CHANGELOG.html#change_11","text":"Reduce of past-tense in change log headlines","title":"Change"},{"location":"CHANGELOG.html#fix_12","text":"Phar uploads for the releases 0.0.52 and 0.0.51 on Github as they did not match the original signatures from date of release due to an error in the build re-building them within a dirty repo (old files show \"+\" at the end of their version number (0.0.52+; 0.0.51+), correct phar files do not. Shell tests in CI taint phar build, since 0.0.51 Validating w/ empty file-name ( --validate= ), since 0.0.44 Changelog missing links to \"since x.x.x\" revisions Changelog missing dash \"-\" in last revision headline","title":"Fix"},{"location":"CHANGELOG.html#0052---2020-12-31","text":"","title":"0.0.52 - 2020-12-31"},{"location":"CHANGELOG.html#change_12","text":"Tests expect Xdebug 3 by default, run $ composer ppconf xdebug2 for Xdebug 2 compatibility. Continue migration from Travis-CI to Github-Actions Rename tests folder to test to streamline directory names.","title":"Change"},{"location":"CHANGELOG.html#fix_13","text":"Composer which script compatibility with composer 2 < 2.0.7. Quoting new-line character at the end of argument, since 0.0.1 Phpunit test-case shim for invalid-argument-helper since Phpunit 6.x, missing in 0.0.51","title":"Fix"},{"location":"CHANGELOG.html#0051---2020-12-09","text":"","title":"0.0.51 - 2020-12-09"},{"location":"CHANGELOG.html#add_9","text":"Support for PHP 8 Support for Composer 2 Migration from Travis-CI to Github-Actions Documentation about development (source only) pipelines --xdebug option to run within php with xdebug extension and config for CLI (server-name is pipelines-cli ) Composer script descriptions More pipeline example YAML files","title":"Add"},{"location":"CHANGELOG.html#change_13","text":"Testsuite for PHP 8 changes Updated documentation about working offline","title":"Change"},{"location":"CHANGELOG.html#fix_14","text":"Unintended object to array conversion, supports PHP 8 Done message for --validate saying \"verify done\" instead of \"validate done\" since 0.0.44 Detecting readable local streams wrong for non-local remote streams Shell test for artifacts, missing in 0.0.50","title":"Fix"},{"location":"CHANGELOG.html#0050---2020-09-14","text":"","title":"0.0.50 - 2020-09-14"},{"location":"CHANGELOG.html#add_10","text":"Documentation about working offline incl. pipelines example yaml files Shell test for artifacts","title":"Add"},{"location":"CHANGELOG.html#change_14","text":"Updated readme","title":"Change"},{"location":"CHANGELOG.html#fix_15","text":"Artifacts. Broken since 0.0.43","title":"Fix"},{"location":"CHANGELOG.html#0049---2020-08-10","text":"","title":"0.0.49 - 2020-08-10"},{"location":"CHANGELOG.html#add_11","text":"--show step caches step caches validation against cache definitions (broken name, undefined custom cache) when parsing step caches ( --show , running a pipeline step etc.). php cs-fixer custom fixers (thanks Kuba Werlos )","title":"Add"},{"location":"CHANGELOG.html#change_15","text":"improve pipeline cache docs","title":"Change"},{"location":"CHANGELOG.html#0048---2020-07-31","text":"","title":"0.0.48 - 2020-07-31"},{"location":"CHANGELOG.html#add_12","text":"dependency caches and --no-cache for previous behavior, caches documentation","title":"Add"},{"location":"CHANGELOG.html#fix_16","text":"Integration test polluting $HOME for docker client stub","title":"Fix"},{"location":"CHANGELOG.html#0047---2020-07-23","text":"","title":"0.0.47 - 2020-07-23"},{"location":"CHANGELOG.html#add_13","text":"version info to --debug above the stacktrace/s (thanks Andreas Sundqvist )","title":"Add"},{"location":"CHANGELOG.html#0046---2020-07-10","text":"","title":"0.0.46 - 2020-07-10"},{"location":"CHANGELOG.html#fix_17","text":"Regression of missing labels for step containers since 0.0.43","title":"Fix"},{"location":"CHANGELOG.html#0045---2020-07-09","text":"","title":"0.0.45 - 2020-07-09"},{"location":"CHANGELOG.html#add_14","text":"step.clone-path configuration parameter for the path inside the step container to deploy the project files, defaults to /app Schema for new <pipeline>.step.condition directives Jun 2020 ( Peter Plewa ) Schema for new clone and <pipeline>.step.clone options Feb/Apr 2020 ( Antoine B\u00fcsch )","title":"Add"},{"location":"CHANGELOG.html#0044---2020-07-06","text":"","title":"0.0.44 - 2020-07-06"},{"location":"CHANGELOG.html#add_15","text":"--validate[=<path>] option to schema-validate a bitbucket-pipelines.yml ; can be used multiple times; validates and exists, non-zero if one or multiple files do not validate","title":"Add"},{"location":"CHANGELOG.html#0043---2020-07-05","text":"","title":"0.0.43 - 2020-07-05"},{"location":"CHANGELOG.html#add_16","text":"Labels for step and service containers","title":"Add"},{"location":"CHANGELOG.html#change_16","text":"Re-arrange help section for more specific runner options","title":"Change"},{"location":"CHANGELOG.html#fix_18","text":"Regression identifying pipelines service container ( --docker-list etc.) since 0.0.42","title":"Fix"},{"location":"CHANGELOG.html#0042---2020-06-25","text":"","title":"0.0.42 - 2020-06-25"},{"location":"CHANGELOG.html#add_17","text":"script.exit-early bool configuration parameter to exit early in step scripts on error more strictly. defaults to false. --ssh option to mount $SSH_AUTH_SOCK into the pipeline/step container, SSH agent forwarding (#6) --user[=<uid>:<gid>] option to run pipeline/step container as current or specific user (and group) (#6)","title":"Add"},{"location":"CHANGELOG.html#change_17","text":"Improved container names, service containers names start with pipelines.<service> instead of pipelines-<service> .","title":"Change"},{"location":"CHANGELOG.html#0041---2020-06-21","text":"","title":"0.0.41 - 2020-06-21"},{"location":"CHANGELOG.html#add_18","text":"Add -c <name>=<value> option to pass a configuration parameter to the command. Support for BITBUCKET_STEP_RUN_NUMBER environment parameter: defaults to 1 and set to 1 after first successful step.","title":"Add"},{"location":"CHANGELOG.html#0040---2020-06-17","text":"","title":"0.0.40 - 2020-06-17"},{"location":"CHANGELOG.html#fix_19","text":"Wording of pipe scripts comments Tmp cleanup in tests Parsing a pipe in after-script Parsing a pipeline with variables keyword","title":"Fix"},{"location":"CHANGELOG.html#0039---2020-06-02","text":"","title":"0.0.39 - 2020-06-02"},{"location":"CHANGELOG.html#add_19","text":"Service documentation --service <service> run <service> attached for trouble shooting a service configuration or watch logs while the service is running --file accepts the special file-name - to read pipelines from standard input","title":"Add"},{"location":"CHANGELOG.html#fix_20","text":"Parsing empty step","title":"Fix"},{"location":"CHANGELOG.html#0038---2020-06-01","text":"","title":"0.0.38 - 2020-06-01"},{"location":"CHANGELOG.html#add_20","text":"--show each step and step services --images shows service images --show-pipelines for old --show format/ behaviour --show-services to show services in use of pipeline steps","title":"Add"},{"location":"CHANGELOG.html#change_18","text":"Improve parse error reporting of variables in service definitions","title":"Change"},{"location":"CHANGELOG.html#0037---2020-05-30","text":"","title":"0.0.37 - 2020-05-30"},{"location":"CHANGELOG.html#add_21","text":"Pipeline services other than docker (redis, mysql, ...)","title":"Add"},{"location":"CHANGELOG.html#fix_21","text":"Comment formatting in .env.dist (minor)","title":"Fix"},{"location":"CHANGELOG.html#0036---2020-05-28","text":"","title":"0.0.36 - 2020-05-28"},{"location":"CHANGELOG.html#add_22","text":"Help section w/ help message from src in readme","title":"Add"},{"location":"CHANGELOG.html#fix_22","text":"Pipeline default variables command line arguments parsing Help message on trigger destination branch name Remove static calls to throw a status exception","title":"Fix"},{"location":"CHANGELOG.html#0035---2020-05-24","text":"","title":"0.0.35 - 2020-05-24"},{"location":"CHANGELOG.html#change_19","text":"Updated readme for instructions","title":"Change"},{"location":"CHANGELOG.html#fix_23","text":"Shell test runner run on invalid test-case/driver Type-handling in code and dead/superfluous code Remove static calls to throw a parse exception After-script ignored all script errors, fix is to exit on first error Remove outdated docker client package basename property from test harness Add Apache-2.0 license text (in doc) Markdown documentation file typos, wording and structure Docker name tag syntax (in doc) Change log two release links","title":"Fix"},{"location":"CHANGELOG.html#0034---2020-05-13","text":"","title":"0.0.34 - 2020-05-13"},{"location":"CHANGELOG.html#fix_24","text":"Show and fake run pipelines with a pipe (#5)","title":"Fix"},{"location":"CHANGELOG.html#0033---2020-04-26","text":"","title":"0.0.33 - 2020-04-26"},{"location":"CHANGELOG.html#change_20","text":"Readme for container re-use","title":"Change"},{"location":"CHANGELOG.html#fix_25","text":"Build for Phpunit 7.5+ Diverse lower-level code issues","title":"Fix"},{"location":"CHANGELOG.html#0032---2020-04-11","text":"","title":"0.0.32 - 2020-04-11"},{"location":"CHANGELOG.html#add_23","text":"Travis PHP 7.4 build Composer script phpunit for use in diverse test scripts using the same configuration Composer scripts which and which-php to obtain the path to composer and php in use","title":"Add"},{"location":"CHANGELOG.html#change_21","text":"Test-case forward compatibility for Phpunit 8 (for PHP 7.4 build)","title":"Change"},{"location":"CHANGELOG.html#fix_26","text":"Deprecation warning when running pipelines w/ PHP 7.4 w/o the yaml extension Code coverage w/ PHP 7.4 / Xdebug 2.9.3 Composer script ci to run pipelines by build PHP version Shell test runner usage information Travis build configuration validation fixes Do not hide errors in Phpunit test-suite","title":"Fix"},{"location":"CHANGELOG.html#0031---2020-04-06","text":"","title":"0.0.31 - 2020-04-06"},{"location":"CHANGELOG.html#fix_27","text":"Patch fstat permission bits after PHP bug #79082 & #77022 fix to restore reproducible phar build Correct missing link in Change Log","title":"Fix"},{"location":"CHANGELOG.html#0030---2020-04-05","text":"","title":"0.0.30 - 2020-04-05"},{"location":"CHANGELOG.html#add_24","text":"Support for after-script: incl. BITBUCKET_EXIT_CODE environment parameter.","title":"Add"},{"location":"CHANGELOG.html#fix_28","text":"Corrections in Read Me and Change Log","title":"Fix"},{"location":"CHANGELOG.html#0029---2020-04-04","text":"","title":"0.0.29 - 2020-04-04"},{"location":"CHANGELOG.html#change_22","text":"Pin composer version in phar build; show which composer version in use Add destination to pull request --trigger pr: :","title":"Change"},{"location":"CHANGELOG.html#0028---2020-03-16","text":"","title":"0.0.28 - 2020-03-16"},{"location":"CHANGELOG.html#change_23","text":"Phar build: Do not require platform dependencies for composer install Travis build: Handle GPG key import before install completely","title":"Change"},{"location":"CHANGELOG.html#fix_29","text":"Tainted phar build on Travis (adding \"+\" to versions in error), since 0.0.25","title":"Fix"},{"location":"CHANGELOG.html#0027---2020-03-15","text":"","title":"0.0.27 - 2020-03-15"},{"location":"CHANGELOG.html#add_25","text":"--no-manual option to not stop at manual step(s). The new default is to stop at steps marked manual trigger: manual . The first step of a pipeline can not be manual, and the first step executed with --steps will never stop even if it has a trigger: manual .","title":"Add"},{"location":"CHANGELOG.html#fix_30","text":"Base unit-test-case missing shim createConfiguredMock method","title":"Fix"},{"location":"CHANGELOG.html#0026---2020-03-09","text":"","title":"0.0.26 - 2020-03-09"},{"location":"CHANGELOG.html#add_26","text":"--steps option to specify which step(s) of a pipeline to run. Same as --step and to reserve both. 1 as well as 1,2,3 , 1-3 or -2,3- are valid","title":"Add"},{"location":"CHANGELOG.html#fix_31","text":"Base unit-test-case exception expectation optional message parameter Travis build pulling php:5.3 Read Me and Change Log fixes for links, WS fixes and typo for other documentation files (.md, comment in .sh) Travis build w/ peer verification issues in composer (disabling HTTPS/TLS in custom/unit-tests-php-5.3 pipeline) Patch fstat permission bits after PHP bug #79082 fix to restore reproducible phar build","title":"Fix"},{"location":"CHANGELOG.html#0025---2019-12-30","text":"","title":"0.0.25 - 2019-12-30"},{"location":"CHANGELOG.html#add_27","text":"Support of Docker in rootless mode and a How-To in the docs folder. Support of DOCKER_HOST parameter for unix:// sockets (Docker Service) --docker-client-pkgs option to list available docker client binary packages (Docker Service) --docker-client option to specify which docker client binary to use (Docker Service) Docker Service in YAML injects Linux X86_64 docker client binary (Docker Service)","title":"Add"},{"location":"CHANGELOG.html#change_24","text":"Internal improvements of the step runner","title":"Change"},{"location":"CHANGELOG.html#0024---2019-12-21","text":"","title":"0.0.24 - 2019-12-21"},{"location":"CHANGELOG.html#add_28","text":"PIPELINES_PROJECT_PATH parameter","title":"Add"},{"location":"CHANGELOG.html#change_25","text":"More readable step scripts","title":"Change"},{"location":"CHANGELOG.html#fix_32","text":"Pipelines w/ --deploy mount inside a pipeline of --deploy copy , the current default. Busybox on Atlassian Bitbucket Cloud","title":"Fix"},{"location":"CHANGELOG.html#0023---2019-12-17","text":"","title":"0.0.23 - 2019-12-17"},{"location":"CHANGELOG.html#change_26","text":"Improve --help display Show scripts with -v and drop temporary files for scripts","title":"Change"},{"location":"CHANGELOG.html#fix_33","text":"Exec tester unintentionally override of phpunit test case results","title":"Fix"},{"location":"CHANGELOG.html#0022---2019-10-12","text":"","title":"0.0.22 - 2019-10-12"},{"location":"CHANGELOG.html#change_27","text":"Use Symfony YAML as fall-back parser, replaces Mustangostang Spyc (#4)","title":"Change"},{"location":"CHANGELOG.html#0021---2019-09-23","text":"","title":"0.0.21 - 2019-09-23"},{"location":"CHANGELOG.html#fix_34","text":"Unintended output of \"\\x1D\" on some container systems","title":"Fix"},{"location":"CHANGELOG.html#0020---2019-09-20","text":"","title":"0.0.20 - 2019-09-20"},{"location":"CHANGELOG.html#add_29","text":"File format support to check if a step has services Test case base class fall-back to Phpunit create* mock functions","title":"Add"},{"location":"CHANGELOG.html#change_28","text":"Execute script as a single script instead of executing line by line","title":"Change"},{"location":"CHANGELOG.html#fix_35","text":"Container exited while running script (136, broken pipe on socket etc.) Remove PHP internal variables like $argv from the environment variable maps in containers","title":"Fix"},{"location":"CHANGELOG.html#0019---2019-04-02","text":"","title":"0.0.19 - 2019-04-02"},{"location":"CHANGELOG.html#add_30","text":"Suggestion to install the PHP YAML extension Kept containers are automatically re-used if they still exist Support for pull request pipelines","title":"Add"},{"location":"CHANGELOG.html#change_29","text":"Reduce artifact chunk size from fixed number 1792 to string length based","title":"Change"},{"location":"CHANGELOG.html#fix_36","text":"Patch fstat permission bits after PHP bug #77022 fix to restore reproducible phar build","title":"Fix"},{"location":"CHANGELOG.html#0018---2018-08-07","text":"","title":"0.0.18 - 2018-08-07"},{"location":"CHANGELOG.html#add_31","text":"Add --docker-zap flag kill and clean all pipeline docker containers at once Fallback for readable file check for systems w/ ACLs where a file is not readable by permission but can be read (#1)","title":"Add"},{"location":"CHANGELOG.html#change_30","text":"Pipeline step specific container names instead of random UUIDs so that keeping pipelines (and only if in mind) makes this all much more predictable","title":"Change"},{"location":"CHANGELOG.html#0017---2018-05-29","text":"","title":"0.0.17 - 2018-05-29"},{"location":"CHANGELOG.html#change_31","text":"Reduce artifact chunk size from 2048 to 1792","title":"Change"},{"location":"CHANGELOG.html#fix_37","text":"Symbolic links in artifacts Read me file has some errors and inconsistencies. Again.","title":"Fix"},{"location":"CHANGELOG.html#0016---2018-05-04","text":"","title":"0.0.16 - 2018-05-04"},{"location":"CHANGELOG.html#add_32","text":"Support for PHP YAML extension, is preferred over Spyc lib if available; highly recommended","title":"Add"},{"location":"CHANGELOG.html#fix_38","text":"All uppercase hexits in builder phar info","title":"Fix"},{"location":"CHANGELOG.html#0015---2018-04-23","text":"","title":"0.0.15 - 2018-04-23"},{"location":"CHANGELOG.html#add_33","text":"Add --no-dot-env-files and --no-dot-env-dot-dist flags to not pass .env.dist and .env files to docker as --env-file arguments","title":"Add"},{"location":"CHANGELOG.html#0014---2018-04-18","text":"","title":"0.0.14 - 2018-04-18"},{"location":"CHANGELOG.html#add_34","text":"Tag script to make releases","title":"Add"},{"location":"CHANGELOG.html#change_32","text":"More useful default BITBUCKET_REPO_SLUG value","title":"Change"},{"location":"CHANGELOG.html#fix_39","text":"Coverage checker script precision Duplicate output of non-zero exit code information","title":"Fix"},{"location":"CHANGELOG.html#0013---2018-03-20","text":"","title":"0.0.13 - 2018-03-20"},{"location":"CHANGELOG.html#fix_40","text":"Fix --error-keep keeping containers","title":"Fix"},{"location":"CHANGELOG.html#0012---2018-03-19","text":"","title":"0.0.12 - 2018-03-19"},{"location":"CHANGELOG.html#add_35","text":"Utility status exception","title":"Add"},{"location":"CHANGELOG.html#change_33","text":"Streamline of file parse error handling Streamline of utility option and argument errors Parsing of utility options and arguments in run routine","title":"Change"},{"location":"CHANGELOG.html#fix_41","text":"Code coverage for unit tests","title":"Fix"},{"location":"CHANGELOG.html#0011---2018-03-13","text":"","title":"0.0.11 - 2018-03-13"},{"location":"CHANGELOG.html#add_36","text":"Keep container on error option: --error-keep","title":"Add"},{"location":"CHANGELOG.html#change_34","text":"Do not keep containers by default, not even on error","title":"Change"},{"location":"CHANGELOG.html#fix_42","text":"Code style","title":"Fix"},{"location":"CHANGELOG.html#0010---2018-03-12","text":"","title":"0.0.10 - 2018-03-12"},{"location":"CHANGELOG.html#add_37","text":"Coverage check","title":"Add"},{"location":"CHANGELOG.html#change_35","text":"Code style Readme for corrections and coverage","title":"Change"},{"location":"CHANGELOG.html#fix_43","text":"Resolution of environment variables (esp. w/ numbers in name)","title":"Fix"},{"location":"CHANGELOG.html#009---2018-02-28","text":"","title":"0.0.9 - 2018-02-28"},{"location":"CHANGELOG.html#add_38","text":"Traverse upwards for pipelines file","title":"Add"},{"location":"CHANGELOG.html#fix_44","text":"Phive release signing App coverage for deploy copy mode","title":"Fix"},{"location":"CHANGELOG.html#008---2018-02-27","text":"","title":"0.0.8 - 2018-02-27"},{"location":"CHANGELOG.html#add_39","text":"Phive release signing","title":"Add"},{"location":"CHANGELOG.html#fix_45","text":"Hardencoded /tmp directory","title":"Fix"},{"location":"CHANGELOG.html#007---2018-02-27","text":"","title":"0.0.7 - 2018-02-27"},{"location":"CHANGELOG.html#fix_46","text":"Describe missing --trigger in help text Build directory owner and attributes for deploy copy mode Do not capture artifacts files after failed step","title":"Fix"},{"location":"CHANGELOG.html#006---2018-02-14","text":"","title":"0.0.6 - 2018-02-14"},{"location":"CHANGELOG.html#add_40","text":"Support for .env / .env.dist file(s) Support for Docker Hub private repositories incl. providing credentials via --env or --env-file environment variables","title":"Add"},{"location":"CHANGELOG.html#change_36","text":"Readme for corrections and coverage","title":"Change"},{"location":"CHANGELOG.html#fix_47","text":"Support for large number of artifacts files Crash with image run-as-user property in pipelines file Deploy copy mode fail-safe against copying errors (e.g. permission denied on a file to copy)","title":"Fix"},{"location":"CHANGELOG.html#005---2018-01-29","text":"","title":"0.0.5 - 2018-01-29"},{"location":"CHANGELOG.html#add_41","text":"Docker environment variables options: -e , --env for variables and --env-file for files Composer \"ci\" script to integrate continuously --no-keep option to never keep containers, even on error","title":"Add"},{"location":"CHANGELOG.html#change_37","text":"Default --deploy mode is now copy , was mount previously","title":"Change"},{"location":"CHANGELOG.html#fix_48","text":"Image name validation Image as a section Show same image name only once Remove version output from -v, --verbose Validation of --basename actually being a basename Error messages now show the utility name","title":"Fix"},{"location":"CHANGELOG.html#004---2018-01-16","text":"","title":"0.0.4 - 2018-01-16"},{"location":"CHANGELOG.html#add_42","text":"Release phar files on Github","title":"Add"},{"location":"CHANGELOG.html#change_38","text":"Various code style improvements Readme for corrections and coverage","title":"Change"},{"location":"CHANGELOG.html#003---2018-01-14","text":"","title":"0.0.3 - 2018-01-14"},{"location":"CHANGELOG.html#add_43","text":"Keep container on pipeline step failure automatically --verbatim option to only output from pipeline, not pipelines","title":"Add"},{"location":"CHANGELOG.html#change_39","text":"--help information Various code style improvements","title":"Change"},{"location":"CHANGELOG.html#002---2018-01-11","text":"","title":"0.0.2 - 2018-01-11"},{"location":"CHANGELOG.html#add_44","text":"Brace glob pattern in pipelines Change log","title":"Add"},{"location":"CHANGELOG.html#001---2018-01-10","text":"","title":"0.0.1 - 2018-01-10"},{"location":"CHANGELOG.html#add_45","text":"Initial release","title":"Add"},{"location":"COPYING.html","text":"Copying \u00b6 .md-nav__list .md-nav__list .md-nav__list {display: none} a[title^=\"Edit this page\"].md-content__button.md-icon {display: none} Pipelines - Run Bitbucket Pipelines Wherever They Dock. Copyright (C) 2017-2021 Tom Klingenberg This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see \\ https://www.gnu.org/licenses/ . GNU Affero General Public License \u00b6 GNU AFFERO GENERAL PUBLIC LICENSE Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/> Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software. A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public. The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version. An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS 0. Definitions. \"This License\" refers to version 3 of the GNU Affero General Public License. \"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations. To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work. A \"covered work\" means either the unmodified Program or a work based on the Program. To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. 1. Source Code. The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work. A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. 2. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. 3. Protecting Users' Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures. 4. Conveying Verbatim Copies. You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. 5. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. 6. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. 7. Additional Terms. \"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. 8. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. 9. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. 10. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. 11. Patents. A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\". A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. 12. No Surrender of Others' Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. 13. Remote Network Interaction; Use with the GNU General Public License. Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License. 14. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. 15. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 16. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 17. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. <one line to give the program's name and a brief idea of what it does.> Copyright (C) <year> <name of author> This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see <https://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements. You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see <https://www.gnu.org/licenses/>. SPDX \u00b6 Full Name: GNU Affero General Public License v3.0 or later Short Identifier: AGPL-3.0-or-later Reference: https://spdx.org/licenses/AGPL-3.0-or-later.html Additional Licensing \u00b6 Some files in the project have been copied, are a derivative work and the original file(s) are under a different than and compatible to the projects' license. Their original license is preserved to ease exchange with the upstream projects. Affected files follow with a description and their license: Docker/Open Container Initiative (Apache License Version 2.0) \u00b6 The file doc/DOCKER-NAME-TAG.md is derived from Docker documentation (Copyright 2013-2017 Docker, Inc.) and Open Container Initiative (OCI) Image Format Specification (Copyright 2016 The Linux Foundation); both used under the Apache License Version 2.0 . Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. SPDX \u00b6 Full Name: Apache License 2.0 Short Identifier: Apache-2.0 Reference: https://spdx.org/licenses/Apache-2.0.html Timestamps (MIT License) \u00b6 The file src/PharBuild/Timestamps.php is derived from phar-utils (Seldaek/Jordi Boggiano) and used under the MIT License . Copyright (c) 2015 Jordi Boggiano Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. SPDX \u00b6 Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html Coverage Checker (MIT License) \u00b6 The file lib/build/coverage-checker.php is based on the coverage-checker.php script written by Marco Pivetta (ocramius) and used under the MIT License per the licensing in the VersionEyeModule file: THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This software consists of voluntary contributions made by many individuals and is licensed under the MIT license. SPDX \u00b6 Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html Schema (MIT License) \u00b6 The file lib/pipelines/schema/piplines-schema.json is derived from atlascode (Atlassian Partner Integrations) and used under the MIT License . MIT License Copyright (c) Atlassian and others. All rights reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. SPDX \u00b6 Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html Licensing for Libraries \u00b6 The project depends on third party software that is managed via Composer. These packages are under a different than and compatible to the projects' license. Their original license is preserved to ease exchange with the upstream projects when forked. Json-Schema (MIT License) \u00b6 From the package https://packagist.org/packages/justinrainbow/json-schema MIT License Copyright (c) 2016 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. SPDX \u00b6 Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html Symfony-Yaml (MIT License) \u00b6 From the package https://packagist.org/packages/ktomk/symfony-yaml Copyright 2020, 2021, 2022 Tom Klingenberg This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <https://www.gnu.org/licenses/>. Parts or base of this software are originally licensed: Copyright (c) 2004-2015 Fabien Potencier Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. SPDX \u00b6 Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html HTML Documentation \u00b6 Parts of the HTML documentation are derived from other projects. These projects are under a different than and compatible to the Pipelines projects' license. Projects derived from in the HTML documentation follow with a description and their license. When files from these projects have been incorporated into the pipelines project, they remain under their original license to ease contributing changes back upstream. Mkdocs (BSD 2 Clause) \u00b6 The HTML documentation is build by and derived from Mkdocs - Project documentation with Markdown , incorporating its theme files which are used under license: Copyright \u00a9 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. SPDX \u00b6 Full Name: BSD 2-Clause \"Simplified\" License Short Identifier: BSD-2-Clause Reference: https://spdx.org/licenses/BSD-2-Clause.html Material for Mkdocs (MIT License) \u00b6 The Mkdocs theme is extended by Material for Mkdocs - A Material Design theme for MkDocs and its files are used under license. Project modifications and incorporation on file-level can be found in the lib/build/mkdocs directory. Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. SPDX \u00b6 Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html Iframe-Worker (MIT License) \u00b6 The Material for Mkdocs based theme is extended in the pipelines project with iframe-worker - A tiny WebWorker polyfill for the file:// protocol and used under license. The pipelines repository is with the minified library only, see file assets/javascripts/iframe-worker.js , for sources please see the upstream project and/or the fork kept to the date of the non-source file shipping with the pipelines documentation located at https://github.com/ktomk/iframe-worker . Copyright (c) 2020 Martin Donath <martin.donath@squidfunk.com> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. SPDX \u00b6 Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html Font Awesome (Font Awesome Free License) \u00b6 The Material for Mkdocs theme contains SVG files (named Icons in the Font Awesome Free License ) from the Font Awesome - The iconic SVG, font, and CSS framework project. Some of these SVG files are put into the HTML output and used under license: Font Awesome Free License ------------------------- Font Awesome Free is free, open source, and GPL friendly. You can use it for commercial projects, open source projects, or really almost whatever you want. Full Font Awesome Free license: https://fontawesome.com/license/free. # Icons: CC BY 4.0 License (https://creativecommons.org/licenses/by/4.0/) In the Font Awesome Free download, the CC BY 4.0 license applies to all icons packaged as SVG and JS file types. # Fonts: SIL OFL 1.1 License (https://scripts.sil.org/OFL) In the Font Awesome Free download, the SIL OFL license applies to all icons packaged as web and desktop font files. # Code: MIT License (https://opensource.org/licenses/MIT) In the Font Awesome Free download, the MIT license applies to all non-font and non-icon files. # Attribution Attribution is required by MIT, SIL OFL, and CC BY licenses. Downloaded Font Awesome Free files already contain embedded comments with sufficient attribution, so you shouldn't need to do anything additional when using these files normally. We've kept attribution comments terse, so we ask that you do not actively work to remove them from files, especially code. They're a great way for folks to learn about Font Awesome. # Brand Icons All brand icons are trademarks of their respective owners. The use of these trademarks does not indicate endorsement of the trademark holder by Font Awesome, nor vice versa. **Please do not use brand logos for any purpose except to represent the company, product, or service to which they refer.** SPDX \u00b6 Full Name: Creative Commons Attribution 4.0 International Short Identifier: CC-BY-4.0 Reference: https://spdx.org/licenses/CC-BY-4.0.html Additional Credits \u00b6 Some software that has no specific requirement when (indirectly) in use to be made available nor noted, does not belong into the bill of material. Nevertheless credit where credit is due. Mkdocs Local-Search Plugin (MIT License) \u00b6 The HTML documentation could not be build without the Mkdocs Local Search Plugin written by Lars Wilhelmer; A MkDocs plugin to make the native \"search\" plugin work locally (file:// protocol) . Its license does not require a notion comme \u00e7a as the documentation is already build (when shipped/published), nevertheless the build would not work without it and a HTML documentation that works (static HTML is best served fresh on platter) can not be taken for granted these days and requires notion. Copyright (c) 2019 Lars Wilhelmer Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. SPDX \u00b6 Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html","title":"Copying"},{"location":"COPYING.html#copying","text":".md-nav__list .md-nav__list .md-nav__list {display: none} a[title^=\"Edit this page\"].md-content__button.md-icon {display: none} Pipelines - Run Bitbucket Pipelines Wherever They Dock. Copyright (C) 2017-2021 Tom Klingenberg This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see \\ https://www.gnu.org/licenses/ .","title":"Copying"},{"location":"COPYING.html#gnu-affero-general-public-license","text":"GNU AFFERO GENERAL PUBLIC LICENSE Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/> Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software. A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public. The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version. An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS 0. Definitions. \"This License\" refers to version 3 of the GNU Affero General Public License. \"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations. To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work. A \"covered work\" means either the unmodified Program or a work based on the Program. To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. 1. Source Code. The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work. A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. 2. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. 3. Protecting Users' Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures. 4. Conveying Verbatim Copies. You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. 5. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. 6. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. 7. Additional Terms. \"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. 8. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. 9. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. 10. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. 11. Patents. A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\". A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. 12. No Surrender of Others' Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. 13. Remote Network Interaction; Use with the GNU General Public License. Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License. 14. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. 15. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 16. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 17. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. <one line to give the program's name and a brief idea of what it does.> Copyright (C) <year> <name of author> This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see <https://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements. You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see <https://www.gnu.org/licenses/>.","title":"GNU Affero General Public License"},{"location":"COPYING.html#spdx","text":"Full Name: GNU Affero General Public License v3.0 or later Short Identifier: AGPL-3.0-or-later Reference: https://spdx.org/licenses/AGPL-3.0-or-later.html","title":"SPDX"},{"location":"COPYING.html#additional-licensing","text":"Some files in the project have been copied, are a derivative work and the original file(s) are under a different than and compatible to the projects' license. Their original license is preserved to ease exchange with the upstream projects. Affected files follow with a description and their license:","title":"Additional Licensing"},{"location":"COPYING.html#dockeropen-container-initiative-apache-license-version-20","text":"The file doc/DOCKER-NAME-TAG.md is derived from Docker documentation (Copyright 2013-2017 Docker, Inc.) and Open Container Initiative (OCI) Image Format Specification (Copyright 2016 The Linux Foundation); both used under the Apache License Version 2.0 . Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Docker/Open Container Initiative (Apache License Version 2.0)"},{"location":"COPYING.html#spdx_1","text":"Full Name: Apache License 2.0 Short Identifier: Apache-2.0 Reference: https://spdx.org/licenses/Apache-2.0.html","title":"SPDX"},{"location":"COPYING.html#timestamps-mit-license","text":"The file src/PharBuild/Timestamps.php is derived from phar-utils (Seldaek/Jordi Boggiano) and used under the MIT License . Copyright (c) 2015 Jordi Boggiano Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Timestamps (MIT License)"},{"location":"COPYING.html#spdx_2","text":"Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html","title":"SPDX"},{"location":"COPYING.html#coverage-checker-mit-license","text":"The file lib/build/coverage-checker.php is based on the coverage-checker.php script written by Marco Pivetta (ocramius) and used under the MIT License per the licensing in the VersionEyeModule file: THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This software consists of voluntary contributions made by many individuals and is licensed under the MIT license.","title":"Coverage Checker (MIT License)"},{"location":"COPYING.html#spdx_3","text":"Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html","title":"SPDX"},{"location":"COPYING.html#schema-mit-license","text":"The file lib/pipelines/schema/piplines-schema.json is derived from atlascode (Atlassian Partner Integrations) and used under the MIT License . MIT License Copyright (c) Atlassian and others. All rights reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Schema (MIT License)"},{"location":"COPYING.html#spdx_4","text":"Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html","title":"SPDX"},{"location":"COPYING.html#licensing-for-libraries","text":"The project depends on third party software that is managed via Composer. These packages are under a different than and compatible to the projects' license. Their original license is preserved to ease exchange with the upstream projects when forked.","title":"Licensing for Libraries"},{"location":"COPYING.html#json-schema-mit-license","text":"From the package https://packagist.org/packages/justinrainbow/json-schema MIT License Copyright (c) 2016 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Json-Schema (MIT License)"},{"location":"COPYING.html#spdx_5","text":"Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html","title":"SPDX"},{"location":"COPYING.html#symfony-yaml-mit-license","text":"From the package https://packagist.org/packages/ktomk/symfony-yaml Copyright 2020, 2021, 2022 Tom Klingenberg This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <https://www.gnu.org/licenses/>. Parts or base of this software are originally licensed: Copyright (c) 2004-2015 Fabien Potencier Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Symfony-Yaml (MIT License)"},{"location":"COPYING.html#spdx_6","text":"Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html","title":"SPDX"},{"location":"COPYING.html#html-documentation","text":"Parts of the HTML documentation are derived from other projects. These projects are under a different than and compatible to the Pipelines projects' license. Projects derived from in the HTML documentation follow with a description and their license. When files from these projects have been incorporated into the pipelines project, they remain under their original license to ease contributing changes back upstream.","title":"HTML Documentation"},{"location":"COPYING.html#mkdocs-bsd-2-clause","text":"The HTML documentation is build by and derived from Mkdocs - Project documentation with Markdown , incorporating its theme files which are used under license: Copyright \u00a9 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Mkdocs (BSD 2 Clause)"},{"location":"COPYING.html#spdx_7","text":"Full Name: BSD 2-Clause \"Simplified\" License Short Identifier: BSD-2-Clause Reference: https://spdx.org/licenses/BSD-2-Clause.html","title":"SPDX"},{"location":"COPYING.html#material-for-mkdocs-mit-license","text":"The Mkdocs theme is extended by Material for Mkdocs - A Material Design theme for MkDocs and its files are used under license. Project modifications and incorporation on file-level can be found in the lib/build/mkdocs directory. Copyright (c) 2016-2020 Martin Donath <martin.donath@squidfunk.com> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Material for Mkdocs (MIT License)"},{"location":"COPYING.html#spdx_8","text":"Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html","title":"SPDX"},{"location":"COPYING.html#iframe-worker-mit-license","text":"The Material for Mkdocs based theme is extended in the pipelines project with iframe-worker - A tiny WebWorker polyfill for the file:// protocol and used under license. The pipelines repository is with the minified library only, see file assets/javascripts/iframe-worker.js , for sources please see the upstream project and/or the fork kept to the date of the non-source file shipping with the pipelines documentation located at https://github.com/ktomk/iframe-worker . Copyright (c) 2020 Martin Donath <martin.donath@squidfunk.com> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Iframe-Worker (MIT License)"},{"location":"COPYING.html#spdx_9","text":"Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html","title":"SPDX"},{"location":"COPYING.html#font-awesome-font-awesome-free-license","text":"The Material for Mkdocs theme contains SVG files (named Icons in the Font Awesome Free License ) from the Font Awesome - The iconic SVG, font, and CSS framework project. Some of these SVG files are put into the HTML output and used under license: Font Awesome Free License ------------------------- Font Awesome Free is free, open source, and GPL friendly. You can use it for commercial projects, open source projects, or really almost whatever you want. Full Font Awesome Free license: https://fontawesome.com/license/free. # Icons: CC BY 4.0 License (https://creativecommons.org/licenses/by/4.0/) In the Font Awesome Free download, the CC BY 4.0 license applies to all icons packaged as SVG and JS file types. # Fonts: SIL OFL 1.1 License (https://scripts.sil.org/OFL) In the Font Awesome Free download, the SIL OFL license applies to all icons packaged as web and desktop font files. # Code: MIT License (https://opensource.org/licenses/MIT) In the Font Awesome Free download, the MIT license applies to all non-font and non-icon files. # Attribution Attribution is required by MIT, SIL OFL, and CC BY licenses. Downloaded Font Awesome Free files already contain embedded comments with sufficient attribution, so you shouldn't need to do anything additional when using these files normally. We've kept attribution comments terse, so we ask that you do not actively work to remove them from files, especially code. They're a great way for folks to learn about Font Awesome. # Brand Icons All brand icons are trademarks of their respective owners. The use of these trademarks does not indicate endorsement of the trademark holder by Font Awesome, nor vice versa. **Please do not use brand logos for any purpose except to represent the company, product, or service to which they refer.**","title":"Font Awesome (Font Awesome Free License)"},{"location":"COPYING.html#spdx_10","text":"Full Name: Creative Commons Attribution 4.0 International Short Identifier: CC-BY-4.0 Reference: https://spdx.org/licenses/CC-BY-4.0.html","title":"SPDX"},{"location":"COPYING.html#additional-credits","text":"Some software that has no specific requirement when (indirectly) in use to be made available nor noted, does not belong into the bill of material. Nevertheless credit where credit is due.","title":"Additional Credits"},{"location":"COPYING.html#mkdocs-local-search-plugin-mit-license","text":"The HTML documentation could not be build without the Mkdocs Local Search Plugin written by Lars Wilhelmer; A MkDocs plugin to make the native \"search\" plugin work locally (file:// protocol) . Its license does not require a notion comme \u00e7a as the documentation is already build (when shipped/published), nevertheless the build would not work without it and a HTML documentation that works (static HTML is best served fresh on platter) can not be taken for granted these days and requires notion. Copyright (c) 2019 Lars Wilhelmer Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Mkdocs Local-Search Plugin (MIT License)"},{"location":"COPYING.html#spdx_11","text":"Full Name: MIT License Short Identifier: MIT Reference: https://spdx.org/licenses/MIT.html","title":"SPDX"},{"location":"doc/CONFIGURATION-PARAMETERS.html","text":"Configuration Parameters \u00b6 Currently there are some configuration parameters for pipelines . They can be set with -c <name>=<value> and are set to their default values. There is yet no configuration file or similar. docker.client.path \u00b6 Path of the docker client binary within the pipeline step container. Default: /usr/bin/docker Related option: --docker-client docker.socket.path \u00b6 Path to docker socket file, mount point within the container to mount the docker socket and as well to check for the docker socket on the host and within mount binds. Default: /var/run/docker.sock Related environment parameter: DOCKER_HOST (See Environment variables in Use the Docker command line ) Related: How-To Rootless Pipelines script.bash-runner \u00b6 Conditionally run scripts with /bin/bash instead of script.runner ( /bin/sh by default) if /bin/bash is a standard file and executable in the container. Setting script.bash-runner to false prevents this check. Default: true (bool) Related configuration parameter: script.runner script.exit-early \u00b6 Executing the step and after scripts use set -e to exit early. To eliminate side-effects, a more strict check after each script command can be enabled by setting the parameter to true with the effect that set -e / set +e is ineffective and the step script stops after any shell pipe with a non-zero exit status. Default: false (bool). script.runner \u00b6 The default script runner ( /bin/sh ), step scripts are run with this command. The step script is passed to the runner via standard input. The runner must be executable in the container. When script.bash-runner is true (default), the script.runner may be overridden by /bin/bash if found in the container. This behaviour can be disabled by setting script.bash-runner is false . Default: /bin/sh Related configuration parameter: script.bash-runner Related option: --step-script step.clone-path \u00b6 Mount point / destination of the project files within a pipeline step container. Is the absolute path inside the container and must start with a slash ( / ), multiple directory components are supported, none of them must be relative (no . or .. segments allowed). The path must not end with a slash. Default: /app Related environment parameter: BITBUCKET_CLONE_DIR (Compare Pipelines Environment Variable Usage )","title":"Configuration Parameters"},{"location":"doc/CONFIGURATION-PARAMETERS.html#configuration-parameters","text":"Currently there are some configuration parameters for pipelines . They can be set with -c <name>=<value> and are set to their default values. There is yet no configuration file or similar.","title":"Configuration Parameters"},{"location":"doc/CONFIGURATION-PARAMETERS.html#dockerclientpath","text":"Path of the docker client binary within the pipeline step container. Default: /usr/bin/docker Related option: --docker-client","title":"docker.client.path"},{"location":"doc/CONFIGURATION-PARAMETERS.html#dockersocketpath","text":"Path to docker socket file, mount point within the container to mount the docker socket and as well to check for the docker socket on the host and within mount binds. Default: /var/run/docker.sock Related environment parameter: DOCKER_HOST (See Environment variables in Use the Docker command line ) Related: How-To Rootless Pipelines","title":"docker.socket.path"},{"location":"doc/CONFIGURATION-PARAMETERS.html#scriptbash-runner","text":"Conditionally run scripts with /bin/bash instead of script.runner ( /bin/sh by default) if /bin/bash is a standard file and executable in the container. Setting script.bash-runner to false prevents this check. Default: true (bool) Related configuration parameter: script.runner","title":"script.bash-runner"},{"location":"doc/CONFIGURATION-PARAMETERS.html#scriptexit-early","text":"Executing the step and after scripts use set -e to exit early. To eliminate side-effects, a more strict check after each script command can be enabled by setting the parameter to true with the effect that set -e / set +e is ineffective and the step script stops after any shell pipe with a non-zero exit status. Default: false (bool).","title":"script.exit-early"},{"location":"doc/CONFIGURATION-PARAMETERS.html#scriptrunner","text":"The default script runner ( /bin/sh ), step scripts are run with this command. The step script is passed to the runner via standard input. The runner must be executable in the container. When script.bash-runner is true (default), the script.runner may be overridden by /bin/bash if found in the container. This behaviour can be disabled by setting script.bash-runner is false . Default: /bin/sh Related configuration parameter: script.bash-runner Related option: --step-script","title":"script.runner"},{"location":"doc/CONFIGURATION-PARAMETERS.html#stepclone-path","text":"Mount point / destination of the project files within a pipeline step container. Is the absolute path inside the container and must start with a slash ( / ), multiple directory components are supported, none of them must be relative (no . or .. segments allowed). The path must not end with a slash. Default: /app Related environment parameter: BITBUCKET_CLONE_DIR (Compare Pipelines Environment Variable Usage )","title":"step.clone-path"},{"location":"doc/DEVELOPMENT.html","text":"Pipelines Utility Development \u00b6 Pipelines is a command-line utility written in PHP [PHP] and developed as a composer [COMPOSER] package with a phar file build [PHAR]. The currently pinned composer version is 2.1.11 , pinning the version is mostly important for the phar build as it is not reproducible otherwise. In general any version of composer is supported. Composer 2 Support Composer 2 is the standard in the Pipelines project. Previously it was Composer 1. Just note that the composer.lock file should be written with the pinned composer version. Currently the composer.lock targets a PHP 7.4 system with Phpunit 9 for development. It should be similar for PHP 8.0. Xdebug 3 Support Xdebug 3.0.0 [XDEBUG] is supported but may make some tests, especially the PHPT tests, fail (segmentation faults). If so then check you have got the latest Xdebug 3 version. As a fall-back Xdebug 2 can be used for any PHP version below PHP 8.0. See Supported Versions and Compatibility also for older Xdebug and PHP versions . After checkout run composer install to bootstrap and composer ci for the local CI. It will also run pipelines pipelines, so docker is required, the shell tests require bash. Use composer run-script --list to get a list of all composer scripts. composer ci should run through after checkout and before committing any changes. while developing, composer dev is a good intermediate. PHP Backwards Compatibility Even the pipelines project accepts issues/PRs with pseudo-code, when it is about concrete PHP code, the requirement is that it is (backwards) compatible with PHP 5.3.3 (released Jul 2010). This is not a common requirement and the pipelines project tries to offer support even in local development (like running the phpunit tests under the minimum version as a custom pipeline). If you use Phpstorm [PHPSTORM] enable the PHP 5.3 language level for your cloned pipelines project. This helps preventing falling into new language syntax unnoticed. In any case don't fear, this is normally easy to sort-out. Just leave a comment in case things didn't work for you out of the box and share the PHP version in use. Support of Different PHP Versions \u00b6 While pipelines runs and builds on PHP 5.3.3+, the PHP version when developing pipelines may differ. Please see the following table for a break-down based on PHP versions incl. remarks: CAPABILITY 5.3 5.4 5.5 5.6 7.0 7.1 7.2 7.3 7.4 8.0 8.1 composer *1 X X X X X X X X X X X phpunit (version) X (4) X (4 *2) X (4 *2) X (5) X (6) X (6) X (7) X (8) X (8/ 9 *3) X (8 *2/ 9) X (8 *2/ 9) phar-build X X X X X X X X X X X php-cs-fixer X X X *4 xdebug 3 *5 X *6 X X X X Remarks Composer 1 and 2 are supported. Configuration expected to work but undefined in (local) CI. See as well Phpunit/PHP version compatibility matrix [PHPUNIT], e.g. it is possible to run PHP 8 with Phpunit 8 (since 30 th Nov 2020). Phpunit 9 is used in the local development package ( composer.lock ), remote Travis CI [TRAVIS-CI] is using a Phpunit 8 profile for PHP 7.4. PHP-CS-Fixer is used in pipelines to control the code style, so relatively central in the CI pipeline. Unfortunately, it does (did) not cover all the PHP versions that pipelines supports. Especially with PHP 8.1 it refuses to install with composer. This requires to fake the PHP version (e.g. as 7.4.99) or to use the --ignore-platform-reqs - or more precisely --ignore-platform-req=php - option of composer to install with PHP 8.1. Xdebug 3 is required for PHP 8 and in tests for all PHP versions it supports. It is somewhat new, segfaults may have been spotted in CI runs (local, on Scrutinizer). Check the version, and then an upgrade. Pipelines uses Xdebug in development for code-coverage and step-debugging. See Xdebug 3 Support above. Xdebug 3 support for PHP 7.2 is limited to security support. Same applies to Xdebug 2.9 for PHP 7.2. No other Xdebug version next to 3.0 or 2.9 is supported at all for PHP 7.2. See Xdebug 3 Support above. Currently the test-suite does not work for PHP 7.2 and Xdebug 3 running on Phpunit 7. PHP Version and Reproducible PHAR Build \u00b6 The phar build, on a clean checkout, with the pinned composer version creates the same phar file (signature/checksum) regardless of the php version in use. Invoke the phar build with: composer build Afterwards find the phar-file as build/pipelines.phar . Test / Build with Different PHP Versions \u00b6 The local CI can be run with different php versions by running composer with a specific PHP version: php8.0 -f \"$(composer which)\" -- ci (use ci or any other project composer script command) The shell tests and many other scripts when using PHP are respecting the PHP_BINARY environment variable: $ PHP_BINARY=php5.6 lib/script/ppconf.sh self-test ppconf self-test bash....: /bin/bash GNU bash, version 4.4.20(1)-release (x86_64-pc-linux-gnu) composer: /home/user/bin/composer Composer version 1.10.17 2020-10-30 22:31:58 (/home/user/bin/composer.phar) find....: /usr/bin/find find (GNU findutils) 4.7.0-git gpg.....: /usr/bin/gpg gpg (GnuPG) 2.2.4 make....: /usr/bin/make GNU Make 4.1 openssl.: /usr/bin/openssl OpenSSL 1.1.1g 21 Apr 2020 os......: NAME=\"Ubuntu\" os......: VERSION=\"18.04.5 LTS (Bionic Beaver)\" os......: ID=ubuntu php.....: /usr/bin/php5.6 PHP 5.6.40-38+ubuntu18.04.1+deb.sury.org+1 (cli) (/usr/bin/php5.6) python..: /usr/bin/python Python 2.7.17 python3.: /usr/bin/python3 Python 3.6.9 sed.....: /bin/sed sed (GNU sed) 4.4 sh......: /bin/sh /bin/dash tar.....: /bin/tar tar (GNU tar) 1.29 unzip...: /usr/bin/unzip UnZip 6.00 of 20 April 2009, by Debian. Original by Info-ZIP. xdebug..: no (note the given \"php\" version information) References \u00b6 [COMPOSER]: https://getcomposer.org/ [PHAR]: https://www.php.net/phar [PHP]: https://www.php.net/ [PHP-CS-FIXER]: https://github.com/FriendsOfPHP/PHP-CS-Fixer [PHPSTORM]: https://www.jetbrains.com/phpstorm/ [PHPUNIT]: https://phpunit.de/supported-versions.html [TRAVIS-CI]: https://travis-ci.com/github/ktomk/pipelines [XDEBUG]: https://xdebug.org/","title":"Utility Development"},{"location":"doc/DEVELOPMENT.html#pipelines-utility-development","text":"Pipelines is a command-line utility written in PHP [PHP] and developed as a composer [COMPOSER] package with a phar file build [PHAR]. The currently pinned composer version is 2.1.11 , pinning the version is mostly important for the phar build as it is not reproducible otherwise. In general any version of composer is supported. Composer 2 Support Composer 2 is the standard in the Pipelines project. Previously it was Composer 1. Just note that the composer.lock file should be written with the pinned composer version. Currently the composer.lock targets a PHP 7.4 system with Phpunit 9 for development. It should be similar for PHP 8.0. Xdebug 3 Support Xdebug 3.0.0 [XDEBUG] is supported but may make some tests, especially the PHPT tests, fail (segmentation faults). If so then check you have got the latest Xdebug 3 version. As a fall-back Xdebug 2 can be used for any PHP version below PHP 8.0. See Supported Versions and Compatibility also for older Xdebug and PHP versions . After checkout run composer install to bootstrap and composer ci for the local CI. It will also run pipelines pipelines, so docker is required, the shell tests require bash. Use composer run-script --list to get a list of all composer scripts. composer ci should run through after checkout and before committing any changes. while developing, composer dev is a good intermediate. PHP Backwards Compatibility Even the pipelines project accepts issues/PRs with pseudo-code, when it is about concrete PHP code, the requirement is that it is (backwards) compatible with PHP 5.3.3 (released Jul 2010). This is not a common requirement and the pipelines project tries to offer support even in local development (like running the phpunit tests under the minimum version as a custom pipeline). If you use Phpstorm [PHPSTORM] enable the PHP 5.3 language level for your cloned pipelines project. This helps preventing falling into new language syntax unnoticed. In any case don't fear, this is normally easy to sort-out. Just leave a comment in case things didn't work for you out of the box and share the PHP version in use.","title":"Pipelines Utility Development"},{"location":"doc/DEVELOPMENT.html#support-of-different-php-versions","text":"While pipelines runs and builds on PHP 5.3.3+, the PHP version when developing pipelines may differ. Please see the following table for a break-down based on PHP versions incl. remarks: CAPABILITY 5.3 5.4 5.5 5.6 7.0 7.1 7.2 7.3 7.4 8.0 8.1 composer *1 X X X X X X X X X X X phpunit (version) X (4) X (4 *2) X (4 *2) X (5) X (6) X (6) X (7) X (8) X (8/ 9 *3) X (8 *2/ 9) X (8 *2/ 9) phar-build X X X X X X X X X X X php-cs-fixer X X X *4 xdebug 3 *5 X *6 X X X X Remarks Composer 1 and 2 are supported. Configuration expected to work but undefined in (local) CI. See as well Phpunit/PHP version compatibility matrix [PHPUNIT], e.g. it is possible to run PHP 8 with Phpunit 8 (since 30 th Nov 2020). Phpunit 9 is used in the local development package ( composer.lock ), remote Travis CI [TRAVIS-CI] is using a Phpunit 8 profile for PHP 7.4. PHP-CS-Fixer is used in pipelines to control the code style, so relatively central in the CI pipeline. Unfortunately, it does (did) not cover all the PHP versions that pipelines supports. Especially with PHP 8.1 it refuses to install with composer. This requires to fake the PHP version (e.g. as 7.4.99) or to use the --ignore-platform-reqs - or more precisely --ignore-platform-req=php - option of composer to install with PHP 8.1. Xdebug 3 is required for PHP 8 and in tests for all PHP versions it supports. It is somewhat new, segfaults may have been spotted in CI runs (local, on Scrutinizer). Check the version, and then an upgrade. Pipelines uses Xdebug in development for code-coverage and step-debugging. See Xdebug 3 Support above. Xdebug 3 support for PHP 7.2 is limited to security support. Same applies to Xdebug 2.9 for PHP 7.2. No other Xdebug version next to 3.0 or 2.9 is supported at all for PHP 7.2. See Xdebug 3 Support above. Currently the test-suite does not work for PHP 7.2 and Xdebug 3 running on Phpunit 7.","title":"Support of Different PHP Versions"},{"location":"doc/DEVELOPMENT.html#php-version-and-reproducible-phar-build","text":"The phar build, on a clean checkout, with the pinned composer version creates the same phar file (signature/checksum) regardless of the php version in use. Invoke the phar build with: composer build Afterwards find the phar-file as build/pipelines.phar .","title":"PHP Version and Reproducible PHAR Build"},{"location":"doc/DEVELOPMENT.html#test--build-with-different-php-versions","text":"The local CI can be run with different php versions by running composer with a specific PHP version: php8.0 -f \"$(composer which)\" -- ci (use ci or any other project composer script command) The shell tests and many other scripts when using PHP are respecting the PHP_BINARY environment variable: $ PHP_BINARY=php5.6 lib/script/ppconf.sh self-test ppconf self-test bash....: /bin/bash GNU bash, version 4.4.20(1)-release (x86_64-pc-linux-gnu) composer: /home/user/bin/composer Composer version 1.10.17 2020-10-30 22:31:58 (/home/user/bin/composer.phar) find....: /usr/bin/find find (GNU findutils) 4.7.0-git gpg.....: /usr/bin/gpg gpg (GnuPG) 2.2.4 make....: /usr/bin/make GNU Make 4.1 openssl.: /usr/bin/openssl OpenSSL 1.1.1g 21 Apr 2020 os......: NAME=\"Ubuntu\" os......: VERSION=\"18.04.5 LTS (Bionic Beaver)\" os......: ID=ubuntu php.....: /usr/bin/php5.6 PHP 5.6.40-38+ubuntu18.04.1+deb.sury.org+1 (cli) (/usr/bin/php5.6) python..: /usr/bin/python Python 2.7.17 python3.: /usr/bin/python3 Python 3.6.9 sed.....: /bin/sed sed (GNU sed) 4.4 sh......: /bin/sh /bin/dash tar.....: /bin/tar tar (GNU tar) 1.29 unzip...: /usr/bin/unzip UnZip 6.00 of 20 April 2009, by Debian. Original by Info-ZIP. xdebug..: no (note the given \"php\" version information)","title":"Test / Build with Different PHP Versions"},{"location":"doc/DEVELOPMENT.html#references","text":"[COMPOSER]: https://getcomposer.org/ [PHAR]: https://www.php.net/phar [PHP]: https://www.php.net/ [PHP-CS-FIXER]: https://github.com/FriendsOfPHP/PHP-CS-Fixer [PHPSTORM]: https://www.jetbrains.com/phpstorm/ [PHPUNIT]: https://phpunit.de/supported-versions.html [TRAVIS-CI]: https://travis-ci.com/github/ktomk/pipelines [XDEBUG]: https://xdebug.org/","title":"References"},{"location":"doc/DOCKER-NAME-TAG.html","text":"Docker Container (Image/Tag) Name Syntax \u00b6 This document describes the syntax of a docker container name, an image name w/ or w/o tag or digest (supported since 0.0.64). Created for the image name validation in pipelines and kept for reference and further improvements. Syntax | Known Issues | Grammar | References Syntax \u00b6 The textual description is taken from the Docker documentation and following is a transposition into a more formal grammar similar to EBNF. Encoding is US-ASCII; grammar at once . container ::= image-name ( \":\" tag-name | \"@\" digest )? /* \":\" ASCII 58 x3A colon */ Image Name | Tag Name | Digest Image Name \u00b6 A container consists of the name of an image optionally followed by a tag or digest (both separated by a colon). image-name ::= prefix? name-components prefix ::= hostname port? \"/\" hostname ::= [a-zA-Z0-9.-]+ port ::= \":\" [0-9]+ /* \"/\" ASCII 47 x2F slash */ /* \"-\" ASCII 45 x2D dash */ An image name is made up of slash-separated name components, optionally prefixed by a registry hostname. The hostname must comply with standard DNS rules, but may not contain underscores. If a hostname is present, it may optionally be followed by a port number in the format :8080 . If not present, the command uses Docker\u2019s public registry located at registry-1.docker.io by default. name-components ::= name-component ( \"/\" name-component )? name-component ::= name ( name-separator name )* name ::= [a-z0-9]+ name-separator ::= ( \".\" | \"_\" \"_\"? | \"-\"+ ) /* \".\" ASCII 46 x2E period */ /* \"_\" ASCII 95 x5F underscore */ Name components may contain lowercase letters, digits and separators. A separator is defined as a period, one or two underscores, or one or more dashes. A name component may not start or end with a separator. Tag Name \u00b6 A tag name must be valid ASCII and may contain lowercase and uppercase letters, digits, underscores, periods and dashes. A tag name may not start with a period or a dash and may contain a maximum of 128 characters. tag-name ::= tag-start tag-follow{0,127} tag-start ::= [a-zA-Z0-9_] tag-follow ::= tag-start | [.-] Digest \u00b6 In pipelines the digest is first of all supported to allow docker to fetch container images from docker-hub by content / collision-resistant hash of the bytes (the digest) and not by tag (only; tags are not stable). Image with: Tag: ktomk/pipelines:busybox Digest: ktomk/pipelines@sha256:2ef9a59041a7c4f36001abaec4fe7c10c26c1ead4da11515ba2af346fe60ddac In practice the SHA-256 is in use by all compliant implementations (this is Open Container Interface (OCI) material). digest ::= algorithm \":\" encoded algorithm ::= algorithm-component (algorithm-separator algorithm-component)* algorithm-component ::= [a-z0-9]+ algorithm-separator ::= [+._-] encoded ::= [a-zA-Z0-9=_-]+ /* \"=\" ASCII 61 x3D equals-sign */ Known Issues \u00b6 The hostname part is not RFC conform (also depending on which RFC) and has not yet been double checked with standard DNS rules. The digest part currently only verifies per the grammar for forward compatibility. No constraint is made on validation on the algorithm component for example. Often this level of specific is not needed for pipelines so prone to errors apart of the validation itself. Image/container names are passed to docker(1) which might then unable to pull an image even if the regular expression based validation in the pipelines utility did let it pass as valid. Double check the hostname and the digest then. Grammar \u00b6 For field formats described in this document, we use a limited subset of Extended Backus-Naur Form [EBNF], similar to that used by the XML specification [XML-EBNF]. Encoding is US-ASCII. container ::= image-name ( \":\" ( tag-name | digest ) )? image-name ::= prefix? name-components prefix ::= hostname port? \"/\" hostname ::= [a-zA-Z0-9.-]+ port ::= \":\" [0-9]+ name-components ::= name-comp ( \"/\" name-component )? name-component ::= name ( name-separator name )* name ::= [a-z0-9]+ name-separator ::= ( \".\" | \"_\" \"_\"? | \"-\"+ ) tag-name ::= tag-start tag-follow{0,127} tag-start ::= [a-zA-Z0-9_] tag-follow ::= tag-start | [.-] digest ::= algorithm \":\" encoded algorithm ::= algorithm-component (algorithm-separator algorithm-component)* algorithm-component ::= [a-z0-9]+ algorithm-separator ::= [+._-] encoded ::= [a-zA-Z0-9=_-]+ /* \":\" ASCII 58 x3A colon */ /* \"/\" ASCII 47 x2F slash */ /* \"-\" ASCII 45 x2D dash */ /* \".\" ASCII 46 x2E period */ /* \"_\" ASCII 95 x5F underscore */ /* \"=\" ASCII 61 x3D equals-sign */ References \u00b6 Source: https://docs.docker.com/engine/reference/commandline/tag/ Original Copyright: Code and documentation copyright 2017 Docker, inc, released under the Apache 2.0 license. License: Apache-2.0 ; LICENSE-2.0.txt Source: https://github.com/opencontainers/image-spec/blob/main/descriptor.md Original Copyright: Copyright (C) 2020 The Linux Foundation(R). All rights reserved., released under the Apache 2.0 license. License: Apache-2.0 ; LICENSE-2.0.txt [EBNF]: https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form [XML-EBNF]: https://www.w3.org/TR/REC-xml/#sec-notation","title":"Docker Container (Image/Tag) Name Syntax"},{"location":"doc/DOCKER-NAME-TAG.html#docker-container-imagetag-name-syntax","text":"This document describes the syntax of a docker container name, an image name w/ or w/o tag or digest (supported since 0.0.64). Created for the image name validation in pipelines and kept for reference and further improvements. Syntax | Known Issues | Grammar | References","title":"Docker Container (Image/Tag) Name Syntax"},{"location":"doc/DOCKER-NAME-TAG.html#syntax","text":"The textual description is taken from the Docker documentation and following is a transposition into a more formal grammar similar to EBNF. Encoding is US-ASCII; grammar at once . container ::= image-name ( \":\" tag-name | \"@\" digest )? /* \":\" ASCII 58 x3A colon */ Image Name | Tag Name | Digest","title":"Syntax"},{"location":"doc/DOCKER-NAME-TAG.html#image-name","text":"A container consists of the name of an image optionally followed by a tag or digest (both separated by a colon). image-name ::= prefix? name-components prefix ::= hostname port? \"/\" hostname ::= [a-zA-Z0-9.-]+ port ::= \":\" [0-9]+ /* \"/\" ASCII 47 x2F slash */ /* \"-\" ASCII 45 x2D dash */ An image name is made up of slash-separated name components, optionally prefixed by a registry hostname. The hostname must comply with standard DNS rules, but may not contain underscores. If a hostname is present, it may optionally be followed by a port number in the format :8080 . If not present, the command uses Docker\u2019s public registry located at registry-1.docker.io by default. name-components ::= name-component ( \"/\" name-component )? name-component ::= name ( name-separator name )* name ::= [a-z0-9]+ name-separator ::= ( \".\" | \"_\" \"_\"? | \"-\"+ ) /* \".\" ASCII 46 x2E period */ /* \"_\" ASCII 95 x5F underscore */ Name components may contain lowercase letters, digits and separators. A separator is defined as a period, one or two underscores, or one or more dashes. A name component may not start or end with a separator.","title":"Image Name"},{"location":"doc/DOCKER-NAME-TAG.html#tag-name","text":"A tag name must be valid ASCII and may contain lowercase and uppercase letters, digits, underscores, periods and dashes. A tag name may not start with a period or a dash and may contain a maximum of 128 characters. tag-name ::= tag-start tag-follow{0,127} tag-start ::= [a-zA-Z0-9_] tag-follow ::= tag-start | [.-]","title":"Tag Name"},{"location":"doc/DOCKER-NAME-TAG.html#digest","text":"In pipelines the digest is first of all supported to allow docker to fetch container images from docker-hub by content / collision-resistant hash of the bytes (the digest) and not by tag (only; tags are not stable). Image with: Tag: ktomk/pipelines:busybox Digest: ktomk/pipelines@sha256:2ef9a59041a7c4f36001abaec4fe7c10c26c1ead4da11515ba2af346fe60ddac In practice the SHA-256 is in use by all compliant implementations (this is Open Container Interface (OCI) material). digest ::= algorithm \":\" encoded algorithm ::= algorithm-component (algorithm-separator algorithm-component)* algorithm-component ::= [a-z0-9]+ algorithm-separator ::= [+._-] encoded ::= [a-zA-Z0-9=_-]+ /* \"=\" ASCII 61 x3D equals-sign */","title":"Digest"},{"location":"doc/DOCKER-NAME-TAG.html#known-issues","text":"The hostname part is not RFC conform (also depending on which RFC) and has not yet been double checked with standard DNS rules. The digest part currently only verifies per the grammar for forward compatibility. No constraint is made on validation on the algorithm component for example. Often this level of specific is not needed for pipelines so prone to errors apart of the validation itself. Image/container names are passed to docker(1) which might then unable to pull an image even if the regular expression based validation in the pipelines utility did let it pass as valid. Double check the hostname and the digest then.","title":"Known Issues"},{"location":"doc/DOCKER-NAME-TAG.html#grammar","text":"For field formats described in this document, we use a limited subset of Extended Backus-Naur Form [EBNF], similar to that used by the XML specification [XML-EBNF]. Encoding is US-ASCII. container ::= image-name ( \":\" ( tag-name | digest ) )? image-name ::= prefix? name-components prefix ::= hostname port? \"/\" hostname ::= [a-zA-Z0-9.-]+ port ::= \":\" [0-9]+ name-components ::= name-comp ( \"/\" name-component )? name-component ::= name ( name-separator name )* name ::= [a-z0-9]+ name-separator ::= ( \".\" | \"_\" \"_\"? | \"-\"+ ) tag-name ::= tag-start tag-follow{0,127} tag-start ::= [a-zA-Z0-9_] tag-follow ::= tag-start | [.-] digest ::= algorithm \":\" encoded algorithm ::= algorithm-component (algorithm-separator algorithm-component)* algorithm-component ::= [a-z0-9]+ algorithm-separator ::= [+._-] encoded ::= [a-zA-Z0-9=_-]+ /* \":\" ASCII 58 x3A colon */ /* \"/\" ASCII 47 x2F slash */ /* \"-\" ASCII 45 x2D dash */ /* \".\" ASCII 46 x2E period */ /* \"_\" ASCII 95 x5F underscore */ /* \"=\" ASCII 61 x3D equals-sign */","title":"Grammar"},{"location":"doc/DOCKER-NAME-TAG.html#references","text":"Source: https://docs.docker.com/engine/reference/commandline/tag/ Original Copyright: Code and documentation copyright 2017 Docker, inc, released under the Apache 2.0 license. License: Apache-2.0 ; LICENSE-2.0.txt Source: https://github.com/opencontainers/image-spec/blob/main/descriptor.md Original Copyright: Copyright (C) 2020 The Linux Foundation(R). All rights reserved., released under the Apache 2.0 license. License: Apache-2.0 ; LICENSE-2.0.txt [EBNF]: https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form [XML-EBNF]: https://www.w3.org/TR/REC-xml/#sec-notation","title":"References"},{"location":"doc/GETTING-STARTED.html","text":"Getting Started with Pipelines \u00b6 This is a short \"getting started\" guide. You can find out quickly if you have all common requirements available to get pipelines up and running. As a cherry on top, you'll find the browse- and search-able documentation on your system afterwards. Duration: ca. 1-3 minutes (depends on download speed) Location: on the command-line Utility Dependencies: php , git , composer and docker Install Pipelines \u00b6 The heavy lifting of pipelines installation is done by composer create-project in this tutorial: $ composer create-project -n --prefer-source --keep-vcs --no-dev \\ ktomk/pipelines Creating a \"ktomk/pipelines\" project at \"./pipelines\" ... Generating autoload files $ cd \" $PWD /pipelines\" The pipelines utility is available at bin/pipelines now. Let's display the installed version: $ bin/pipelines --version pipelines version 0.0.60 If so far everything worked it confirms a working PHP version with a Composer and Git installation. Run a Pipeline to Generate the HTML Documentation \u00b6 Time to put some pressure on a pipeline. Let's generate the manual from sources and open it: $ bin/pipelines --pipeline custom/html-docs +++ step #1 name...........: \"build html-docs with mkdocs\" ... +++ copying artifacts from container... $ xdg-open build/html-docs/index.html This effectively tests if Docker is available. On the first run it will take a bit more time as remote resources are downloaded (build container and dependencies) and installed. These are cached afterwards, which is why any future run will be faster. In this little getting started tutorial we could see the following: A working PHP + Composer setup for pipelines Installed pipelines with Composer + Git Docker (or similar docker(1) ) is configured for pipelines A container is pulled and build dependencies are downloaded during a pipeline run. These are cached . Doing real work with a pipeline to transfer the HTML documentation from sources. These artefacts are then served fresh on platter. Next Steps Suggestions \u00b6 Read: Continue reading the documentation you just build Setup: Install pipelines in your $PATH Exercise: Run a \"Hello World\" pipeline from your shell as a one-liner","title":"Getting Started"},{"location":"doc/GETTING-STARTED.html#getting-started-with-pipelines","text":"This is a short \"getting started\" guide. You can find out quickly if you have all common requirements available to get pipelines up and running. As a cherry on top, you'll find the browse- and search-able documentation on your system afterwards. Duration: ca. 1-3 minutes (depends on download speed) Location: on the command-line Utility Dependencies: php , git , composer and docker","title":"Getting Started with Pipelines"},{"location":"doc/GETTING-STARTED.html#install-pipelines","text":"The heavy lifting of pipelines installation is done by composer create-project in this tutorial: $ composer create-project -n --prefer-source --keep-vcs --no-dev \\ ktomk/pipelines Creating a \"ktomk/pipelines\" project at \"./pipelines\" ... Generating autoload files $ cd \" $PWD /pipelines\" The pipelines utility is available at bin/pipelines now. Let's display the installed version: $ bin/pipelines --version pipelines version 0.0.60 If so far everything worked it confirms a working PHP version with a Composer and Git installation.","title":"Install Pipelines"},{"location":"doc/GETTING-STARTED.html#run-a-pipeline-to-generate-the-html-documentation","text":"Time to put some pressure on a pipeline. Let's generate the manual from sources and open it: $ bin/pipelines --pipeline custom/html-docs +++ step #1 name...........: \"build html-docs with mkdocs\" ... +++ copying artifacts from container... $ xdg-open build/html-docs/index.html This effectively tests if Docker is available. On the first run it will take a bit more time as remote resources are downloaded (build container and dependencies) and installed. These are cached afterwards, which is why any future run will be faster. In this little getting started tutorial we could see the following: A working PHP + Composer setup for pipelines Installed pipelines with Composer + Git Docker (or similar docker(1) ) is configured for pipelines A container is pulled and build dependencies are downloaded during a pipeline run. These are cached . Doing real work with a pipeline to transfer the HTML documentation from sources. These artefacts are then served fresh on platter.","title":"Run a Pipeline to Generate the HTML Documentation"},{"location":"doc/GETTING-STARTED.html#next-steps-suggestions","text":"Read: Continue reading the documentation you just build Setup: Install pipelines in your $PATH Exercise: Run a \"Hello World\" pipeline from your shell as a one-liner","title":"Next Steps Suggestions"},{"location":"doc/PIPELINES-CACHES.html","text":"Working with Pipeline Caches \u00b6 Caches within pipelines can be used to cache build dependencies. The pipelines utility has caches support since version 0.0.48 (July 2020), docker was always \"cached\" as it is handled by docker on your host. What is the benefit of a cache when running pipelines locally? Pipeline caches can help to speed-up pipeline execution and spare network round-trips and computation for the price of your (precious) disk space. Once populated, caches even allow to run pipelines completely offline from your local system (if the tools in the build script support offline usage, e.g. composer does fall-back to the cache in case remote resources are offline/ unreachable, composer is also much faster when it can install from cache - just to use it as an example, there is no inherit requirement for composer with pipelines ). To ignore caches for a pipeline run, add the --no-cache switch which effectively does not use caches and establishes the old behaviour. Most often caches are for caching build dependencies. For example Composer packages, Node modules etc. and make use of caches as well for your very own specific build requirements as well. Predefined Caches \u00b6 All predefined caches are supported as documented in Bitbucket Pipelines file-format [BBPL-CACHES] which makes it straight forward to configure a dependency cache just by its name: Cache Name Path docker handled by docker on your host composer ~/.composer/cache dotnetcore ~/.nuget/packages gradle ~/.gradle/caches ivy2 ~/.ivy2/cache maven ~/.m2/repository node node_modules pip ~/.cache/pip sbt ~/.sbt Predefined caches and new ones can be (re-)defined by adding them to the caches entry in the defintions section of bitbucket-pipelines.yml file. Example of Caches in a Pipelines Step \u00b6 The following example shows a custom pipeline that is using a very dated PHP version (5.3) to run the projects phpunit test-suite. This requires to remove some project dependencies that are managed by composer and install an also dated but PHP 5.3 compatible version of Phpunit (4.x), shown in the following YAML. Having the composer cache allows to cache all these outdated dependencies, the pipeline runs much faster as composer installs the dependencies from cache, making integration testing against specific configurations straight forward: pipelines : custom : unit-tests-php-5.3 : - step : image : cespi/php-5.3:cli-latest # has no zip: tomsowerby/php-5.3:cli caches : - composer - build-http-cache script : - command -v composer || lib/pipelines/composer-install.sh - composer remove --dev friendsofphp/php-cs-fixer phpunit/phpunit - composer require --dev phpunit/phpunit ^4 - composer install - vendor/bin/phpunit # --testsuite unit,integration by default w/ phpunit 4.8.36 definitions : caches : build-http-cache : build/store/http-cache It is also an example of a cache named build-http-cache which is caching the path build/store/http-cache , a relative path to the clone directory which is also the pipelines' default working directory (and the project directory). Note If --deploy mount is in use and a cache path is relative to the clone directory, regardless of caches, the dependencies as within the mount, will populate the local directory. This may not be wanted, consider to not use --deploy mount but (implicit) --deploy copy and appropriate caches or artifacts. Cache paths are resolved against the running pipelines' container, the use of ~ or $HOME is supported. Differences to Bitbucket Pipelines \u00b6 pipelines caches should be transparent compared to running on Bitbucket from the in-container perspective, the pipeline should execute and cache definitions work as expected. However as running local, there are some differences in how caches are kept and updated. Invalidating the Cache \u00b6 Bitbucket Pipelines keeps a cache for seven days, then deletes/drops it. A cache is only created and never updated. As after seven days it will be deleted (or if deleted manually earlier), it will be re-created on the next pipeline run. In difference, the pipelines utility keeps caches endlessly and updates them after each (successful) run. This better reflects running pipelines locally as it keeps any cache policy out of the equation, there aren't any remote resources to be managed. Having up-to-date caches after each successful local pipeline step run is considered an improvement as caches are less stale. The meaning of remote is directly in the meaning of the dependencies and not with the indirection that the pipeline is run in some cloud far away. Shift left (iterate and get results faster). The Docker Cache \u00b6 Another difference is the docker cache . It is always \"cached\" as it is docker on your host and falls under your own configuration/policy - more control for you as a user. More interoperable with your overall docker. Makes containers (and Docker) more transparent. Resource Limits and Remote Usage \u00b6 There are no resource limits per cache other than the cache is limited by your own disk-space and other resources you allow docker to use. Take this with a grain of salt, there is no specific handling that makes pipelines fatal or panic if there is not enought disk-space left. pipelines will fail if your disk runs out of space, but you normally want to know earlier (unless a throw-away system), so if you're running short on disk-space, consider for what to use the (not) remaining space. Apropos locally: pipelines itself does not upload the cache files to any remote location on its own. However if the folder it stores the tar-files in is being shared (e.g. shared home folder), sharing the cache may happen (see as well the next section, especially if this is not your intention). So just in case your home directory is managed and will fill-up some remote storage towards its limits talk with your sysadmin before this happens and tell her/him about your needs upfront. Disk space is relatively cheap these days but if managed, it is better to raise requirements earlier than later. Where/How Pipelines Stores Caches \u00b6 Caching is per project by keeping the cache as a tar-file with the name of the cache in it: ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/<project>/<cache-name>.tar In case you find it cryptic to read: This respects the XDG Base Directory Specification [XDG-BASEDIR] and keeps cache files pipelines creates within your home directory unless XDG_CACHE_HOME has been set outside your home directory. This is the directory where all of the cache files will end up, there is one tar-file for each cache. So there is one tar-file per each pipeline cache per project. Paths in the tar file are relative to the cache path in the container. Having a central directory on a standard system path comes with the benefit that it is easy to manage centrally. Be it for your local system or for remote systems. Remote? For example, if pipelines is used in a remote build environment/system, the projects pipeline caches can be cached again by the remote build system with ease. For example when running in a CI system like Travis CI [TRAVIS-CI], caches can be easily retained between (remote) builds by caching the entire XDG_CACHE_HOME / HOME cache folder. Apply any caching policy of your own needs then with such an integration to fit your expectations and (remote) build requirements. Also if you want to move the cache out of the home directory, you can make use of the XDG_CACHE_HOME environment variable to have it at any location you like. Cache Operations \u00b6 Support for caches in the pipelines utility is sufficient for supporting them in the file-format (read: minimal), this leaves it open to maintaining them on your system (read: done by you). Do not fear, it normally is merely straight forward for simple things like dropping caches and leaves you with many more options for inspecting caches, populating caches (warming them up), even merging into caches, sharing across projects and merging back into your very own file-system. Your system, your control. As caches are based on standard tar-files, common and more advanced tasks can be done by interacting with all the (shell) utilities you love and hate and at the end of the day know best. Following are a couple of usage examples for tasks pipelines might look lacking, it is not that you should not file any kind of feature request or missing option, it is just that pipelines will never be able to provide all these capabilities on its own, so if you find them helpful or inspiring or both, this is what they are for: List and Inspect Caches \u00b6 To list all caches on disk including size information the du utility can be used: $ du -ch ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/*/*.tar 35M /home/user/.cache/pipelines/caches/pipelines/build-http-cache.tar 69M /home/user/.cache/pipelines/caches/pipelines/composer.tar 104M total As each cache is a tar-file, any utility showing information about files will give a good summary already. For more details on any specific cache, the tar utility is of use: $ tar -vtf ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/pipelines/build-http-cache.tar drwxrwxr-x 1000/1000 0 2020-07-29 01:12 ./ -rw-r--r-- 1000/1000 629 2020-07-05 10:53 ./.gitignore -rwxrwxrwx 0/0 1969526 2020-07-24 01:42 ./composer.phar -rw-rw-r-- 1000/1000 34272897 2019-12-15 22:58 ./docker-17.12.0-ce.tgz (using gnu tar here, check your very own tar version for all the options as these might differ depending on which flavor) This example also shows that it is possible to migrate caches by changing the cache definition path as all paths are relative to the cache path (they all start with ./ in the tar-file). Migrating from Project Directory to Caches \u00b6 (this section certainly is of special-interest and focus to detail on migrating to caches locally from not using caches earlier incl. being in an offline scenario) In the example above also an intersection of copying files into the container ( --deploy copy ) and with downloading new files in the container ( here composer.phar ) is shown (without caches, copy back into the project via artifacts was in effect earlier). This is because previously there was no caching/ were no caches and the project directory has been used as a store for offline copies of remote files; used here as an example to migrate away from such a workaround. The example combines a workaround for a pipelines version with no cache support (and no docker client support, using the path build/store/http-cache ) but running pipelines offline regardless. Technically this is not necessary any longer, but it shows how this has been done without any caches support in pipelines earlier. As now caches are supported, the cache can be moved into ~/.cache/build-http-cache (exemplary) by changing the path in the definitions section. As the name of the cache does not change - just the path in the definition - after updating all affected pipelines that used the previous workaround where they kept their own \"cache\", only the path in the cache definition needs to be changed. It only needs one run where artifacts and the cache(s) path(s) align. Afterwards as the cache(s) are filled, they can be moved to a different location outside of the clone directory by changing the cache paths of the definition(s) and/ or removing the artifact(s) definition for these. See as well Populate Caches below. Dropping a Cache \u00b6 To drop a cache, removing the tar file in the cache directory suffices. Warning Dropping a cache may prevent executing a pipeline successfully when being offline. To test dropping a cache, move the cache tar file to a backup location before removing it from the filesystem so in case of error iterating the pipeline run fails, it can be moved back. Example (removal): $ rm -vi -- ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/pipelines/build-http-cache.tar rm: remove regular file '/home/user/.cache/pipelines/caches/pipelines/build-http-cache.tar'? y removed '/home/user/.cache/pipelines/caches/pipelines/build-http-cache.tar' Here the cache named build-http-cache from the pipelines project is dropped. Commanded rm for interactivity it requires to confirm the deletion. Additionally verbosity dictates to show any file removal. Modify the rm command as you see fit to drop a cache (or all caches of a project or even all projects). Use the mv command to (re)-move the cache by moving the file to a different location in the local file-system. Populate Caches \u00b6 It is possible to pre-populate caches by importing files/directories from the local system (aka warming up caches). This works by creating a tar-file with relative paths on the container cache directory. For example in an offline situation when the cache does not yet exist. The following section demonstrates it by the example of composer [COMPOSER]. Populate Composer Cache from Your Host \u00b6 To populate the composer cache from your local system, all that needs to be done is to create a tar-file at the right place with the contents of your (local) composer cache. Mind that the project-name as well as the local cache directory of composer can differ: Warning Utilities like tar in the following example do overwrite existing files. Better safe than sorry, backup the cache tar-file first in case it already exists. This is especially noteworthy for offline scenarios as once dependencies are lost, your build breaks unless you're able to retain them from the wide-area network (going online) again. $ tar -cf ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/pipelines/composer.tar \\ -C ~/.cache/composer/ . Merge Back to Your Host \u00b6 Similar it is also possible to import/merge caches back into the local system by un-tarring to a local directory: $ tar -xf ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/pipelines/composer.tar \\ -C ~/.cache/composer/ Note It depends on the underlying utility if merging caches is possible or not. Here for composer it is, for other utilities this might vary on their robustness and cache structure. Merging into tar files is similarly possible, see your tar manual for append operations. Alternatives \u00b6 Next to caches pipelines file-format support - and independent to it - containers can be kept and re-used ( --keep ) which can help with faster re- iterations when writing pipelines by the price of less isolation (just say --no-cache with --keep until caches are ready to implement). As kept containers are with all the changes to the file-system, this effectively caches all build dependencies on the fly w/o specifying/defining caches first. As artifacts are copied back into the project directory (at least at a successful last step) making use of artifacts also adds some options. References \u00b6 [BBPL-CACHES]: https://support.atlassian.com/bitbucket-cloud/docs/cache-dependencies/ [COMPOSER]: https://getcomposer.org/ [TRAVIS-CI]: https://travis-ci.org/ [XDG-BASEDIR]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html","title":"Working with Pipeline Caches"},{"location":"doc/PIPELINES-CACHES.html#working-with-pipeline-caches","text":"Caches within pipelines can be used to cache build dependencies. The pipelines utility has caches support since version 0.0.48 (July 2020), docker was always \"cached\" as it is handled by docker on your host. What is the benefit of a cache when running pipelines locally? Pipeline caches can help to speed-up pipeline execution and spare network round-trips and computation for the price of your (precious) disk space. Once populated, caches even allow to run pipelines completely offline from your local system (if the tools in the build script support offline usage, e.g. composer does fall-back to the cache in case remote resources are offline/ unreachable, composer is also much faster when it can install from cache - just to use it as an example, there is no inherit requirement for composer with pipelines ). To ignore caches for a pipeline run, add the --no-cache switch which effectively does not use caches and establishes the old behaviour. Most often caches are for caching build dependencies. For example Composer packages, Node modules etc. and make use of caches as well for your very own specific build requirements as well.","title":"Working with Pipeline Caches"},{"location":"doc/PIPELINES-CACHES.html#predefined-caches","text":"All predefined caches are supported as documented in Bitbucket Pipelines file-format [BBPL-CACHES] which makes it straight forward to configure a dependency cache just by its name: Cache Name Path docker handled by docker on your host composer ~/.composer/cache dotnetcore ~/.nuget/packages gradle ~/.gradle/caches ivy2 ~/.ivy2/cache maven ~/.m2/repository node node_modules pip ~/.cache/pip sbt ~/.sbt Predefined caches and new ones can be (re-)defined by adding them to the caches entry in the defintions section of bitbucket-pipelines.yml file.","title":"Predefined Caches"},{"location":"doc/PIPELINES-CACHES.html#example-of-caches-in-a-pipelines-step","text":"The following example shows a custom pipeline that is using a very dated PHP version (5.3) to run the projects phpunit test-suite. This requires to remove some project dependencies that are managed by composer and install an also dated but PHP 5.3 compatible version of Phpunit (4.x), shown in the following YAML. Having the composer cache allows to cache all these outdated dependencies, the pipeline runs much faster as composer installs the dependencies from cache, making integration testing against specific configurations straight forward: pipelines : custom : unit-tests-php-5.3 : - step : image : cespi/php-5.3:cli-latest # has no zip: tomsowerby/php-5.3:cli caches : - composer - build-http-cache script : - command -v composer || lib/pipelines/composer-install.sh - composer remove --dev friendsofphp/php-cs-fixer phpunit/phpunit - composer require --dev phpunit/phpunit ^4 - composer install - vendor/bin/phpunit # --testsuite unit,integration by default w/ phpunit 4.8.36 definitions : caches : build-http-cache : build/store/http-cache It is also an example of a cache named build-http-cache which is caching the path build/store/http-cache , a relative path to the clone directory which is also the pipelines' default working directory (and the project directory). Note If --deploy mount is in use and a cache path is relative to the clone directory, regardless of caches, the dependencies as within the mount, will populate the local directory. This may not be wanted, consider to not use --deploy mount but (implicit) --deploy copy and appropriate caches or artifacts. Cache paths are resolved against the running pipelines' container, the use of ~ or $HOME is supported.","title":"Example of Caches in a Pipelines Step"},{"location":"doc/PIPELINES-CACHES.html#differences-to-bitbucket-pipelines","text":"pipelines caches should be transparent compared to running on Bitbucket from the in-container perspective, the pipeline should execute and cache definitions work as expected. However as running local, there are some differences in how caches are kept and updated.","title":"Differences to Bitbucket Pipelines"},{"location":"doc/PIPELINES-CACHES.html#invalidating-the-cache","text":"Bitbucket Pipelines keeps a cache for seven days, then deletes/drops it. A cache is only created and never updated. As after seven days it will be deleted (or if deleted manually earlier), it will be re-created on the next pipeline run. In difference, the pipelines utility keeps caches endlessly and updates them after each (successful) run. This better reflects running pipelines locally as it keeps any cache policy out of the equation, there aren't any remote resources to be managed. Having up-to-date caches after each successful local pipeline step run is considered an improvement as caches are less stale. The meaning of remote is directly in the meaning of the dependencies and not with the indirection that the pipeline is run in some cloud far away. Shift left (iterate and get results faster).","title":"Invalidating the Cache"},{"location":"doc/PIPELINES-CACHES.html#the-docker-cache","text":"Another difference is the docker cache . It is always \"cached\" as it is docker on your host and falls under your own configuration/policy - more control for you as a user. More interoperable with your overall docker. Makes containers (and Docker) more transparent.","title":"The Docker Cache"},{"location":"doc/PIPELINES-CACHES.html#resource-limits-and-remote-usage","text":"There are no resource limits per cache other than the cache is limited by your own disk-space and other resources you allow docker to use. Take this with a grain of salt, there is no specific handling that makes pipelines fatal or panic if there is not enought disk-space left. pipelines will fail if your disk runs out of space, but you normally want to know earlier (unless a throw-away system), so if you're running short on disk-space, consider for what to use the (not) remaining space. Apropos locally: pipelines itself does not upload the cache files to any remote location on its own. However if the folder it stores the tar-files in is being shared (e.g. shared home folder), sharing the cache may happen (see as well the next section, especially if this is not your intention). So just in case your home directory is managed and will fill-up some remote storage towards its limits talk with your sysadmin before this happens and tell her/him about your needs upfront. Disk space is relatively cheap these days but if managed, it is better to raise requirements earlier than later.","title":"Resource Limits and Remote Usage"},{"location":"doc/PIPELINES-CACHES.html#wherehow-pipelines-stores-caches","text":"Caching is per project by keeping the cache as a tar-file with the name of the cache in it: ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/<project>/<cache-name>.tar In case you find it cryptic to read: This respects the XDG Base Directory Specification [XDG-BASEDIR] and keeps cache files pipelines creates within your home directory unless XDG_CACHE_HOME has been set outside your home directory. This is the directory where all of the cache files will end up, there is one tar-file for each cache. So there is one tar-file per each pipeline cache per project. Paths in the tar file are relative to the cache path in the container. Having a central directory on a standard system path comes with the benefit that it is easy to manage centrally. Be it for your local system or for remote systems. Remote? For example, if pipelines is used in a remote build environment/system, the projects pipeline caches can be cached again by the remote build system with ease. For example when running in a CI system like Travis CI [TRAVIS-CI], caches can be easily retained between (remote) builds by caching the entire XDG_CACHE_HOME / HOME cache folder. Apply any caching policy of your own needs then with such an integration to fit your expectations and (remote) build requirements. Also if you want to move the cache out of the home directory, you can make use of the XDG_CACHE_HOME environment variable to have it at any location you like.","title":"Where/How Pipelines Stores Caches"},{"location":"doc/PIPELINES-CACHES.html#cache-operations","text":"Support for caches in the pipelines utility is sufficient for supporting them in the file-format (read: minimal), this leaves it open to maintaining them on your system (read: done by you). Do not fear, it normally is merely straight forward for simple things like dropping caches and leaves you with many more options for inspecting caches, populating caches (warming them up), even merging into caches, sharing across projects and merging back into your very own file-system. Your system, your control. As caches are based on standard tar-files, common and more advanced tasks can be done by interacting with all the (shell) utilities you love and hate and at the end of the day know best. Following are a couple of usage examples for tasks pipelines might look lacking, it is not that you should not file any kind of feature request or missing option, it is just that pipelines will never be able to provide all these capabilities on its own, so if you find them helpful or inspiring or both, this is what they are for:","title":"Cache Operations"},{"location":"doc/PIPELINES-CACHES.html#list-and-inspect-caches","text":"To list all caches on disk including size information the du utility can be used: $ du -ch ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/*/*.tar 35M /home/user/.cache/pipelines/caches/pipelines/build-http-cache.tar 69M /home/user/.cache/pipelines/caches/pipelines/composer.tar 104M total As each cache is a tar-file, any utility showing information about files will give a good summary already. For more details on any specific cache, the tar utility is of use: $ tar -vtf ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/pipelines/build-http-cache.tar drwxrwxr-x 1000/1000 0 2020-07-29 01:12 ./ -rw-r--r-- 1000/1000 629 2020-07-05 10:53 ./.gitignore -rwxrwxrwx 0/0 1969526 2020-07-24 01:42 ./composer.phar -rw-rw-r-- 1000/1000 34272897 2019-12-15 22:58 ./docker-17.12.0-ce.tgz (using gnu tar here, check your very own tar version for all the options as these might differ depending on which flavor) This example also shows that it is possible to migrate caches by changing the cache definition path as all paths are relative to the cache path (they all start with ./ in the tar-file).","title":"List and Inspect Caches"},{"location":"doc/PIPELINES-CACHES.html#migrating-from-project-directory-to-caches","text":"(this section certainly is of special-interest and focus to detail on migrating to caches locally from not using caches earlier incl. being in an offline scenario) In the example above also an intersection of copying files into the container ( --deploy copy ) and with downloading new files in the container ( here composer.phar ) is shown (without caches, copy back into the project via artifacts was in effect earlier). This is because previously there was no caching/ were no caches and the project directory has been used as a store for offline copies of remote files; used here as an example to migrate away from such a workaround. The example combines a workaround for a pipelines version with no cache support (and no docker client support, using the path build/store/http-cache ) but running pipelines offline regardless. Technically this is not necessary any longer, but it shows how this has been done without any caches support in pipelines earlier. As now caches are supported, the cache can be moved into ~/.cache/build-http-cache (exemplary) by changing the path in the definitions section. As the name of the cache does not change - just the path in the definition - after updating all affected pipelines that used the previous workaround where they kept their own \"cache\", only the path in the cache definition needs to be changed. It only needs one run where artifacts and the cache(s) path(s) align. Afterwards as the cache(s) are filled, they can be moved to a different location outside of the clone directory by changing the cache paths of the definition(s) and/ or removing the artifact(s) definition for these. See as well Populate Caches below.","title":"Migrating from Project Directory to Caches"},{"location":"doc/PIPELINES-CACHES.html#dropping-a-cache","text":"To drop a cache, removing the tar file in the cache directory suffices. Warning Dropping a cache may prevent executing a pipeline successfully when being offline. To test dropping a cache, move the cache tar file to a backup location before removing it from the filesystem so in case of error iterating the pipeline run fails, it can be moved back. Example (removal): $ rm -vi -- ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/pipelines/build-http-cache.tar rm: remove regular file '/home/user/.cache/pipelines/caches/pipelines/build-http-cache.tar'? y removed '/home/user/.cache/pipelines/caches/pipelines/build-http-cache.tar' Here the cache named build-http-cache from the pipelines project is dropped. Commanded rm for interactivity it requires to confirm the deletion. Additionally verbosity dictates to show any file removal. Modify the rm command as you see fit to drop a cache (or all caches of a project or even all projects). Use the mv command to (re)-move the cache by moving the file to a different location in the local file-system.","title":"Dropping a Cache"},{"location":"doc/PIPELINES-CACHES.html#populate-caches","text":"It is possible to pre-populate caches by importing files/directories from the local system (aka warming up caches). This works by creating a tar-file with relative paths on the container cache directory. For example in an offline situation when the cache does not yet exist. The following section demonstrates it by the example of composer [COMPOSER].","title":"Populate Caches"},{"location":"doc/PIPELINES-CACHES.html#populate-composer-cache-from-your-host","text":"To populate the composer cache from your local system, all that needs to be done is to create a tar-file at the right place with the contents of your (local) composer cache. Mind that the project-name as well as the local cache directory of composer can differ: Warning Utilities like tar in the following example do overwrite existing files. Better safe than sorry, backup the cache tar-file first in case it already exists. This is especially noteworthy for offline scenarios as once dependencies are lost, your build breaks unless you're able to retain them from the wide-area network (going online) again. $ tar -cf ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/pipelines/composer.tar \\ -C ~/.cache/composer/ .","title":"Populate Composer Cache from Your Host"},{"location":"doc/PIPELINES-CACHES.html#merge-back-to-your-host","text":"Similar it is also possible to import/merge caches back into the local system by un-tarring to a local directory: $ tar -xf ${XDG_CACHE_HOME-$HOME/.cache}/pipelines/caches/pipelines/composer.tar \\ -C ~/.cache/composer/ Note It depends on the underlying utility if merging caches is possible or not. Here for composer it is, for other utilities this might vary on their robustness and cache structure. Merging into tar files is similarly possible, see your tar manual for append operations.","title":"Merge Back to Your Host"},{"location":"doc/PIPELINES-CACHES.html#alternatives","text":"Next to caches pipelines file-format support - and independent to it - containers can be kept and re-used ( --keep ) which can help with faster re- iterations when writing pipelines by the price of less isolation (just say --no-cache with --keep until caches are ready to implement). As kept containers are with all the changes to the file-system, this effectively caches all build dependencies on the fly w/o specifying/defining caches first. As artifacts are copied back into the project directory (at least at a successful last step) making use of artifacts also adds some options.","title":"Alternatives"},{"location":"doc/PIPELINES-CACHES.html#references","text":"[BBPL-CACHES]: https://support.atlassian.com/bitbucket-cloud/docs/cache-dependencies/ [COMPOSER]: https://getcomposer.org/ [TRAVIS-CI]: https://travis-ci.org/ [XDG-BASEDIR]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html","title":"References"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html","text":"How-To Docker Client Binary Packages for Pipelines \u00b6 Introduction \u00b6 Pipelines ships with some hard-encoded packages for a smaller number of different docker client versions including the exact same one at the time of writing that is used by Atlassian Bitbucket Cloud Pipelines Plugin. These packages are meta-data information on how to obtain a static binary of the Docker client and allow to use a static Docker client binary in a reproducible manner. Static Docker client binaries are useful for pipelines as they can be mounted into nearly any Linux container without further installation requirements (next to provide connectivity to the Docker daemon). This document is about how-to obtain the static docker client binary and all the meta-data to make use of it and also on how to create such packages next to existing packages or for improving pipelines with new package definitions. Be it a local .yml package file, a package that ships with the pipelines utility or a custom build static docker client binary. While doing so, this document also explains many details about the file- format of .yml docker client binary packages and the --docker-client option and arguments as well as the --docker-client-pkgs option. These options are generally useful to use a different than the default docker client. Table of Contents \u00b6 List all Available Docker Static Binary Packages Create Docker Client Static Binary Packages for Pipelines Find the Binary and Download the Release Package Obtain Meta-Information Encode Meta-Information into a YAML File Add Packages to Pipelines Project Last but not Least: Use a Custom Build Static Docker Client Binary List all Available Docker Static Binary Packages \u00b6 To display a list of all available docker client versions use the --docker-client-pkgs option: $ bin/pipelines --docker-client-pkgs docker-17.12.0-ce-linux-static-x86_64 docker-18.09.1-linux-static-x86_64 docker-19.03.1-linux-static-x86_64 docker-42.42.1-binsh-test-stub # only for development version It shows the list of the built-in packages which can be used right away as an option to the --docker-client argument. This list does not highlight the default one, so here a short description about each of these: docker-17.12.0-ce-linux-static-x86_64 : this version was in use when using the docker-client-install.sh script inside a pipeline. docker-18.09.1-linux-static-x86_64 : this version was found in use by the Atlassian Bitbucket Cloud Pipelines Plugin: $ docker --version Docker version 18 .09.1, build 4c52b90 docker-19.03.1-linux-static-x86_64 : the version used mainly to develop pipelines, in specific the Docker Service feature, which was sort of current while doing the development work. It includes fixes in environment variable handling (18.07.0 / docker/cli#1019 ) which prevented pipelines to properly import environment variables from the users' environment by environment variable files. This became prominent back in April 2018. docker-42.42.1-binsh-test-stub : a fake package used for testing that is not a Docker client at all. When the pipelines project is cloned, this is the only package that works offline out of the box and is only available after composer install has been run. Create Docker Client Static Binary Packages for Pipelines \u00b6 It might not be always in your case that pipelines ships with the package of your wish. Find the binary and download the release package \u00b6 Docker binaries are available for different platforms from the download.docker.com website: https://download.docker.com/linux/static/stable/x86_64/ Browse for the version that you need to work in a pipeline container, it might be necessary to change the architecture, fiddle with the path component of the URL above. In writing this how-to the docker-17.12.0-ce.tgz for Linux x86_64 has been created exemplary (it is now part of pipelines): https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz To download a utility like wget or similar can be used: $ wget https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz --2019-12-15 17 :10:57-- https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz Resolving download.docker.com ( download.docker.com ) ... 2600 :9000:21c7:ac00:3:db06:4200:93a1, 2600 :9000:21c7:a200:3:db06:4200:93a1, 2600 :9000:21c7:5c00:3:db06:4200:93a1, ... Connecting to download.docker.com ( download.docker.com ) | 2600 :9000:21c7:ac00:3:db06:4200:93a1 | :443... connected. HTTP request sent, awaiting response... 200 OK Length: 34272897 ( 33M ) [ application/x-tar ] Saving to: \u2018docker-17.12.0-ce.tgz\u2019 docker-17.12.0-ce.tgz 100 % [= . .. = > ] 32 ,68M ?,79MB/s in ?,8s 2019 -12-15 17 :11:02 ( 6 ,76 MB/s ) - \u2018docker-17.12.0-ce.tgz\u2019 saved [ 34272897 /34272897 ] Obtain Meta-Information \u00b6 As of time of writing, a package in pipelines for the docker client binary contains a bit more meta-information than the download location: The name of the package - to identify it, in this how-to we take docker-17.12.0-ce-linux-static-x86_64 , the name should be portable The URI - in this how-to it is the https URL of the download package The SHA-256 checksum of the download package - to verify if the download was successful (complete) The path of the docker binary inside the download package - this path is needed for extraction The SHA-256 checksum of the docker binary - to verify extraction from the package was successful (complete) To create the SHA-256 checksum of the download package: $ shasum -a256 docker-17.12.0-ce.tgz 692e1c72937f6214b1038def84463018d8e320c8eaf8530546c84c2f8f9c767d docker-17.12.0-ce.tgz Next look into the download package for the path of the docker client: $ tar -tvf docker-17.12.0-ce.tgz drwxr-xr-x 0 /1000 0 2017 -12-27 21 :13 docker/ -rwxr-xr-x 0 /1000 4320064 2017 -12-27 21 :13 docker/docker-containerd-shim -rwxr-xr-x 0 /1000 15433832 2017 -12-27 21 :13 docker/docker-containerd -rwxr-xr-x 0 /1000 7550928 2017 -12-27 21 :13 docker/docker-runc -rwxr-xr-x 0 /1000 19938610 2017 -12-27 21 :13 docker/docker -rwxr-xr-x 0 /1000 760040 2017 -12-27 21 :13 docker/docker-init -rwxr-xr-x 0 /1000 12773768 2017 -12-27 21 :13 docker/docker-containerd-ctr -rwxr-xr-x 0 /1000 2517244 2017 -12-27 21 :13 docker/docker-proxy -rwxr-xr-x 0 /1000 46366152 2017 -12-27 21 :13 docker/dockerd It normally is docker/docker and so it is in this how-to. Locate the path and create a SHA-256 checksum of it: $ tar -xf docker-17.12.0-ce.tgz -O docker/docker | shasum -a256 c77a64bf37b4e89cc4d6f35433baa44fb33e6f89e2f2c3406256e55f7a05504b - Now all required meta-data has been obtained to create a package. Encode Meta-Information into a YAML File \u00b6 The only thing left to fully produce a package file is to encode the collected information as YAML: $ <<'YAML' cat > docker-17.12.0-ce-linux-static-x86_64.yml --- # this file is part of pipelines # # binary docker client package format # name: docker-17.12.0-ce-linux-static-x86_64 uri: https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz sha256: 692e1c72937f6214b1038def84463018d8e320c8eaf8530546c84c2f8f9c767d binary: docker/docker binary_sha256: c77a64bf37b4e89cc4d6f35433baa44fb33e6f89e2f2c3406256e55f7a05504b YAML When done you find a .yml file with the name in the current directory. Note: You can find another description of each part of the package format in the commented package after composer install : lib/package/docker-42.42.1-binsh-test-stub.yml This package .yml file can already be used for a first test to see if it works. Just run pipelines with the --docker-client option having the path to the .yml file as argument: $ bin/pipelines --pipeline custom/docker \\ --docker-client docker-17.12.0-ce-linux-static-x86_64.yml ... +++ docker client install...: Docker version 17 .12.0-ce, build c97c6d6 \u001d+ docker version --format {{ .Client.Version }} 17 .12.0-ce ... This runs pipelines with a pipeline that has the docker service and therefore injects the docker client binary from the package .yml file provided by --docker-client argument. In this case the projects' custom pipeline named \" docker-in-docker \" (id: custom/docker ). The benefit of having a .yml package file is that all the meta- information is already documented, including on how to obtain the file. This includes benefiting for a user-wide store of checksum/hash'ed files. The tar package files are cached in the XDG_CACHE_HOME directory, which by default is: ~/.cache/pipelines/package-docker As long as the user decides to keep the cache, this spares downloading of the tar package again and again. The docker client binaries are not cached but kept and stored so that they are available in user-land until the user decides to remove the application in the XDG_DATA_HOME directory: ~/.local/share/pipelines/static-docker Add Packages to Pipelines Project \u00b6 Instead of having a .yml package somewhere on the local disk it is better to add it to the pipelines utility itself. Copy the file into the source directory for package files ( here from within a checked-out copy of the pipelines project): $ cp -vi docker-17.12.0-ce-linux-static-x86_64.yml lib/package 'docker-17.12.0-ce-linux-static-x86_64.yml' -> ... ... 'lib/package/docker-17.12.0-ce-linux-static-x86_64.yml' To test if the new package name works, invoke Pipelines with the name of this package (not the full file-name or path, e.g. not the path to the file but just the basename of it excluding the .yml extension) with a pipeline that is making use of the docker service, here exemplary again with the projects' custom pipeline named \" docker-in-docker \" ( custom/docker ): $ bin/pipelines --pipeline custom/docker \\ --docker-client docker-17.12.0-ce-linux-static-x86_64 [ ... ] +++ docker client install...: Docker version 17 .12.0-ce, build c97c6d6 \u001d+ docker version --format {{ .Client.Version }} 17 .12.0-ce [ ... ] Note: If you wish to see a specific package of yours also within the upstream distribution feel free to file a pull-request . Last but not Least: Use a Custom Build Static Docker Client Binary \u00b6 Pipelines as an integration utility allows to use docker client binaries directly. For example a binary created from a docker client development build. In this variant, no package exists at all and the binary is just given as the --docker-client options argument as path to the binary. No download will take place, so is no caching nor adding to any local store (this is all not needed, the file already exists locally). The package is only temporary so to say and the meta-information is non- existent. This mode of operation is very useful for testing any kind of binary as a docker client. To distinguish between a package name and binary potentially of that same name, the --docker-client argument needs to have at least a single slash (\" / \") in the path to the binary. For example, if the docker client binary is just a local file in the working directory prefix it with ./ so that pipelines can understand it is not a package of the same name. For example with the test stub that is available in pipelines development: $ bin/pipelines --pipeline custom/docker \\ --docker-client test/data/package/docker-test-stub [ ... ] +++ docker client install...: 42 .42.1 ( --version ) \u001d+ docker version --format {{ .Client.Version }} 42 .42.1 ( version --format {{ .Client.Version }} ) [ ... ] Here a fake .sh script is used as the docker client binary which outputs a bogus version number ( 42.42.1 ) and all options and arguments with which it was invoked which makes it suitable for testing purposes.","title":"Docker Client Binary Packages for Pipelines"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#how-to-docker-client-binary-packages-for-pipelines","text":"","title":"How-To Docker Client Binary Packages for Pipelines"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#introduction","text":"Pipelines ships with some hard-encoded packages for a smaller number of different docker client versions including the exact same one at the time of writing that is used by Atlassian Bitbucket Cloud Pipelines Plugin. These packages are meta-data information on how to obtain a static binary of the Docker client and allow to use a static Docker client binary in a reproducible manner. Static Docker client binaries are useful for pipelines as they can be mounted into nearly any Linux container without further installation requirements (next to provide connectivity to the Docker daemon). This document is about how-to obtain the static docker client binary and all the meta-data to make use of it and also on how to create such packages next to existing packages or for improving pipelines with new package definitions. Be it a local .yml package file, a package that ships with the pipelines utility or a custom build static docker client binary. While doing so, this document also explains many details about the file- format of .yml docker client binary packages and the --docker-client option and arguments as well as the --docker-client-pkgs option. These options are generally useful to use a different than the default docker client.","title":"Introduction"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#table-of-contents","text":"List all Available Docker Static Binary Packages Create Docker Client Static Binary Packages for Pipelines Find the Binary and Download the Release Package Obtain Meta-Information Encode Meta-Information into a YAML File Add Packages to Pipelines Project Last but not Least: Use a Custom Build Static Docker Client Binary","title":"Table of Contents"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#list-all-available-docker-static-binary-packages","text":"To display a list of all available docker client versions use the --docker-client-pkgs option: $ bin/pipelines --docker-client-pkgs docker-17.12.0-ce-linux-static-x86_64 docker-18.09.1-linux-static-x86_64 docker-19.03.1-linux-static-x86_64 docker-42.42.1-binsh-test-stub # only for development version It shows the list of the built-in packages which can be used right away as an option to the --docker-client argument. This list does not highlight the default one, so here a short description about each of these: docker-17.12.0-ce-linux-static-x86_64 : this version was in use when using the docker-client-install.sh script inside a pipeline. docker-18.09.1-linux-static-x86_64 : this version was found in use by the Atlassian Bitbucket Cloud Pipelines Plugin: $ docker --version Docker version 18 .09.1, build 4c52b90 docker-19.03.1-linux-static-x86_64 : the version used mainly to develop pipelines, in specific the Docker Service feature, which was sort of current while doing the development work. It includes fixes in environment variable handling (18.07.0 / docker/cli#1019 ) which prevented pipelines to properly import environment variables from the users' environment by environment variable files. This became prominent back in April 2018. docker-42.42.1-binsh-test-stub : a fake package used for testing that is not a Docker client at all. When the pipelines project is cloned, this is the only package that works offline out of the box and is only available after composer install has been run.","title":"List all Available Docker Static Binary Packages"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#create-docker-client-static-binary-packages-for-pipelines","text":"It might not be always in your case that pipelines ships with the package of your wish.","title":"Create Docker Client Static Binary Packages for Pipelines"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#find-the-binary-and-download-the-release-package","text":"Docker binaries are available for different platforms from the download.docker.com website: https://download.docker.com/linux/static/stable/x86_64/ Browse for the version that you need to work in a pipeline container, it might be necessary to change the architecture, fiddle with the path component of the URL above. In writing this how-to the docker-17.12.0-ce.tgz for Linux x86_64 has been created exemplary (it is now part of pipelines): https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz To download a utility like wget or similar can be used: $ wget https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz --2019-12-15 17 :10:57-- https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz Resolving download.docker.com ( download.docker.com ) ... 2600 :9000:21c7:ac00:3:db06:4200:93a1, 2600 :9000:21c7:a200:3:db06:4200:93a1, 2600 :9000:21c7:5c00:3:db06:4200:93a1, ... Connecting to download.docker.com ( download.docker.com ) | 2600 :9000:21c7:ac00:3:db06:4200:93a1 | :443... connected. HTTP request sent, awaiting response... 200 OK Length: 34272897 ( 33M ) [ application/x-tar ] Saving to: \u2018docker-17.12.0-ce.tgz\u2019 docker-17.12.0-ce.tgz 100 % [= . .. = > ] 32 ,68M ?,79MB/s in ?,8s 2019 -12-15 17 :11:02 ( 6 ,76 MB/s ) - \u2018docker-17.12.0-ce.tgz\u2019 saved [ 34272897 /34272897 ]","title":"Find the binary and download the release package"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#obtain-meta-information","text":"As of time of writing, a package in pipelines for the docker client binary contains a bit more meta-information than the download location: The name of the package - to identify it, in this how-to we take docker-17.12.0-ce-linux-static-x86_64 , the name should be portable The URI - in this how-to it is the https URL of the download package The SHA-256 checksum of the download package - to verify if the download was successful (complete) The path of the docker binary inside the download package - this path is needed for extraction The SHA-256 checksum of the docker binary - to verify extraction from the package was successful (complete) To create the SHA-256 checksum of the download package: $ shasum -a256 docker-17.12.0-ce.tgz 692e1c72937f6214b1038def84463018d8e320c8eaf8530546c84c2f8f9c767d docker-17.12.0-ce.tgz Next look into the download package for the path of the docker client: $ tar -tvf docker-17.12.0-ce.tgz drwxr-xr-x 0 /1000 0 2017 -12-27 21 :13 docker/ -rwxr-xr-x 0 /1000 4320064 2017 -12-27 21 :13 docker/docker-containerd-shim -rwxr-xr-x 0 /1000 15433832 2017 -12-27 21 :13 docker/docker-containerd -rwxr-xr-x 0 /1000 7550928 2017 -12-27 21 :13 docker/docker-runc -rwxr-xr-x 0 /1000 19938610 2017 -12-27 21 :13 docker/docker -rwxr-xr-x 0 /1000 760040 2017 -12-27 21 :13 docker/docker-init -rwxr-xr-x 0 /1000 12773768 2017 -12-27 21 :13 docker/docker-containerd-ctr -rwxr-xr-x 0 /1000 2517244 2017 -12-27 21 :13 docker/docker-proxy -rwxr-xr-x 0 /1000 46366152 2017 -12-27 21 :13 docker/dockerd It normally is docker/docker and so it is in this how-to. Locate the path and create a SHA-256 checksum of it: $ tar -xf docker-17.12.0-ce.tgz -O docker/docker | shasum -a256 c77a64bf37b4e89cc4d6f35433baa44fb33e6f89e2f2c3406256e55f7a05504b - Now all required meta-data has been obtained to create a package.","title":"Obtain Meta-Information"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#encode-meta-information-into-a-yaml-file","text":"The only thing left to fully produce a package file is to encode the collected information as YAML: $ <<'YAML' cat > docker-17.12.0-ce-linux-static-x86_64.yml --- # this file is part of pipelines # # binary docker client package format # name: docker-17.12.0-ce-linux-static-x86_64 uri: https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz sha256: 692e1c72937f6214b1038def84463018d8e320c8eaf8530546c84c2f8f9c767d binary: docker/docker binary_sha256: c77a64bf37b4e89cc4d6f35433baa44fb33e6f89e2f2c3406256e55f7a05504b YAML When done you find a .yml file with the name in the current directory. Note: You can find another description of each part of the package format in the commented package after composer install : lib/package/docker-42.42.1-binsh-test-stub.yml This package .yml file can already be used for a first test to see if it works. Just run pipelines with the --docker-client option having the path to the .yml file as argument: $ bin/pipelines --pipeline custom/docker \\ --docker-client docker-17.12.0-ce-linux-static-x86_64.yml ... +++ docker client install...: Docker version 17 .12.0-ce, build c97c6d6 \u001d+ docker version --format {{ .Client.Version }} 17 .12.0-ce ... This runs pipelines with a pipeline that has the docker service and therefore injects the docker client binary from the package .yml file provided by --docker-client argument. In this case the projects' custom pipeline named \" docker-in-docker \" (id: custom/docker ). The benefit of having a .yml package file is that all the meta- information is already documented, including on how to obtain the file. This includes benefiting for a user-wide store of checksum/hash'ed files. The tar package files are cached in the XDG_CACHE_HOME directory, which by default is: ~/.cache/pipelines/package-docker As long as the user decides to keep the cache, this spares downloading of the tar package again and again. The docker client binaries are not cached but kept and stored so that they are available in user-land until the user decides to remove the application in the XDG_DATA_HOME directory: ~/.local/share/pipelines/static-docker","title":"Encode Meta-Information into a YAML File"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#add-packages-to-pipelines-project","text":"Instead of having a .yml package somewhere on the local disk it is better to add it to the pipelines utility itself. Copy the file into the source directory for package files ( here from within a checked-out copy of the pipelines project): $ cp -vi docker-17.12.0-ce-linux-static-x86_64.yml lib/package 'docker-17.12.0-ce-linux-static-x86_64.yml' -> ... ... 'lib/package/docker-17.12.0-ce-linux-static-x86_64.yml' To test if the new package name works, invoke Pipelines with the name of this package (not the full file-name or path, e.g. not the path to the file but just the basename of it excluding the .yml extension) with a pipeline that is making use of the docker service, here exemplary again with the projects' custom pipeline named \" docker-in-docker \" ( custom/docker ): $ bin/pipelines --pipeline custom/docker \\ --docker-client docker-17.12.0-ce-linux-static-x86_64 [ ... ] +++ docker client install...: Docker version 17 .12.0-ce, build c97c6d6 \u001d+ docker version --format {{ .Client.Version }} 17 .12.0-ce [ ... ] Note: If you wish to see a specific package of yours also within the upstream distribution feel free to file a pull-request .","title":"Add Packages to Pipelines Project"},{"location":"doc/PIPELINES-HOWTO-DOCKER-CLIENT-BINARY.html#last-but-not-least-use-a-custom-build-static-docker-client-binary","text":"Pipelines as an integration utility allows to use docker client binaries directly. For example a binary created from a docker client development build. In this variant, no package exists at all and the binary is just given as the --docker-client options argument as path to the binary. No download will take place, so is no caching nor adding to any local store (this is all not needed, the file already exists locally). The package is only temporary so to say and the meta-information is non- existent. This mode of operation is very useful for testing any kind of binary as a docker client. To distinguish between a package name and binary potentially of that same name, the --docker-client argument needs to have at least a single slash (\" / \") in the path to the binary. For example, if the docker client binary is just a local file in the working directory prefix it with ./ so that pipelines can understand it is not a package of the same name. For example with the test stub that is available in pipelines development: $ bin/pipelines --pipeline custom/docker \\ --docker-client test/data/package/docker-test-stub [ ... ] +++ docker client install...: 42 .42.1 ( --version ) \u001d+ docker version --format {{ .Client.Version }} 42 .42.1 ( version --format {{ .Client.Version }} ) [ ... ] Here a fake .sh script is used as the docker client binary which outputs a bogus version number ( 42.42.1 ) and all options and arguments with which it was invoked which makes it suitable for testing purposes.","title":"Last but not Least: Use a Custom Build Static Docker Client Binary"},{"location":"doc/PIPELINES-HOWTO-ROOTLESS.html","text":"How-To Rootless Pipelines \u00b6 The pipelines utility runs the pipelines with Docker. Rootless [ROOTLESS] mode was introduced in Docker Engine 19.03. If the local user has Docker running in rootless mode, it is possible to run pipelines rootless. This protects the system the user operates the pipelines utility on. This How-To describes how to install docker rootless on Ubuntu 18.04 LTS (standard procedure) and how to run the pipelines utility with it. Installing Docker Rootless \u00b6 If Docker is installed as daemon (standard), stop it: # systemctl stop docker This should be the only command that needs to be executed as root. Read the installation instructions by reading the source code of the docker rootless installer: $ curl -fsSL https://get.docker.com/rootless -o get-docker.sh $ less get-docker.sh Install docker rootless following the scripted instructions: $ sh get-docker.sh The script should run through so that there should be no manual intervention necessary. At the very end the script displays the DOCKER_HOST environment parameter with it's value and how to export it to the environment like this: $ export DOCKER_HOST = unix:///run/user/1000/docker.sock It also shows which commands to run to start Docker rootless: systemctl --user (start|stop|restart) docker Prepare the environment to run pipelines with Docker rootless: If the DOCKER_HOST environment parameter is not set, export it to the environment first: $ export DOCKER_HOST = \"unix:// ${ XDG_RUNTIME_DIR } /docker.sock\" This environment parameter is necessary so that the Docker client knows how to connect to the Docker rootless daemon. Start the Docker rootless daemon if not yet started: $ systemctl --user start docker Use status instead of start to see if and how the daemon is running. Use stop to stop it again. Test pipelines to run with Docker rootless (done in the pipelines project itself) Easiest and fastest is to run the default pipeline w/ mount as it does not change anything in the project: $ pipelines --deploy mount If the installation of Docker rootless is incomplete, you will see the pipelines utility to complain about setting up the container providing more info that docker has issues connecting to the Docker daemon like so: pipelines: setting up the container failed docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. See 'docker run --help' . exit status: 125 This means either the DOCKER_HOST environment parameter is missing or not pointing to the correct socket or the Daemon is not running. The address of the Docker daemon in the error message is useful to review to learn about the connection issue. How To Progress \u00b6 Switching the Docker daemon to rootless needs Docker to pull images again as they are stored in the users home folder (e.g. ~/.local/share/docker ). See Ready for Offline in Read Me on how to pull pipeline images in batches. To switch back from rootless to system: $ systemctl --user stop docker ; sudo systemctl start docker \\ && export DOCKER_HOST = \"unix:///var/run/docker.sock\" and back to rootless: $ sudo systemctl stop docker ; systemctl --user start docker \\ && export DOCKER_HOST = \"unix:// ${ XDG_RUNTIME_DIR } /docker.sock\" To better integrate with your own users' environment, one could Run Docker rootless on login Add environment parameter to bashrc / zsh / shell profile depending on preference. This is covered in Rootless [ROOTLESS] as well. References \u00b6 [ROOTLESS]: https://docs.docker.com/engine/security/rootless/","title":"Rootless Pipelines"},{"location":"doc/PIPELINES-HOWTO-ROOTLESS.html#how-to-rootless-pipelines","text":"The pipelines utility runs the pipelines with Docker. Rootless [ROOTLESS] mode was introduced in Docker Engine 19.03. If the local user has Docker running in rootless mode, it is possible to run pipelines rootless. This protects the system the user operates the pipelines utility on. This How-To describes how to install docker rootless on Ubuntu 18.04 LTS (standard procedure) and how to run the pipelines utility with it.","title":"How-To Rootless Pipelines"},{"location":"doc/PIPELINES-HOWTO-ROOTLESS.html#installing-docker-rootless","text":"If Docker is installed as daemon (standard), stop it: # systemctl stop docker This should be the only command that needs to be executed as root. Read the installation instructions by reading the source code of the docker rootless installer: $ curl -fsSL https://get.docker.com/rootless -o get-docker.sh $ less get-docker.sh Install docker rootless following the scripted instructions: $ sh get-docker.sh The script should run through so that there should be no manual intervention necessary. At the very end the script displays the DOCKER_HOST environment parameter with it's value and how to export it to the environment like this: $ export DOCKER_HOST = unix:///run/user/1000/docker.sock It also shows which commands to run to start Docker rootless: systemctl --user (start|stop|restart) docker Prepare the environment to run pipelines with Docker rootless: If the DOCKER_HOST environment parameter is not set, export it to the environment first: $ export DOCKER_HOST = \"unix:// ${ XDG_RUNTIME_DIR } /docker.sock\" This environment parameter is necessary so that the Docker client knows how to connect to the Docker rootless daemon. Start the Docker rootless daemon if not yet started: $ systemctl --user start docker Use status instead of start to see if and how the daemon is running. Use stop to stop it again. Test pipelines to run with Docker rootless (done in the pipelines project itself) Easiest and fastest is to run the default pipeline w/ mount as it does not change anything in the project: $ pipelines --deploy mount If the installation of Docker rootless is incomplete, you will see the pipelines utility to complain about setting up the container providing more info that docker has issues connecting to the Docker daemon like so: pipelines: setting up the container failed docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. See 'docker run --help' . exit status: 125 This means either the DOCKER_HOST environment parameter is missing or not pointing to the correct socket or the Daemon is not running. The address of the Docker daemon in the error message is useful to review to learn about the connection issue.","title":"Installing Docker Rootless"},{"location":"doc/PIPELINES-HOWTO-ROOTLESS.html#how-to-progress","text":"Switching the Docker daemon to rootless needs Docker to pull images again as they are stored in the users home folder (e.g. ~/.local/share/docker ). See Ready for Offline in Read Me on how to pull pipeline images in batches. To switch back from rootless to system: $ systemctl --user stop docker ; sudo systemctl start docker \\ && export DOCKER_HOST = \"unix:///var/run/docker.sock\" and back to rootless: $ sudo systemctl stop docker ; systemctl --user start docker \\ && export DOCKER_HOST = \"unix:// ${ XDG_RUNTIME_DIR } /docker.sock\" To better integrate with your own users' environment, one could Run Docker rootless on login Add environment parameter to bashrc / zsh / shell profile depending on preference. This is covered in Rootless [ROOTLESS] as well.","title":"How To Progress"},{"location":"doc/PIPELINES-HOWTO-ROOTLESS.html#references","text":"[ROOTLESS]: https://docs.docker.com/engine/security/rootless/","title":"References"},{"location":"doc/PIPELINES-OFFLINE.html","text":"Working Offline \u00b6 Running pipelines locally allows offline usage. This is by design, however existing pipelines, especially when written for a remote service like Bitbucket, might not work out of the box and need some preparation. This is not inherently hard and this document shows hand-on advice how to do it. A build is best run locally and a stable one should not depend (always) on remote resources so that it has less brittle dependencies and executes fast on the many re-iterations. Running the build locally and with no connection to the internet is a simple test for that. The pipelines utility as well as the pipelines YAML file-format have plenty of properties to support a more independent and faster build. In this document, read about more general considerations and also find some specific examples for offline usage of pipelines incl. containers and what to do with remote files, dependencies and packages. General Considerations \u00b6 Pipelines are based on Docker containers. Their images need to be pulled if not stored locally. Local Docker Containers \u00b6 Before going into offline mode, make use of --images and the shell: $ pipelines --images | while read -r image ; do \\ docker image pull \" $image \" ; done ; If there are specific Docker container images in a project, build all images before going offline as these at some place at least depend on a base container and resources in such container definitions that are very likely not to exist locally. That is because building Docker images most often make use of remote resources that are unavailable when not connected to the internet. Remember: Pipelines always prefers locally stored containers. As long as they are available, no internet connection is required to use them. Build Dependencies \u00b6 Pulling Docker images might not suffice to run a pipeline with pipelines completely offline. For example when fetching build dependencies with tools like npm or composer , these can not be installed when offline. First of all pipelines is relatively immune to such an effect if these dependencies were already installed within the project. As pipelines makes a full copy of the project into the pipeline container, dependencies might already be available. However this might not always be the case and then offline usage would be broken. A build might also use different than the development dependencies to build and therefore would need to fetch while offline. Caching of Build Dependencies \u00b6 Caches within pipelines can be used to cache build dependencies. Add caches to your pipelines by giving them a distinct name (all caches are shared on a per project basis) and run the pipelines before going offline to populate the cache. When the build utilities support a cache to fall back to when being offline, this is an easy way to not only make pipelines faster by sparing network round-trips, but also to make them work with no internet connection. Read more about caches in Working with Pipeline Caches . Remote Build Services \u00b6 Despite all the offline capabilities, when a pipeline step fires webhooks or uploads build artifacts to remote services, this can not be done offline. The remote service is a hard dependency of the pipeline then. A prominent example for that are for example deployment steps as most often they need to interact with remote services. Depending on your build needs, this may be a showstopper. To not be completely blocked from building in an offline situation, this can be handled by separating artifacts creation from uploading them in pipeline steps of their own. Pipeline artifacts can be used to make build artifacts from one step available in the following steps. pipelines then can run those pipeline steps specifically which are not affected when being offline with the --step argument and artifacts afterwards inspected within your project. Test Offline Usage \u00b6 As there is no guarantee that using pipelines offline works out of the box, it is useful to give it a test-run before entering a true offline situation. This can be done by actually going offline and do the development work including running the pipelines needed for it if being offline is not your standard way to work (hard to imagine for some persons nowadays, give it a try if you think so as it can be very interesting how hard you're affected by that with benefits working online as well). Special Offline Requirements \u00b6 For some containers it might be standard to modify their base-images' environment on a pipeline run. For example, a PHP image is optimized for a build pipeline step by installing composer and system packages like unzip and git. While it could be better to actually build dedicated build-images of their own and have them at hand already in the pipeline step, while developing intermediate results might be achieved differently. Before introducing too much into generic base container images too early, it is often preferable as these things tend to change a lot in the beginning to do the groundwork in a less fixed fashion, for example by extending a pipeline step script first. As pipelines is a development utility, find more specialized offline configurations following. Individual HTTP Caching \u00b6 Obtaining files via http while building can be common, e.g. when installing more specific tools or libraries. One example of that within the pipelines project itself is the composer install script. That is a simple shell script wrapping the composer installation. As it normally acquires the composer installer from Github, it won't work offline (and would always need to download the composer installer and composer itself). The whole process can be short-circuited within the pipeline script already by not calling the composer installer script when composer is already installed: script : - command -v composer || lib/pipelines/composer-install.sh ( unless the composer command is installed, install it ) It would already be installed for example if the --keep option is in use as the container would be re-used with all the file-system changes. The --keep option most certainly is not applicable when working offline (despite it could help in some situations). Instead the individual http caching is in the script lib/pipelines/composer-install.sh itself. The solution is simple. The script changes the working directory into a http-caching directory ~/.cache/build-http-cache and unless files are already available they are downloaded relative to this working directory. In the pipelines file the cache is defined and all downloads get automatically cached: definitions : caches : build-http-cache : ~/.cache/build-http-cache More is not needed to cache things. A simple shell script can help in maintaining the procedure. This is just exemplary. Composer offers its own docker image. Also the package manager of the Linux distribution may also have Composer available as a package. So take this example with a grain of salt. It is more an example how HTTP caching can be done than how it should be done. Strategically it is most often better to prepare build containers and have their lifecycle well defined on project level. Individual HTTP Cache example pipelines file Composer installer wrapper shell script Cache Alpine APK Packages \u00b6 In Alpine based container images, the apk package manager commonly is in use. By default these dependencies can not be cached with predefined Pipeline caches as there is no such apk-cache predefined. To nevertheless make use of apk add <package> within a pipeline while being offline, the folder /etc/apk/cache needs to be cached by defining a cache (here apk ) and using it: - apk add bash - /bin/bash --version caches : - apk definitions : caches : apk : /etc/apk/cache Once the apk cache is populated, the pipeline runs offline despite the bash package is installed in the pipeline script. This first of all works without further ado, after some time, apk tries to re-connect for getting a fresh package index. This will cost some 2-3 seconds, as apk does this two times, see the try again later messages, but as the package is found in the cache, adding the package works without any error: ... fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz ERROR: http://dl-cdn.alpinelinux.org/alpine/v3.12/main: temporary error (try again later) fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz ERROR: http://dl-cdn.alpinelinux.org/alpine/v3.12/community: temporary error (try again later) (1/4) Installing ncurses-terminfo-base (6.2_p20200523-r0) ... The --no-cache Option Do not use apk add with the --no-cache argument. It effectively prevents storing the files into the cache. Then, when offline, apk(1) can not select the package and gives an error message showing that apk failed to select the package. This is different to a Dockerfile that keeps a layer size down. This is normally not the case within a pipeline. Tested with alpine releases 3.12.0 and 3.13.5. References: Alpine APK Cache example pipelines file Alpine Special Caching Configurations (wiki.alpinelinux.org) Alpine Docker Image Alpine Linux Cache Python pip install \u00b6 Enriching a Python based pipeline with pip install (see as well Installing Packages in the pip manual ) can be cached up to full package downloads (which is a requirement when working offline). A common example is the use of a requirements.txt file, it works likewise with package names: $ pip install --user -r requirements.txt Next to the pre-defined pip cache, to use pip offline fully, the installed packages need to be cached, too. To not cache all Python packages (that would include all those a pipeline container image already contains which would be bulky) pip offers the --user flag to install the packages into the user-site. That is a separate location, by default beneath the $HOME directory, to install to (see as well User Installs in the pip manual ). This \"user-site\" directory can be obtained with the following python command: $ python -m site --user-site /home/user/.local/lib/python/site-packages This needs to be obtained in concrete with the actual pipeline container image, e.g. the docker image for the mkdocs-material based build that pipelines is using to build the HTML docs which is in use for this example: $ docker run --rm --entrypoint /bin/sh squidfunk/mkdocs-material \\ -c 'python -m site --user-site' /root/.local/lib/python3.8/site-packages The output already shows the site-packages directory which is the cache directory. The $HOME part of it can be abbreviated in pipelines with the tilde ( ~ ) or $HOME : definitions: caches: pip-site-packages: ~/.local/lib/python3.8/site-packages ( define a pipeline cache for pip site-packages, here the user is \"root\" ) With such a pip site packages cache in use, the pip install command can run offline already. This is not complete for all packages. Some pip packages install more, for example commands into ~/.local/bin . These need another cache just for that folder: caches: pip-site-packages: ~/.local/lib/python3.8/site-packages localbin: ~/.local/bin That done, for the example of a tailored build with the mkdocs-material image, the python based pipeline runs flawlessly offline (with all the benefits of running much faster locally even when online). References: Python PIP Cache example pipelines file PIP Installing Packages PIP User Installs","title":"Working Offline"},{"location":"doc/PIPELINES-OFFLINE.html#working-offline","text":"Running pipelines locally allows offline usage. This is by design, however existing pipelines, especially when written for a remote service like Bitbucket, might not work out of the box and need some preparation. This is not inherently hard and this document shows hand-on advice how to do it. A build is best run locally and a stable one should not depend (always) on remote resources so that it has less brittle dependencies and executes fast on the many re-iterations. Running the build locally and with no connection to the internet is a simple test for that. The pipelines utility as well as the pipelines YAML file-format have plenty of properties to support a more independent and faster build. In this document, read about more general considerations and also find some specific examples for offline usage of pipelines incl. containers and what to do with remote files, dependencies and packages.","title":"Working Offline"},{"location":"doc/PIPELINES-OFFLINE.html#general-considerations","text":"Pipelines are based on Docker containers. Their images need to be pulled if not stored locally.","title":"General Considerations"},{"location":"doc/PIPELINES-OFFLINE.html#local-docker-containers","text":"Before going into offline mode, make use of --images and the shell: $ pipelines --images | while read -r image ; do \\ docker image pull \" $image \" ; done ; If there are specific Docker container images in a project, build all images before going offline as these at some place at least depend on a base container and resources in such container definitions that are very likely not to exist locally. That is because building Docker images most often make use of remote resources that are unavailable when not connected to the internet. Remember: Pipelines always prefers locally stored containers. As long as they are available, no internet connection is required to use them.","title":"Local Docker Containers"},{"location":"doc/PIPELINES-OFFLINE.html#build-dependencies","text":"Pulling Docker images might not suffice to run a pipeline with pipelines completely offline. For example when fetching build dependencies with tools like npm or composer , these can not be installed when offline. First of all pipelines is relatively immune to such an effect if these dependencies were already installed within the project. As pipelines makes a full copy of the project into the pipeline container, dependencies might already be available. However this might not always be the case and then offline usage would be broken. A build might also use different than the development dependencies to build and therefore would need to fetch while offline.","title":"Build Dependencies"},{"location":"doc/PIPELINES-OFFLINE.html#caching-of-build-dependencies","text":"Caches within pipelines can be used to cache build dependencies. Add caches to your pipelines by giving them a distinct name (all caches are shared on a per project basis) and run the pipelines before going offline to populate the cache. When the build utilities support a cache to fall back to when being offline, this is an easy way to not only make pipelines faster by sparing network round-trips, but also to make them work with no internet connection. Read more about caches in Working with Pipeline Caches .","title":"Caching of Build Dependencies"},{"location":"doc/PIPELINES-OFFLINE.html#remote-build-services","text":"Despite all the offline capabilities, when a pipeline step fires webhooks or uploads build artifacts to remote services, this can not be done offline. The remote service is a hard dependency of the pipeline then. A prominent example for that are for example deployment steps as most often they need to interact with remote services. Depending on your build needs, this may be a showstopper. To not be completely blocked from building in an offline situation, this can be handled by separating artifacts creation from uploading them in pipeline steps of their own. Pipeline artifacts can be used to make build artifacts from one step available in the following steps. pipelines then can run those pipeline steps specifically which are not affected when being offline with the --step argument and artifacts afterwards inspected within your project.","title":"Remote Build Services"},{"location":"doc/PIPELINES-OFFLINE.html#test-offline-usage","text":"As there is no guarantee that using pipelines offline works out of the box, it is useful to give it a test-run before entering a true offline situation. This can be done by actually going offline and do the development work including running the pipelines needed for it if being offline is not your standard way to work (hard to imagine for some persons nowadays, give it a try if you think so as it can be very interesting how hard you're affected by that with benefits working online as well).","title":"Test Offline Usage"},{"location":"doc/PIPELINES-OFFLINE.html#special-offline-requirements","text":"For some containers it might be standard to modify their base-images' environment on a pipeline run. For example, a PHP image is optimized for a build pipeline step by installing composer and system packages like unzip and git. While it could be better to actually build dedicated build-images of their own and have them at hand already in the pipeline step, while developing intermediate results might be achieved differently. Before introducing too much into generic base container images too early, it is often preferable as these things tend to change a lot in the beginning to do the groundwork in a less fixed fashion, for example by extending a pipeline step script first. As pipelines is a development utility, find more specialized offline configurations following.","title":"Special Offline Requirements"},{"location":"doc/PIPELINES-OFFLINE.html#individual-http-caching","text":"Obtaining files via http while building can be common, e.g. when installing more specific tools or libraries. One example of that within the pipelines project itself is the composer install script. That is a simple shell script wrapping the composer installation. As it normally acquires the composer installer from Github, it won't work offline (and would always need to download the composer installer and composer itself). The whole process can be short-circuited within the pipeline script already by not calling the composer installer script when composer is already installed: script : - command -v composer || lib/pipelines/composer-install.sh ( unless the composer command is installed, install it ) It would already be installed for example if the --keep option is in use as the container would be re-used with all the file-system changes. The --keep option most certainly is not applicable when working offline (despite it could help in some situations). Instead the individual http caching is in the script lib/pipelines/composer-install.sh itself. The solution is simple. The script changes the working directory into a http-caching directory ~/.cache/build-http-cache and unless files are already available they are downloaded relative to this working directory. In the pipelines file the cache is defined and all downloads get automatically cached: definitions : caches : build-http-cache : ~/.cache/build-http-cache More is not needed to cache things. A simple shell script can help in maintaining the procedure. This is just exemplary. Composer offers its own docker image. Also the package manager of the Linux distribution may also have Composer available as a package. So take this example with a grain of salt. It is more an example how HTTP caching can be done than how it should be done. Strategically it is most often better to prepare build containers and have their lifecycle well defined on project level. Individual HTTP Cache example pipelines file Composer installer wrapper shell script","title":"Individual HTTP Caching"},{"location":"doc/PIPELINES-OFFLINE.html#cache-alpine-apk-packages","text":"In Alpine based container images, the apk package manager commonly is in use. By default these dependencies can not be cached with predefined Pipeline caches as there is no such apk-cache predefined. To nevertheless make use of apk add <package> within a pipeline while being offline, the folder /etc/apk/cache needs to be cached by defining a cache (here apk ) and using it: - apk add bash - /bin/bash --version caches : - apk definitions : caches : apk : /etc/apk/cache Once the apk cache is populated, the pipeline runs offline despite the bash package is installed in the pipeline script. This first of all works without further ado, after some time, apk tries to re-connect for getting a fresh package index. This will cost some 2-3 seconds, as apk does this two times, see the try again later messages, but as the package is found in the cache, adding the package works without any error: ... fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz ERROR: http://dl-cdn.alpinelinux.org/alpine/v3.12/main: temporary error (try again later) fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz ERROR: http://dl-cdn.alpinelinux.org/alpine/v3.12/community: temporary error (try again later) (1/4) Installing ncurses-terminfo-base (6.2_p20200523-r0) ... The --no-cache Option Do not use apk add with the --no-cache argument. It effectively prevents storing the files into the cache. Then, when offline, apk(1) can not select the package and gives an error message showing that apk failed to select the package. This is different to a Dockerfile that keeps a layer size down. This is normally not the case within a pipeline. Tested with alpine releases 3.12.0 and 3.13.5. References: Alpine APK Cache example pipelines file Alpine Special Caching Configurations (wiki.alpinelinux.org) Alpine Docker Image Alpine Linux","title":"Cache Alpine APK Packages"},{"location":"doc/PIPELINES-OFFLINE.html#cache-python-pip-install","text":"Enriching a Python based pipeline with pip install (see as well Installing Packages in the pip manual ) can be cached up to full package downloads (which is a requirement when working offline). A common example is the use of a requirements.txt file, it works likewise with package names: $ pip install --user -r requirements.txt Next to the pre-defined pip cache, to use pip offline fully, the installed packages need to be cached, too. To not cache all Python packages (that would include all those a pipeline container image already contains which would be bulky) pip offers the --user flag to install the packages into the user-site. That is a separate location, by default beneath the $HOME directory, to install to (see as well User Installs in the pip manual ). This \"user-site\" directory can be obtained with the following python command: $ python -m site --user-site /home/user/.local/lib/python/site-packages This needs to be obtained in concrete with the actual pipeline container image, e.g. the docker image for the mkdocs-material based build that pipelines is using to build the HTML docs which is in use for this example: $ docker run --rm --entrypoint /bin/sh squidfunk/mkdocs-material \\ -c 'python -m site --user-site' /root/.local/lib/python3.8/site-packages The output already shows the site-packages directory which is the cache directory. The $HOME part of it can be abbreviated in pipelines with the tilde ( ~ ) or $HOME : definitions: caches: pip-site-packages: ~/.local/lib/python3.8/site-packages ( define a pipeline cache for pip site-packages, here the user is \"root\" ) With such a pip site packages cache in use, the pip install command can run offline already. This is not complete for all packages. Some pip packages install more, for example commands into ~/.local/bin . These need another cache just for that folder: caches: pip-site-packages: ~/.local/lib/python3.8/site-packages localbin: ~/.local/bin That done, for the example of a tailored build with the mkdocs-material image, the python based pipeline runs flawlessly offline (with all the benefits of running much faster locally even when online). References: Python PIP Cache example pipelines file PIP Installing Packages PIP User Installs","title":"Cache Python pip install"},{"location":"doc/PIPELINES-SERVICES.html","text":"Working with Pipeline Services \u00b6 The pipelines local runner supports services since version 0.0.37 (the special docker service from version 0.0.25 is not covered by this document, see Pipelines inside Pipeline ). Introduction | Example | Validate | Trouble-Shoot | Debug Introduction to Services in Pipelines \u00b6 A service is another container that is started before the step script using host networking both for the service as well as for the pipeline step container. The step script can then access on localhost the started service. After the step script is run (and any optional after-script ), the step container is shut down and removed; afterwards any service containers are shut down and removed. Services are defined in the bitbucket-pipelines.yml file and then referenced by a pipeline step. Next to running bitbucket pipelines locally with services, the pipelines runner has options for validating , trouble-shooting and debugging services . Service Example \u00b6 Here a simple service example with a redis service in a step script that pings it. Defining a Service \u00b6 A service definition in bitbucket-pipelines.yml is required to make use of the service in a pipeline step: definitions : services : redis : image : redis:6.0.4-alpine Pipelines YAML fragment: Definition of the redis service The service named redis is then defined and ready to use by the step services . Use a Service in a Pipeline Step \u00b6 As now defined, the step is ready to use by the steps' services list by referencing the defined service name , here redis . A default pipeline named redis service example as an example: pipelines : default : - step : name : redis service example image : redis:alpine script : - redis-cli -h localhost ping services : - redis Pipelines YAML fragment: Default pipeline step with the redis service Note the services list at the very end, it has the redis entry. This tells pipelines to start it for the step. Run the Pipeline with the Example Redis Service \u00b6 That is it for the configuration, let us run it: $ pipelines \u001d+++ step #1 name...........: \"redis service example\" effective-image: redis:alpine container......: pipelines-1.redis-service-example.default.pipelines container-id...: 1fd11cdf4291 \u001d+++ copying files into container... \u001d+ redis-cli -h localhost ping PONG Command line example: Running the default pipeline with the redis service successfully pinging the service. Validate Pipeline Services \u00b6 To verify the services are defined and properly wired to pipeline steps, use the --show-services switch, it specifically shows the services by step and checks if services are defined: $ pipelines --show-services PIPELINE ID STEP SERVICE IMAGE default 1 redis redis:6.0.4-alpine Command line example: Default pipeline first step uses the redis service defined with the redis:6.0.4-alpine image. In case of an error, e.g. the service definition is missing or variables are not well-defined, this would be shown: PIPELINE ID STEP SERVICE IMAGE default 1 ERROR Undefined service: \"redis\" PIPELINE ID STEP SERVICE IMAGE ERROR variable MYSQL_RANDOM_ROOT_PASSWORD \\ should be a string (it is currently \\ defined as a boolean) Command line examples: Services validation errors with --show-services The --show-services option exits with zero status or non-zero in case an error was found. Part of this service information is also available with the --show command, errors in the file are highlighted more prominently thought: $ pipelines --show pipelines: file parse error: variable MYSQL_RANDOM_ROOT_PASSWORD \\ should be a string (it is currently defined as a boolean) Command line example: Bogus variable with --show Without any errors the --show option displays the information and exits: $ pipelines --show PIPELINE ID STEP IMAGE NAME default 1 redis:alpine \"redis service example\" default 1 redis:6.0.4-alpine service:redis Command line example: Pipelines shows services for each pipeline step Trouble-Shoot Starting Service Containers \u00b6 Sometimes service containers do not start properly, the service container exits prematurely or other unintended things are happening setting up a service. It is possible to start a pipelines service container manually to review the start sequence. The --service <service> option \u00b6 To start any defined service use the --service option with the name of the service in the definitions section. $ pipelines --service mysql error: database is uninitialized and password option is not specified You need to specify one of MYSQL_ROOT_PASSWORD, \\ MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD Command-line example: Missing variable for pipelines mysql service Fixing the service definition (here by adding a variable to it) and running the pipelines --service mysql again, will show the service properly running by displaying the output of the service. Press ctrl + z to suspend the process and either $ bg to send the service in the background or $ kill % which will shut down the service container. Note: If the --keep or --error-keep option has been used to run the pipeline and the service exited in error, the stopped service container needs to be removed before --service can successfully run the service: $ docker rm pipelines-service-mysql.pipelines pipelines-service-mysql.pipelines Debug Pipeline Services \u00b6 As the pipelines utility is designed to run bitbucket pipelines locally, trouble-shooting and debugging pipeline services is easily possible and supported with various options re-iterating quickly locally. Keep Service Containers with --keep \u00b6 To leave service containers on the system for inspection and re- iteration, use the --keep (or --error-keep ) option. The step container and all service containers are then kept and not removed: $ pipelines --keep \u001d+++ step #1 name...........: \"redis service example\" effective-image: redis:alpine [...] \u001d+ redis-cli -h localhost ping PONG keeping container id 0aa93bcf3b7b keeping service container pipelines-service-redis.pipelines Command line example: Keeping service containers for inspection Inspect Kept Containers with Docker \u00b6 With the containers still running, service configuration problems can be reviewed, e.g. in case a service didn't start well. For example by inspecting the logs: $ docker logs pipelines-service-redis.pipelines 1:C 01 Jun 2020 08:42:42.739 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 01 Jun 2020 08:42:42.739 # Redis version=6.0.4, bits=64, commit=00000000, modified=0, pid=1, just started 1:C 01 Jun 2020 08:42:42.739 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 01 Jun 2020 08:42:42.740 * Running mode=standalone, port=6379. 1:M 01 Jun 2020 08:42:42.740 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 01 Jun 2020 08:42:42.740 # Server initialized 1:M 01 Jun 2020 08:42:42.740 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 01 Jun 2020 08:42:42.740 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 01 Jun 2020 08:42:42.740 * Ready to accept connections Command line example: Reviewing logs of a pipeline service Clean with --docker-zap \u00b6 Running the pipeline again will re-use the existing step and service containers for a fast re-iteration. In case this is unwanted and a fresh run is preferred, just \"zap\" all kept pipeline containers: $ pipelines --docker-zap Command line example: Remove all pipeline containers incl. service containers Afterwards all pipelines containers are gone and will be re-created on next pipelines run.","title":"Working with Pipeline Services"},{"location":"doc/PIPELINES-SERVICES.html#working-with-pipeline-services","text":"The pipelines local runner supports services since version 0.0.37 (the special docker service from version 0.0.25 is not covered by this document, see Pipelines inside Pipeline ). Introduction | Example | Validate | Trouble-Shoot | Debug","title":"Working with Pipeline Services"},{"location":"doc/PIPELINES-SERVICES.html#introduction-to-services-in-pipelines","text":"A service is another container that is started before the step script using host networking both for the service as well as for the pipeline step container. The step script can then access on localhost the started service. After the step script is run (and any optional after-script ), the step container is shut down and removed; afterwards any service containers are shut down and removed. Services are defined in the bitbucket-pipelines.yml file and then referenced by a pipeline step. Next to running bitbucket pipelines locally with services, the pipelines runner has options for validating , trouble-shooting and debugging services .","title":"Introduction to Services in Pipelines"},{"location":"doc/PIPELINES-SERVICES.html#service-example","text":"Here a simple service example with a redis service in a step script that pings it.","title":"Service Example"},{"location":"doc/PIPELINES-SERVICES.html#defining-a-service","text":"A service definition in bitbucket-pipelines.yml is required to make use of the service in a pipeline step: definitions : services : redis : image : redis:6.0.4-alpine Pipelines YAML fragment: Definition of the redis service The service named redis is then defined and ready to use by the step services .","title":"Defining a Service"},{"location":"doc/PIPELINES-SERVICES.html#use-a-service-in-a-pipeline-step","text":"As now defined, the step is ready to use by the steps' services list by referencing the defined service name , here redis . A default pipeline named redis service example as an example: pipelines : default : - step : name : redis service example image : redis:alpine script : - redis-cli -h localhost ping services : - redis Pipelines YAML fragment: Default pipeline step with the redis service Note the services list at the very end, it has the redis entry. This tells pipelines to start it for the step.","title":"Use a Service in a Pipeline Step"},{"location":"doc/PIPELINES-SERVICES.html#run-the-pipeline-with-the-example-redis-service","text":"That is it for the configuration, let us run it: $ pipelines \u001d+++ step #1 name...........: \"redis service example\" effective-image: redis:alpine container......: pipelines-1.redis-service-example.default.pipelines container-id...: 1fd11cdf4291 \u001d+++ copying files into container... \u001d+ redis-cli -h localhost ping PONG Command line example: Running the default pipeline with the redis service successfully pinging the service.","title":"Run the Pipeline with the Example Redis Service"},{"location":"doc/PIPELINES-SERVICES.html#validate-pipeline-services","text":"To verify the services are defined and properly wired to pipeline steps, use the --show-services switch, it specifically shows the services by step and checks if services are defined: $ pipelines --show-services PIPELINE ID STEP SERVICE IMAGE default 1 redis redis:6.0.4-alpine Command line example: Default pipeline first step uses the redis service defined with the redis:6.0.4-alpine image. In case of an error, e.g. the service definition is missing or variables are not well-defined, this would be shown: PIPELINE ID STEP SERVICE IMAGE default 1 ERROR Undefined service: \"redis\" PIPELINE ID STEP SERVICE IMAGE ERROR variable MYSQL_RANDOM_ROOT_PASSWORD \\ should be a string (it is currently \\ defined as a boolean) Command line examples: Services validation errors with --show-services The --show-services option exits with zero status or non-zero in case an error was found. Part of this service information is also available with the --show command, errors in the file are highlighted more prominently thought: $ pipelines --show pipelines: file parse error: variable MYSQL_RANDOM_ROOT_PASSWORD \\ should be a string (it is currently defined as a boolean) Command line example: Bogus variable with --show Without any errors the --show option displays the information and exits: $ pipelines --show PIPELINE ID STEP IMAGE NAME default 1 redis:alpine \"redis service example\" default 1 redis:6.0.4-alpine service:redis Command line example: Pipelines shows services for each pipeline step","title":"Validate Pipeline Services"},{"location":"doc/PIPELINES-SERVICES.html#trouble-shoot-starting-service-containers","text":"Sometimes service containers do not start properly, the service container exits prematurely or other unintended things are happening setting up a service. It is possible to start a pipelines service container manually to review the start sequence.","title":"Trouble-Shoot Starting Service Containers"},{"location":"doc/PIPELINES-SERVICES.html#the---service-service-option","text":"To start any defined service use the --service option with the name of the service in the definitions section. $ pipelines --service mysql error: database is uninitialized and password option is not specified You need to specify one of MYSQL_ROOT_PASSWORD, \\ MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD Command-line example: Missing variable for pipelines mysql service Fixing the service definition (here by adding a variable to it) and running the pipelines --service mysql again, will show the service properly running by displaying the output of the service. Press ctrl + z to suspend the process and either $ bg to send the service in the background or $ kill % which will shut down the service container. Note: If the --keep or --error-keep option has been used to run the pipeline and the service exited in error, the stopped service container needs to be removed before --service can successfully run the service: $ docker rm pipelines-service-mysql.pipelines pipelines-service-mysql.pipelines","title":"The --service &lt;service&gt; option"},{"location":"doc/PIPELINES-SERVICES.html#debug-pipeline-services","text":"As the pipelines utility is designed to run bitbucket pipelines locally, trouble-shooting and debugging pipeline services is easily possible and supported with various options re-iterating quickly locally.","title":"Debug Pipeline Services"},{"location":"doc/PIPELINES-SERVICES.html#keep-service-containers-with---keep","text":"To leave service containers on the system for inspection and re- iteration, use the --keep (or --error-keep ) option. The step container and all service containers are then kept and not removed: $ pipelines --keep \u001d+++ step #1 name...........: \"redis service example\" effective-image: redis:alpine [...] \u001d+ redis-cli -h localhost ping PONG keeping container id 0aa93bcf3b7b keeping service container pipelines-service-redis.pipelines Command line example: Keeping service containers for inspection","title":"Keep Service Containers with --keep"},{"location":"doc/PIPELINES-SERVICES.html#inspect-kept-containers-with-docker","text":"With the containers still running, service configuration problems can be reviewed, e.g. in case a service didn't start well. For example by inspecting the logs: $ docker logs pipelines-service-redis.pipelines 1:C 01 Jun 2020 08:42:42.739 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 01 Jun 2020 08:42:42.739 # Redis version=6.0.4, bits=64, commit=00000000, modified=0, pid=1, just started 1:C 01 Jun 2020 08:42:42.739 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 01 Jun 2020 08:42:42.740 * Running mode=standalone, port=6379. 1:M 01 Jun 2020 08:42:42.740 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 01 Jun 2020 08:42:42.740 # Server initialized 1:M 01 Jun 2020 08:42:42.740 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 01 Jun 2020 08:42:42.740 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 01 Jun 2020 08:42:42.740 * Ready to accept connections Command line example: Reviewing logs of a pipeline service","title":"Inspect Kept Containers with Docker"},{"location":"doc/PIPELINES-SERVICES.html#clean-with---docker-zap","text":"Running the pipeline again will re-use the existing step and service containers for a fast re-iteration. In case this is unwanted and a fresh run is preferred, just \"zap\" all kept pipeline containers: $ pipelines --docker-zap Command line example: Remove all pipeline containers incl. service containers Afterwards all pipelines containers are gone and will be re-created on next pipelines run.","title":"Clean with --docker-zap"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html","text":"Pipelines Environment Variable Usage \u00b6 Environment variables (also known as environment parameters) are a first grade citizen of the pipelines utility. The pipelines utility has support for Bitbucket Cloud Pipelines Plugin environment variables. This document describes along the list of default variables [BBPL-ENV] the level of support and provides useful information for setting and using these variables. This document is a lengthier review of all variables at the time of writing and provides additional information next to the short introduction in the read me , both for limitations of the pipelines utility to this regard (and therefore feature planning) and usage guidelines for \"dot env\" environment files in project scope in a suggested (and best-intend compatible) fashion (fa\u00e7on). Default Variables \u00b6 Variable Name Remarks CI all options; always set to \" true \" BITBUCKET_BOOKMARK --trigger <ref> where <ref> is bookmark:<name> ; for Mercurial projects BITBUCKET_BRANCH --trigger <ref> where <ref> is branch:<name> , pr:<name> , pr:<source>:<destination> ; (source) branch BITBUCKET_BUILD_NUMBER all options; always set to \" 0 \" BITBUCKET_CLONE_DIR all options; set by pipelines, it is the deploy directory inside the container (not clone directory) as pipelines has more options than cloning (it currently actually never clones) BITBUCKET_COMMIT all options; always set to \" 0000000000000000000000000000000000000000 \" BITBUCKET_DEPLOYMENT_ENVIRONMENT -/-; currently unsupported BITBUCKET_DEPLOYMENT_ENVIRONMENT_UUID -/-; currently unsupported BITBUCKET_EXIT_CODE all options; set to the exit status of the script for use in the after-script BITBUCKET_GIT_HTTP_ORIGIN -/-; currently unsupported BITBUCKET_GIT_SSH_ORIGIN -/-; currently unsupported BITBUCKET_PARALLEL_STEP all options; in a parallel step set to zero-based index of the current step in the group, e.g. 0, 1, 2, ... BITBUCKET_PARALLEL_STEP_COUNT all options; in a parallel step set to the total number of steps in the group, e.g. 5. BITBUCKET_PR_DESTINATION_BRANCH --trigger <ref> where <ref> is pr:<source>:<destination> for the <destination> branch (see as well BITBUCKET_BRANCH ); destination branch BITBUCKET_PR_ID -/-; currently unsupported BITBUCKET_PROJECT_KEY -/-; currently unsupported BITBUCKET_PROJECT_UUID -/-; currently unsupported BITBUCKET_REPO_FULL_NAME -/-; currently unsupported BITBUCKET_REPO_OWNER all options; always set to current username from environment or if not available \" nobody \" (which might align w/ the Apache httpd project) BITBUCKET_REPO_OWNER_UUID -/-; currently unsupported BITBUCKET_REPO_SLUG all options; always set to base name of project directory BITBUCKET_REPO_UUID -/-; currently unsupported BITBUCKET_STEP_RUN_NUMBER all options; defaults to \" 1 \" and is set to \" 1 \" after the first run step BITBUCKET_STEP_TRIGGERER_UUID -/-; currently unsupported BITBUCKET_TAG --trigger <ref> where <ref> is tag:<name> ; Git projects Comments on Support and Usage \u00b6 All environment variables are supported in the meaning they can be explicitly passed with any value of the users wish (or command) into the container. That is even true for those variables \"always\" set to a specific value - variables passed by the user will always override any automatically \"always\" set values. Unsupported Features and Environment Variables \u00b6 Some variables yet do not make sense as the pipelines utility does not support the underlying feature. For example, the variable BITBUCKET_DEPLOYMENT_ENVIRONMENT and the depployments are not (yet) supported, therefore the variable BITBUCKET_DEPLOYMENT_ENVIRONMENT is not set. Setting it before call makes not much sense as it would be set for every script, and not the deployment script only. However: Some variables which are valid per project and which are marked \"currently unsupported\" above do make sense to be put into the auto-loading environment files ( .env and .env.dist ). As long as their values are not considered a secret, they can be added including the values to .env.dist which intention is to be committed in the project repository (see your VCS/SCMs' documentation, e.g. for git see git-ignore [GIT-IGNORE]). Otherwise, instead of poisoning your local environment, project secrets can be put into the private .env file (or any other *.env file via the --env-file option). This is as long as you consider your local system safe (if your computer is connected to the internet this might not be the case and might be an overall short-coming with the Atlassian Bitbucket Cloud Pipelines Plugin, take care [!] as Secured Variables are just masked and can be read by any user who has write access to a Bitbucket repository - similar to read access on a/your local system). Dealing w/ Secrets as Variable Values \u00b6 For secret environment parameters, put them into the .env file only (or other .env files - if at all) which by intention is/are not to be committed in the project repository. However: Required variables (with secrets) should be added with their name only to .env.dist file which is the project distribution parameter definition so that it clearly is documented which variables are required (this can be very helpful to document which secrets are needed and for non-secret what [sane] defaults are). Example Configuration of an Environment File \u00b6 Example configuration in a git based project with \"dot env\" files as they work w/ the pipelines utility: The pipelines utility adheres to environment files as by the Docker project. That is mainly b/c it is highly dependent on the Docker client utility and the environment file-format in the Docker project ships with all needed features: environment variables , environment parameter imports / definitions and comments . The following example consists of three files: First a .gitignore file that takes care that no environment files but .env.dist is getting committed to the git repository. This is necessary as these files might contain secrets (even in plain text) which should never go into the projects history (applies to git -utility managed projects only for .gitignore see your VCS utility if different for similar options). From pipelines perspective, the file intended to be committed is the .env.dist file. It should contain all major variables but no secrets. This is possible by writing down the variable (parameter) name only . One such variable in the example is AWS_ACCESS_KEY_ID . The key-id and the access key are considered a secret in the following example. The .env file then (can) contain private information if ok persisted in the local file-system. Different dot-env files can be provided w/ the --env-file <path> option of the pipelines (or docker ) utility. .gitignore file: # ... # Private environment variable files (dot env) /.env* !/.env.dist # ... (for more information on the .gitignore file format best see the gitignore documentation [GIT-IGNORE]) .env.dist file: # --- # Pipeline: Gitlab Cloud Project # BITBUCKET_REPO_OWNER = ACME.Inc. BITBUCKET_REPO_OWNER_UUID = 894510a8-be2a-4660-a276-10fd5280cc61 BITBUCKET_REPO_SLUG = sudo-make-sandwich BITBUCKET_REPO_UUID = 894510a8-be2a-4660-a276-10fd5280cc61 # --- # Pipeline: AWS ECR # AWS_REGION = eu-west-1 AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY (secrets in this .env file are hidden, only their variable names are given, e.g. AWS_SECRET_ACCESS_KEY ) .env file: # --- # Pipeline: AWS ECR # AWS_REGION = eu-west-1 AWS_ACCESS_KEY_ID = ETI1ZUOWJ2GO5Q8IDFDC AWS_SECRET_ACCESS_KEY = IzuobhF55og9fel6hleyuo4UOA0lUL9+GE2RmH6J This example provides the project-wide information which environment variables are required for it. Additionally the project-wide default values are provided and the configuration setup does allow (local) (secret) configuration. The comment blocks for a section are multi-line and follow an exemplary format of the first line of the comment block having a separating three-dash-line and a single (empty) line at the end after the second line containing a section title so that it is easy to diff between .env and .env.dist (and other) files. Why Multi-Line Comments? \u00b6 Why multi-line comments? In practice I found that single line comments are not that practical with text differs for sections , actually this is how this format came to life; when using various diff -utilities to compare these various dot-env files. It is however up to every users discreet own experience(s) and need(s) on how to introduce sections in dot env files. It is only a subjective suggestion. Note that there is a single empty line before any other but the first section comment block if the first section comment block starts on the very first line of the file. This is crucial for all differs I've run across. Environment Parameters Considered for Distribution \u00b6 If a project fully relies on Atlassian Bitbucket Cloud Pipelines Environment (by my reading of their documentation) the following environment variables and values should be distributed within the project: BITBUCKET_PROJECT_KEY BITBUCKET_PROJECT_UUID BITBUCKET_REPO_FULL_NAME BITBUCKET_REPO_OWNER BITBUCKET_REPO_OWNER_UUID BITBUCKET_REPO_SLUG - if not matching project base name BITBUCKET_REPO_UUID Note: Perhaps UUID values are gained best via the Bitbucket Cloud REST API on the shell command line. References \u00b6 [BBPL-ENV]: https://confluence.atlassian.com/bitbucket/environment-variables-794502608.html [GIT-IGNORE]: https://git-scm.com/docs/gitignore","title":"Pipelines Environment Variable Usage"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html#pipelines-environment-variable-usage","text":"Environment variables (also known as environment parameters) are a first grade citizen of the pipelines utility. The pipelines utility has support for Bitbucket Cloud Pipelines Plugin environment variables. This document describes along the list of default variables [BBPL-ENV] the level of support and provides useful information for setting and using these variables. This document is a lengthier review of all variables at the time of writing and provides additional information next to the short introduction in the read me , both for limitations of the pipelines utility to this regard (and therefore feature planning) and usage guidelines for \"dot env\" environment files in project scope in a suggested (and best-intend compatible) fashion (fa\u00e7on).","title":"Pipelines Environment Variable Usage"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html#default-variables","text":"Variable Name Remarks CI all options; always set to \" true \" BITBUCKET_BOOKMARK --trigger <ref> where <ref> is bookmark:<name> ; for Mercurial projects BITBUCKET_BRANCH --trigger <ref> where <ref> is branch:<name> , pr:<name> , pr:<source>:<destination> ; (source) branch BITBUCKET_BUILD_NUMBER all options; always set to \" 0 \" BITBUCKET_CLONE_DIR all options; set by pipelines, it is the deploy directory inside the container (not clone directory) as pipelines has more options than cloning (it currently actually never clones) BITBUCKET_COMMIT all options; always set to \" 0000000000000000000000000000000000000000 \" BITBUCKET_DEPLOYMENT_ENVIRONMENT -/-; currently unsupported BITBUCKET_DEPLOYMENT_ENVIRONMENT_UUID -/-; currently unsupported BITBUCKET_EXIT_CODE all options; set to the exit status of the script for use in the after-script BITBUCKET_GIT_HTTP_ORIGIN -/-; currently unsupported BITBUCKET_GIT_SSH_ORIGIN -/-; currently unsupported BITBUCKET_PARALLEL_STEP all options; in a parallel step set to zero-based index of the current step in the group, e.g. 0, 1, 2, ... BITBUCKET_PARALLEL_STEP_COUNT all options; in a parallel step set to the total number of steps in the group, e.g. 5. BITBUCKET_PR_DESTINATION_BRANCH --trigger <ref> where <ref> is pr:<source>:<destination> for the <destination> branch (see as well BITBUCKET_BRANCH ); destination branch BITBUCKET_PR_ID -/-; currently unsupported BITBUCKET_PROJECT_KEY -/-; currently unsupported BITBUCKET_PROJECT_UUID -/-; currently unsupported BITBUCKET_REPO_FULL_NAME -/-; currently unsupported BITBUCKET_REPO_OWNER all options; always set to current username from environment or if not available \" nobody \" (which might align w/ the Apache httpd project) BITBUCKET_REPO_OWNER_UUID -/-; currently unsupported BITBUCKET_REPO_SLUG all options; always set to base name of project directory BITBUCKET_REPO_UUID -/-; currently unsupported BITBUCKET_STEP_RUN_NUMBER all options; defaults to \" 1 \" and is set to \" 1 \" after the first run step BITBUCKET_STEP_TRIGGERER_UUID -/-; currently unsupported BITBUCKET_TAG --trigger <ref> where <ref> is tag:<name> ; Git projects","title":"Default Variables"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html#comments-on-support-and-usage","text":"All environment variables are supported in the meaning they can be explicitly passed with any value of the users wish (or command) into the container. That is even true for those variables \"always\" set to a specific value - variables passed by the user will always override any automatically \"always\" set values.","title":"Comments on Support and Usage"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html#unsupported-features-and-environment-variables","text":"Some variables yet do not make sense as the pipelines utility does not support the underlying feature. For example, the variable BITBUCKET_DEPLOYMENT_ENVIRONMENT and the depployments are not (yet) supported, therefore the variable BITBUCKET_DEPLOYMENT_ENVIRONMENT is not set. Setting it before call makes not much sense as it would be set for every script, and not the deployment script only. However: Some variables which are valid per project and which are marked \"currently unsupported\" above do make sense to be put into the auto-loading environment files ( .env and .env.dist ). As long as their values are not considered a secret, they can be added including the values to .env.dist which intention is to be committed in the project repository (see your VCS/SCMs' documentation, e.g. for git see git-ignore [GIT-IGNORE]). Otherwise, instead of poisoning your local environment, project secrets can be put into the private .env file (or any other *.env file via the --env-file option). This is as long as you consider your local system safe (if your computer is connected to the internet this might not be the case and might be an overall short-coming with the Atlassian Bitbucket Cloud Pipelines Plugin, take care [!] as Secured Variables are just masked and can be read by any user who has write access to a Bitbucket repository - similar to read access on a/your local system).","title":"Unsupported Features and Environment Variables"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html#dealing-w-secrets-as-variable-values","text":"For secret environment parameters, put them into the .env file only (or other .env files - if at all) which by intention is/are not to be committed in the project repository. However: Required variables (with secrets) should be added with their name only to .env.dist file which is the project distribution parameter definition so that it clearly is documented which variables are required (this can be very helpful to document which secrets are needed and for non-secret what [sane] defaults are).","title":"Dealing w/ Secrets as Variable Values"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html#example-configuration-of-an-environment-file","text":"Example configuration in a git based project with \"dot env\" files as they work w/ the pipelines utility: The pipelines utility adheres to environment files as by the Docker project. That is mainly b/c it is highly dependent on the Docker client utility and the environment file-format in the Docker project ships with all needed features: environment variables , environment parameter imports / definitions and comments . The following example consists of three files: First a .gitignore file that takes care that no environment files but .env.dist is getting committed to the git repository. This is necessary as these files might contain secrets (even in plain text) which should never go into the projects history (applies to git -utility managed projects only for .gitignore see your VCS utility if different for similar options). From pipelines perspective, the file intended to be committed is the .env.dist file. It should contain all major variables but no secrets. This is possible by writing down the variable (parameter) name only . One such variable in the example is AWS_ACCESS_KEY_ID . The key-id and the access key are considered a secret in the following example. The .env file then (can) contain private information if ok persisted in the local file-system. Different dot-env files can be provided w/ the --env-file <path> option of the pipelines (or docker ) utility. .gitignore file: # ... # Private environment variable files (dot env) /.env* !/.env.dist # ... (for more information on the .gitignore file format best see the gitignore documentation [GIT-IGNORE]) .env.dist file: # --- # Pipeline: Gitlab Cloud Project # BITBUCKET_REPO_OWNER = ACME.Inc. BITBUCKET_REPO_OWNER_UUID = 894510a8-be2a-4660-a276-10fd5280cc61 BITBUCKET_REPO_SLUG = sudo-make-sandwich BITBUCKET_REPO_UUID = 894510a8-be2a-4660-a276-10fd5280cc61 # --- # Pipeline: AWS ECR # AWS_REGION = eu-west-1 AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY (secrets in this .env file are hidden, only their variable names are given, e.g. AWS_SECRET_ACCESS_KEY ) .env file: # --- # Pipeline: AWS ECR # AWS_REGION = eu-west-1 AWS_ACCESS_KEY_ID = ETI1ZUOWJ2GO5Q8IDFDC AWS_SECRET_ACCESS_KEY = IzuobhF55og9fel6hleyuo4UOA0lUL9+GE2RmH6J This example provides the project-wide information which environment variables are required for it. Additionally the project-wide default values are provided and the configuration setup does allow (local) (secret) configuration. The comment blocks for a section are multi-line and follow an exemplary format of the first line of the comment block having a separating three-dash-line and a single (empty) line at the end after the second line containing a section title so that it is easy to diff between .env and .env.dist (and other) files.","title":"Example Configuration of an Environment File"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html#why-multi-line-comments","text":"Why multi-line comments? In practice I found that single line comments are not that practical with text differs for sections , actually this is how this format came to life; when using various diff -utilities to compare these various dot-env files. It is however up to every users discreet own experience(s) and need(s) on how to introduce sections in dot env files. It is only a subjective suggestion. Note that there is a single empty line before any other but the first section comment block if the first section comment block starts on the very first line of the file. This is crucial for all differs I've run across.","title":"Why Multi-Line Comments?"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html#environment-parameters-considered-for-distribution","text":"If a project fully relies on Atlassian Bitbucket Cloud Pipelines Environment (by my reading of their documentation) the following environment variables and values should be distributed within the project: BITBUCKET_PROJECT_KEY BITBUCKET_PROJECT_UUID BITBUCKET_REPO_FULL_NAME BITBUCKET_REPO_OWNER BITBUCKET_REPO_OWNER_UUID BITBUCKET_REPO_SLUG - if not matching project base name BITBUCKET_REPO_UUID Note: Perhaps UUID values are gained best via the Bitbucket Cloud REST API on the shell command line.","title":"Environment Parameters Considered for Distribution"},{"location":"doc/PIPELINES-VARIABLE-REFERENCE.html#references","text":"[BBPL-ENV]: https://confluence.atlassian.com/bitbucket/environment-variables-794502608.html [GIT-IGNORE]: https://git-scm.com/docs/gitignore","title":"References"},{"location":"lib/build/mkdocs/index.html","text":"Pipelines HTML Documentation Build \u00b6 The pipelines documentation is a collection of HTML files. There are two flavours: Static HTML files for local browsing ( html-docs ) The project homepage on Github-Pages ( gh-pages ) It is build with Mkdocs [MKDOCS] and the Material for Mkdocs theme [MKDOCS-MATERIAL]. Generating HTML-Docs for local browsing \u00b6 The HTML-Docs for local browsing are created by a custom pipeline: $ bin/pipelines --pipeline custom/html-docs After running this pipeline the documentation is ready to browse at build/html-docs/index.html . HTML Documentation Overview \u00b6 The HTML documentation is generated from Markdown ( .md ) files. Different to a vanilla Mkdocs project, in pipelines the documentation files are within the project repository from its root and not in a sub-folder named docs . This is different to the default Mkdocs layout in so far, that the default expects a docs folder containing the markdown files. In pipelines, this docs folder is created when building the HTML documentation from a small symlink-farm of the Markdown documents from the project root. At the same time some additional content is generated from other project files. All this is wired in a Makefile and for the standard HTML docs build available as a custom pipeline . Getting Started with the HTML-Docs Build \u00b6 The html documentation is build with make by the Makefile in the lib/build/mkdocs directory: $ cd lib/build/mkdocs # mkdocs based build directory $ make # build standard html-docs ... There are two standard goals: html-docs - HTML documentation that can be opened in a browser from the file-system. It includes a javascript based search and keeps online resources (e.g. remote images, fonts or scripts) out of it. (default goal) gh-pages - the online version as on Github-Pages, the pipelines project homepage. This includes online resources like badges in the README.md and Github API requests. For these artifacts the makefile goals: $ make html-docs and $ make gh-pages Results are placed into the build/html-docs and build/gh-pages directories accordingly. Development Build \u00b6 Most of the documentation markdown files can be edited and the documentation is build on-the-fly while served from a running test-server: $ make serve Here again it can be chosen as from the two variants as additional targets: $ make serve-gh # gh-pages goal and: $ make serve-html # html-docs goal Build and Make Requirements \u00b6 To develop the html documentation use the Makefile in the lib/build/mkdocs directory for more fine grained control of the build incl. installing the build tools and control other prerequisites. The build requires: Python3 (as python3 by default, see python build parameter) PIP PHP (with the JSON extension at least) Composer Git GNU Make Bash A file-system that supports relative symbolic links Mkdocs, plugins and themes will be installed if missing. Specifics of the Mkdocs based Build \u00b6 Even both Mkdocs and the Material for Mkdocs theme are pretty well standardized, developing is a moving target much so often. Here some of the details that appear noteworthy about creating the documentation in the pipelines project: Theme Modifications \u00b6 Mkdocs offers a modular way to modify the theme. And Material for Mkdocs is used as the theme. Still some little modifications are done. This is most of all controlled in the standard mkdocs.yml Mkdocs configuration file pointing to resources in the file-system inside the mkdocs library build ( lib/build/mkdocs ) folder: SVG logo and favicon (in docs/assets ) CSS Tweaks (in docs/assets/extra.css ): Vertical scrolling of code-blocks that exceed a certain height. Touch-up of the CSS for snappier table appearances. Tweak on the footer icons to make Material for Mkdocs stand out. Using Ghp-Import to publish to Github-Pages \u00b6 The ghp-import [GHP-IMPORT] command can be used to publish a static site to Github-Pages. By default Mkdocs has publishing to Github-Pages build-in making use of the ghp-import library just with its own utility command. For a more tailored publishing, ghp-import is installed as a utility on its own so that the gh-pages target can contain the appropriate tree layout which is not root but the docs folder as the publishing source and it allows to prominently present a README.md of its own when browsing the publishing branch on Github. This can be easily handled by using the ghp-import utility standalone. The publishing script is script/publish.sh . Configuring a publishing source for your GitHub Pages site (docs.github.com) Making the Mkdocs Build Deterministic \u00b6 A few modifications were necessary to make the Mkdocs build and the publishing to Github-Pages more deterministic: Gzip file timestamp in sitemap.xml.gz (gzip, mkdocs build) <lastmod> timestamps in sitemap.xml (mkdocs build/theme) Timestamps in publishing commit (git) File content (pipelines) Timestamp in sitemap.xml.gz \u00b6 The gzip version of the sitemap.xml file contains the time-stamp of the sitemap.xml file which is generated when the docs are build. Recompressing the file without the name in post processing levitates this ( gzip -n or gzip --no-name ). This gzip(1) build post-processing is not necessary any longer since Mkdocs Version 1.1.1 (2020-05-12) . Mkdocs sitemap.xml.gz file timestamp has support at build time for the [ SOURCE_DATE_EPOCH environment parameter] in [ fa5aa4a2 / #2100] via [ 7f30f7b8 / #1010] and [ 3e10e014 / #939]. Superfluous <lastmod> elements in sitemap.xml \u00b6 On build time, each sitemap entry gets a <lastmod> element entry with the current date ( YYYY-MM-DD ). Removing those <lastmod> elements (done in overriding the sitemap.xml template) prevents running into this problem. In general the <lastmod> element is not mandatory, and in concrete it has very little meaning, as it contains build time, so all last modification dates are the same and are therefore redundant. As an alternative to drop the <lastmod> element, it should be possible as well to make use of Mkdocs update_date / [ build_date_utc ] and/or the mkdocs-git-revision-date-plugin (not tested). Custom sitemap.xml template? (Mkdocs groups.google.com) Hide empty lastmod tags in sitemap.xml #1465 (Mkdocs github.com) Add a \"Last Updated\" field to .md pages (Mkdocs groups.google.com) Timestamp Github Publishing Commit \u00b6 The Makefile build sets the publishing commit timestamp to the date in git of the revision the docs are build from. This ensures that for each documentation revision (content) the timestamp is the same. It also establishes the property that documentation build from the same content (and same python dependencies) results in the same commit hash. Reproducible Pipelines Files \u00b6 When building from a revision/checkout normally all documentation files from the project are of that revision and therefore the build can be deterministic. In pipelines , when the project is checked-out and installed, some files are generated. When any of such files is taken into the documentation build, then these files need to generate always the same by their underlying revision. In the project one such special case is a test package for the docker client binary. This test-package ( lib/package/docker-42.42.1-binsh-test-stub.yml ) references an artifact ( .tar.gz file) with a SHA-256 hash. With each installation, the hash of the package file changed and therefore the test-package .yml file. Exemplary fix was to make the tar package anonymous and set the timestamp to begin of the UNIX epoch ( --owner=0 --group=0 --mtime='UTC 1970-01-01' in gnu tar). Additionally gzip must be commanded to do not save the original file -name and -timestamp ( gzip -n or gzip --no-name ). Migrating URLs on Github-Pages \u00b6 When the initial prototype of the html-docs landed on Github-Pages and since that day made its way into remote references, it was much later realized that the wrong Mkdocs URL path layout was in use ( directory_urls: true ) while it should have been directory_urls: false . This layout saves some of the files and directories to be created. And results in a better representation of the documents. Therefore a migration was due from the old to the new layout. It could be successfully established. Over a period of ca. a month all search engine entries were using the correct URL. Migrating with Rel Canonical \u00b6 Technically this has been done by ensuring the old (wrong) files contain a <link href=\"...\" rel=\"canonical> with the reference to the new (correct) file [ Canonical Link Element ]. Additionally all old (wrong) links within all old (wrong) files are re-written to their new (correct) links. All such fixed files are then merged into the correct build. So in detail, the gh-pages build are two builds. First the correct one (the current one). Second, the wrong one (the old). The later is done into a directory of it's own. Then the resulting files are processed, loaded into memory and when a fixer applies are fixed, re-compressed and written out into the output directory of the first build resulting in a partial merge with all old URIs (so not having any 404s) with appropriate pointers to their correct URIs via the canonical link. Migrating with Redirects \u00b6 Albeit migrating with redirects could have been an option as well, but the Mkdocs redirects plugin does not offer any of the required functionality. It is not aware of the different directory_urls path layouts for the redirect it offers. Working with canonical link relations was considered the alternative then and as it started quickly with simple search and replace operations it was preferred over redirects. It has been proven as the right decision as the search engine results (SERPS) show. Assumably the redirections would have led to a similar result. The script to modify the links in all old index.html files uses a HTML parser and modifier, which is better than the first search and replace approach. It could be extended with another command-line option to add meta- redirects for these files, too. This could be done next to be finally able to remove the old and wrong HTML pages completely without or only very little negative effect. Mkdocs-Material Fonts \u00b6 The webfonts are disabled, if the Roboto and Roboto Mono fonts are available locally, they will be used. This is done via theme: fonts: false in mkdocs-material and by font-local.css that is @import in extra.css . References \u00b6 [MKDOCS]: https://www.mkdocs.org/ [MKDOCS-MATERIAL]: https://squidfunk.github.io/mkdocs-material/ [GHP-IMPORT]: https://pypi.org/project/ghp-import/ [Canonical Link Element]: https://en.wikipedia.org/wiki/Canonical_link_element","title":"HTML Documentation Build"},{"location":"lib/build/mkdocs/index.html#pipelines-html-documentation-build","text":"The pipelines documentation is a collection of HTML files. There are two flavours: Static HTML files for local browsing ( html-docs ) The project homepage on Github-Pages ( gh-pages ) It is build with Mkdocs [MKDOCS] and the Material for Mkdocs theme [MKDOCS-MATERIAL].","title":"Pipelines HTML Documentation Build"},{"location":"lib/build/mkdocs/index.html#generating-html-docs-for-local-browsing","text":"The HTML-Docs for local browsing are created by a custom pipeline: $ bin/pipelines --pipeline custom/html-docs After running this pipeline the documentation is ready to browse at build/html-docs/index.html .","title":"Generating HTML-Docs for local browsing"},{"location":"lib/build/mkdocs/index.html#html-documentation-overview","text":"The HTML documentation is generated from Markdown ( .md ) files. Different to a vanilla Mkdocs project, in pipelines the documentation files are within the project repository from its root and not in a sub-folder named docs . This is different to the default Mkdocs layout in so far, that the default expects a docs folder containing the markdown files. In pipelines, this docs folder is created when building the HTML documentation from a small symlink-farm of the Markdown documents from the project root. At the same time some additional content is generated from other project files. All this is wired in a Makefile and for the standard HTML docs build available as a custom pipeline .","title":"HTML Documentation Overview"},{"location":"lib/build/mkdocs/index.html#getting-started-with-the-html-docs-build","text":"The html documentation is build with make by the Makefile in the lib/build/mkdocs directory: $ cd lib/build/mkdocs # mkdocs based build directory $ make # build standard html-docs ... There are two standard goals: html-docs - HTML documentation that can be opened in a browser from the file-system. It includes a javascript based search and keeps online resources (e.g. remote images, fonts or scripts) out of it. (default goal) gh-pages - the online version as on Github-Pages, the pipelines project homepage. This includes online resources like badges in the README.md and Github API requests. For these artifacts the makefile goals: $ make html-docs and $ make gh-pages Results are placed into the build/html-docs and build/gh-pages directories accordingly.","title":"Getting Started with the HTML-Docs Build"},{"location":"lib/build/mkdocs/index.html#development-build","text":"Most of the documentation markdown files can be edited and the documentation is build on-the-fly while served from a running test-server: $ make serve Here again it can be chosen as from the two variants as additional targets: $ make serve-gh # gh-pages goal and: $ make serve-html # html-docs goal","title":"Development Build"},{"location":"lib/build/mkdocs/index.html#build-and-make-requirements","text":"To develop the html documentation use the Makefile in the lib/build/mkdocs directory for more fine grained control of the build incl. installing the build tools and control other prerequisites. The build requires: Python3 (as python3 by default, see python build parameter) PIP PHP (with the JSON extension at least) Composer Git GNU Make Bash A file-system that supports relative symbolic links Mkdocs, plugins and themes will be installed if missing.","title":"Build and Make Requirements"},{"location":"lib/build/mkdocs/index.html#specifics-of-the-mkdocs-based-build","text":"Even both Mkdocs and the Material for Mkdocs theme are pretty well standardized, developing is a moving target much so often. Here some of the details that appear noteworthy about creating the documentation in the pipelines project:","title":"Specifics of the Mkdocs based Build"},{"location":"lib/build/mkdocs/index.html#theme-modifications","text":"Mkdocs offers a modular way to modify the theme. And Material for Mkdocs is used as the theme. Still some little modifications are done. This is most of all controlled in the standard mkdocs.yml Mkdocs configuration file pointing to resources in the file-system inside the mkdocs library build ( lib/build/mkdocs ) folder: SVG logo and favicon (in docs/assets ) CSS Tweaks (in docs/assets/extra.css ): Vertical scrolling of code-blocks that exceed a certain height. Touch-up of the CSS for snappier table appearances. Tweak on the footer icons to make Material for Mkdocs stand out.","title":"Theme Modifications"},{"location":"lib/build/mkdocs/index.html#using-ghp-import-to-publish-to-github-pages","text":"The ghp-import [GHP-IMPORT] command can be used to publish a static site to Github-Pages. By default Mkdocs has publishing to Github-Pages build-in making use of the ghp-import library just with its own utility command. For a more tailored publishing, ghp-import is installed as a utility on its own so that the gh-pages target can contain the appropriate tree layout which is not root but the docs folder as the publishing source and it allows to prominently present a README.md of its own when browsing the publishing branch on Github. This can be easily handled by using the ghp-import utility standalone. The publishing script is script/publish.sh . Configuring a publishing source for your GitHub Pages site (docs.github.com)","title":"Using Ghp-Import to publish to Github-Pages"},{"location":"lib/build/mkdocs/index.html#making-the-mkdocs-build-deterministic","text":"A few modifications were necessary to make the Mkdocs build and the publishing to Github-Pages more deterministic: Gzip file timestamp in sitemap.xml.gz (gzip, mkdocs build) <lastmod> timestamps in sitemap.xml (mkdocs build/theme) Timestamps in publishing commit (git) File content (pipelines)","title":"Making the Mkdocs Build Deterministic"},{"location":"lib/build/mkdocs/index.html#timestamp-in-sitemapxmlgz","text":"The gzip version of the sitemap.xml file contains the time-stamp of the sitemap.xml file which is generated when the docs are build. Recompressing the file without the name in post processing levitates this ( gzip -n or gzip --no-name ). This gzip(1) build post-processing is not necessary any longer since Mkdocs Version 1.1.1 (2020-05-12) . Mkdocs sitemap.xml.gz file timestamp has support at build time for the [ SOURCE_DATE_EPOCH environment parameter] in [ fa5aa4a2 / #2100] via [ 7f30f7b8 / #1010] and [ 3e10e014 / #939].","title":"Timestamp in sitemap.xml.gz"},{"location":"lib/build/mkdocs/index.html#superfluous-lastmod-elements-in-sitemapxml","text":"On build time, each sitemap entry gets a <lastmod> element entry with the current date ( YYYY-MM-DD ). Removing those <lastmod> elements (done in overriding the sitemap.xml template) prevents running into this problem. In general the <lastmod> element is not mandatory, and in concrete it has very little meaning, as it contains build time, so all last modification dates are the same and are therefore redundant. As an alternative to drop the <lastmod> element, it should be possible as well to make use of Mkdocs update_date / [ build_date_utc ] and/or the mkdocs-git-revision-date-plugin (not tested). Custom sitemap.xml template? (Mkdocs groups.google.com) Hide empty lastmod tags in sitemap.xml #1465 (Mkdocs github.com) Add a \"Last Updated\" field to .md pages (Mkdocs groups.google.com)","title":"Superfluous &lt;lastmod&gt; elements in sitemap.xml"},{"location":"lib/build/mkdocs/index.html#timestamp-github-publishing-commit","text":"The Makefile build sets the publishing commit timestamp to the date in git of the revision the docs are build from. This ensures that for each documentation revision (content) the timestamp is the same. It also establishes the property that documentation build from the same content (and same python dependencies) results in the same commit hash.","title":"Timestamp Github Publishing Commit"},{"location":"lib/build/mkdocs/index.html#reproducible-pipelines-files","text":"When building from a revision/checkout normally all documentation files from the project are of that revision and therefore the build can be deterministic. In pipelines , when the project is checked-out and installed, some files are generated. When any of such files is taken into the documentation build, then these files need to generate always the same by their underlying revision. In the project one such special case is a test package for the docker client binary. This test-package ( lib/package/docker-42.42.1-binsh-test-stub.yml ) references an artifact ( .tar.gz file) with a SHA-256 hash. With each installation, the hash of the package file changed and therefore the test-package .yml file. Exemplary fix was to make the tar package anonymous and set the timestamp to begin of the UNIX epoch ( --owner=0 --group=0 --mtime='UTC 1970-01-01' in gnu tar). Additionally gzip must be commanded to do not save the original file -name and -timestamp ( gzip -n or gzip --no-name ).","title":"Reproducible Pipelines Files"},{"location":"lib/build/mkdocs/index.html#migrating-urls-on-github-pages","text":"When the initial prototype of the html-docs landed on Github-Pages and since that day made its way into remote references, it was much later realized that the wrong Mkdocs URL path layout was in use ( directory_urls: true ) while it should have been directory_urls: false . This layout saves some of the files and directories to be created. And results in a better representation of the documents. Therefore a migration was due from the old to the new layout. It could be successfully established. Over a period of ca. a month all search engine entries were using the correct URL.","title":"Migrating URLs on Github-Pages"},{"location":"lib/build/mkdocs/index.html#migrating-with-rel-canonical","text":"Technically this has been done by ensuring the old (wrong) files contain a <link href=\"...\" rel=\"canonical> with the reference to the new (correct) file [ Canonical Link Element ]. Additionally all old (wrong) links within all old (wrong) files are re-written to their new (correct) links. All such fixed files are then merged into the correct build. So in detail, the gh-pages build are two builds. First the correct one (the current one). Second, the wrong one (the old). The later is done into a directory of it's own. Then the resulting files are processed, loaded into memory and when a fixer applies are fixed, re-compressed and written out into the output directory of the first build resulting in a partial merge with all old URIs (so not having any 404s) with appropriate pointers to their correct URIs via the canonical link.","title":"Migrating with Rel Canonical"},{"location":"lib/build/mkdocs/index.html#migrating-with-redirects","text":"Albeit migrating with redirects could have been an option as well, but the Mkdocs redirects plugin does not offer any of the required functionality. It is not aware of the different directory_urls path layouts for the redirect it offers. Working with canonical link relations was considered the alternative then and as it started quickly with simple search and replace operations it was preferred over redirects. It has been proven as the right decision as the search engine results (SERPS) show. Assumably the redirections would have led to a similar result. The script to modify the links in all old index.html files uses a HTML parser and modifier, which is better than the first search and replace approach. It could be extended with another command-line option to add meta- redirects for these files, too. This could be done next to be finally able to remove the old and wrong HTML pages completely without or only very little negative effect.","title":"Migrating with Redirects"},{"location":"lib/build/mkdocs/index.html#mkdocs-material-fonts","text":"The webfonts are disabled, if the Roboto and Roboto Mono fonts are available locally, they will be used. This is done via theme: fonts: false in mkdocs-material and by font-local.css that is @import in extra.css .","title":"Mkdocs-Material Fonts"},{"location":"lib/build/mkdocs/index.html#references","text":"[MKDOCS]: https://www.mkdocs.org/ [MKDOCS-MATERIAL]: https://squidfunk.github.io/mkdocs-material/ [GHP-IMPORT]: https://pypi.org/project/ghp-import/ [Canonical Link Element]: https://en.wikipedia.org/wiki/Canonical_link_element","title":"References"}]}; var __search = { index: Promise.resolve(local_index) }